================================================
FILE: README.md
================================================
<h1><div align="center">
 <img alt="pipecat" width="300px" height="auto" src="https://raw.githubusercontent.com/pipecat-ai/pipecat-examples/main/pipecat-examples.png">
</div></h1>

[![Docs](https://img.shields.io/badge/Documentation-blue)](https://docs.pipecat.ai) [![Discord](https://img.shields.io/discord/1239284677165056021)](https://discord.gg/pipecat)

A collection of example applications built with [Pipecat](https://github.com/pipecat-ai/pipecat), an open-source framework for building voice and multimodal AI applications.

## New to Pipecat?

**Learning examples** are in the main Pipecat repo, **intermediate and advanced examples** are here.

Start with the [quickstart example](https://github.com/pipecat-ai/pipecat/tree/main/examples/quickstart) in the main Pipecat repo to get your first bot running in 5 minutes.

Then continue learning with these starter examples, located in the Pipecat repo:

- **[client-server-web example](https://github.com/pipecat-ai/pipecat/tree/main/examples/client-server-web)**: Learn client/server architecture with web clients
- **[phone-bot-twilio](https://github.com/pipecat-ai/pipecat/tree/main/examples/phone-bot-twilio)**: Learn how to connect your bot to a phone number using Twilio

Once you understand the basics, check out the examples below.

## Prerequisites

Most examples require:

- Python 3.10 or newer
- API keys for AI services (OpenAI, Deepgram, Cartesia, etc.)
- Additional service-specific requirements (see individual example READMEs)

## Popular Examples

Ready to explore more? These are two of the most useful examples for common use cases:

- **[simple-chatbot](simple-chatbot/)** - Client/server examples with React, JavaScript, Swift, Kotlin, and React Native
- **[twilio-chatbot](twilio-chatbot/)** - Production-ready phone bot with Twilio integration

## Example Categories

### **Telephony & Voice Calls**

- **[phone-chatbot](phone-chatbot/)** - Daily PSTN and SIP dial-in and dial-out examples
- **[twilio-chatbot](twilio-chatbot/)** - Production-ready phone bot with Twilio integration
- **[telnyx-chatbot](telnyx-chatbot/)** - Production-ready phone bot with Telnyx integration
- **[plivo-chatbot](plivo-chatbot/)** - Production-ready phone bot with Plivo integration
- **[exotel-chatbot](exotel-chatbot/)** - Production-ready phone bot with Exotel integration
- **[ivr-navigation](ivr-navigation/)** - Dial-out and navigate an IVR call tree

### **Web & Client Applications**

- **[simple-chatbot](simple-chatbot/)** - Client/server examples with React, JavaScript, Swift, Kotlin, and React Native
- **[push-to-talk](push-to-talk/)** - Client/server example showing how to build a Push-to-Talk app using Voice UI Kit and Pipecat's JS and React SDKs
- **[websocket](websocket/)** - WebSocket-based real-time communication
- **[instant-voice](instant-voice/)** - Enable instant voice communication as soon as a user connects
- **[p2p-webrtc](p2p-webrtc/)** - Simple peer-to-peer WebRTC voice bot

### **Realtime APIs**

- **[word-wrangler-gemini-live](word-wrangler-gemini-live/)** - Web & phone based voice AI word games using Gemini Live
- **[aws-strands](aws-strands/)** - Use AWS Strands for multi-step tasks

### **Multimodal & Creative**

- **[storytelling-chatbot](storytelling-chatbot/)** - Interactive storytelling experiences
- **[news-chatbot](news-chatbot/)** - AI news reader and discussion bot

### **Translation & Localization**

- **[translation-chatbot](translation-chatbot/)** - Real-time language translation
- **[daily-multi-translation](daily-multi-translation/)** - Multi-language conference calls

### **Support, Educational & Specialized**

- **[studypal](studypal/)** - AI-powered study companion

### **Advanced Features**

- **[local-smart-turn](local-smart-turn/)** - Smart conversation turn-taking
- **[daily-custom-tracks](daily-custom-tracks/)** - Custom audio/video track handling
- **[local-input-select-stt](local-input-select-stt/)** - Local audio input selection
- **[bot-ready-signalling](bot-ready-signalling/)** - Bot connect management

### **Deployment & Infrastructure**

- **[deployment](deployment/)** - Production deployment examples using Pipecat Cloud, Fly.io, Modal, Cerebrium

### **Monitoring & Analytics**

- **[open-telemetry](open-telemetry/)** - Observability and tracing examples using Langfuse and Jaeger

### **Testing & Development**

- **[freeze-test](freeze-test/)** - Pipeline freezing and state management testing

## Getting Help

- **Documentation**: [docs.pipecat.ai](https://docs.pipecat.ai)
- **Discord Community**: [discord.gg/pipecat](https://discord.gg/pipecat)
- **Main Repository**: [github.com/pipecat-ai/pipecat](https://github.com/pipecat-ai/pipecat)
- **Issues**: Report bugs or request examples in the [main repo issues](https://github.com/pipecat-ai/pipecat/issues)



================================================
FILE: LICENSE
================================================
BSD 2-Clause License

Copyright (c) 2024–2025, Daily

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



================================================
FILE: pyproject.toml
================================================
[project]
name = "pipecat-examples"
version = "0.0.0"
description = "Examples for Pipecat AI framework"
requires-python = ">=3.10"
dependencies = []

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []



================================================
FILE: .pre-commit-config.yaml
================================================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.9.7
    hooks:
      - id: ruff
        language_version: python3
        args: [--fix]
      - id: ruff-format



================================================
FILE: aws-agentcore/README.md
================================================
# Amazon Bedrock AgentCore Example

This example demonstrates how to integrate an AgentCore-hosted agent into a Pipecat pipeline.

The pipeline looks like a standard Pipecat bot pipeline, but with an AgentCore agent taking the place of an LLM. User audio gets converted to text and sent to the AgentCore agent, which will try to do work on the user's behalf. Responses from the agent are streamed back and spoken. User and agent messages are recorded in a context object.

Note that unlike an LLM service found in a traditional Pipecat bot pipeline, the AgentCore agent by default does not receive the full conversation context after each user turn, only the last user message. It is up to the AgentCore agent to decide whether and how to manage its own memory (AgentCore includes memory capabilities).

## Prerequisites

- Accounts with:
  - AWS (with access to Bedrock AgentCore and Claude 3.7 Sonnet model)
  - Deepgram
  - Cartesia
  - Daily (optional)
- Python 3.10 or higher
- `uv` package manager

## Setup

### Install Dependencies

Install dependencies needed to run the Pipecat bot as well as the AgentCore CLI.

```bash
uv sync
```

This installs:

- **Pipecat** - The voice AI pipeline framework
- **Strands** - AWS's agentic framework (used in the code agent)
- **Bedrock AgentCore Starter Toolkit** - CLI tools for deploying agents
- **Strands Tools** - Pre-built tools like the Code Interpreter

### Set Environment Variables

Copy `env.example` to `.env` and fill in the values in `.env`.

```bash
cp env.example .env
```

**Do not worry** about `AWS_AGENT_ARN` yet. You'll obtain an agent ARN as part of the following steps, when you deploy your agent to AgentCore Runtime.

## Deploying Your Agent to AgentCore Runtime

Before you can run the Pipecat bot file, you need to deploy an agent to AgentCore Runtime. This example includes two agents:

- **Dummy agent** (`dummy_agent.py`) - Reports progress while pretending to carry out a relatively long-running task
- **Code agent** (`code_agent.py`) - An algorithmic-problem-solving agent built with Strands that can write and execute Python code to answer questions

### About the Code Agent

The code agent demonstrates how to use **Strands** (AWS's agentic framework) within AgentCore:

- Uses the **Strands Agent** with Claude 3.7 Sonnet model
- Includes the **AgentCore Code Interpreter** tool for executing Python code
- Streams responses in real-time for a conversational experience
- Designed for voice interaction with TTS-friendly output

Below we'll do a barebones walkthrough of deploying an agent to AgentCore Runtime. For a comprehensive guide to getting started with Amazon Bedrock AgentCore, including detailed setup instructions, see the [Amazon Bedrock AgentCore Developer Guide](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html).

### IAM Setup

Configure your IAM user with the necessary policies for AgentCore usage. Start with these:

- `BedrockAgentCoreFullAccess`
- A new policy (maybe named `BedrockAgentCoreCLI`) configured [like this](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html#runtime-permissions-starter-toolkit)

You can also choose to specify more granular permissions; see [Amazon Bedrock AgentCore docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html) for more information.

### Environment Setup

To simplify the remaining AgentCore deployment steps in this README, it's a good idea to export some AWS-specific environment variables:

```bash
export AWS_SECRET_ACCESS_KEY=...
export AWS_ACCESS_KEY_ID=...
export AWS_REGION=...
```

### Agent Configuration

Create a new AgentCore configuration.

```bash
cd agents
uv run agentcore configure -e code_agent.py
```

Follow the interactive prompts to complete the configuration. It's OK to just accept all defaults.

### Agent Deployment

Deploy your agent to AgentCore Runtime.

```bash
uv run agentcore launch
```

This step will spit out the agent ARN. Copy it and paste it in your `.env` file as your `AWS_AGENT_ARN` value.

The above is also the command you need to run after you've updated your agent code and need to redeploy.

### Validation

Try running your agent on AgentCore Runtime.

```bash
uv run agentcore invoke '{"prompt": "What is the meaning of life?"}'
```

### Obtaining Your Agent ARN at Any Point

Your agent status will include its ARN.

```bash
uv run agentcore status
```

## Running The Example

With your agent deployed to AgentCore, you can now run the example.

```bash
# Using SmallWebRTC transport
uv run python bot.py

# Using Daily transport
uv run python bot.py -t daily -d
```



================================================
FILE: aws-agentcore/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame, TranscriptionMessage
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.transcript_processor import TranscriptProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.aws.agent_core import AWSAgentCoreProcessor
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams

load_dotenv(override=True)


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
}


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    agent = AWSAgentCoreProcessor(agentArn=os.getenv("AWS_AGENT_ARN"))

    transcript = TranscriptProcessor()

    # Register event handler for transcript updates
    @transcript.event_handler("on_transcript_update")
    async def on_transcript_update(processor, frame):
        for msg in frame.messages:
            if isinstance(msg, TranscriptionMessage):
                timestamp = f"[{msg.timestamp}] " if msg.timestamp else ""
                line = f"{timestamp}{msg.role}: {msg.content}"
                logger.info(f"Transcript: {line}")

    messages = [
        {"role": "user", "content": "Find the first 10 prime numbers greater than 1000"},
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            transcript.user(),  # User transcripts
            context_aggregator.user(),
            agent,
            tts,
            transport.output(),
            transcript.assistant(),  # Assistant transcripts
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        idle_timeout_secs=runner_args.pipeline_idle_timeout_secs,
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: aws-agentcore/env.example
================================================
OPENAI_API_KEY=...
DEEPGRAM_API_KEY=...
CARTESIA_API_KEY=...
DAILY_SAMPLE_ROOM_URL=https://<your-domain>.daily.co/<room-name>
DAILY_API_KEY=...
AWS_AGENT_ARN=arn:aws:bedrock-agentcore:...



================================================
FILE: aws-agentcore/pyproject.toml
================================================
[project]
name = "agentcore"
version = "0.1.0"
description = "Example of a Pipecat bot that uses an agent deployed to Amazon Bedrock AgentCore Runtime"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[cartesia,daily,deepgram,local-smart-turn-v3,openai,runner,silero,webrtc,aws]",
]

[dependency-groups]
dev = [
    "strands-agents",
    "strands-agents-tools",
    "bedrock-agentcore-starter-toolkit>=0.1.25",
    "pyright>=1.1.404,<2",
    "ruff>=0.12.11,<1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]



================================================
FILE: aws-agentcore/agents/code_agent.py
================================================
import os

from bedrock_agentcore.memory.integrations.strands.config import (
    AgentCoreMemoryConfig,
    RetrievalConfig,
)
from bedrock_agentcore.memory.integrations.strands.session_manager import (
    AgentCoreMemorySessionManager,
)
from bedrock_agentcore.runtime import BedrockAgentCoreApp
from strands import Agent
from strands_tools.code_interpreter import AgentCoreCodeInterpreter

app = BedrockAgentCoreApp()

MEMORY_ID = os.getenv("BEDROCK_AGENTCORE_MEMORY_ID")
REGION = os.getenv("AWS_REGION")
MODEL_ID = "us.anthropic.claude-3-7-sonnet-20250219-v1:0"


@app.entrypoint
async def invoke(payload, context):
    actor_id = "quickstart-user"

    # Get runtime session ID for isolation
    session_id = getattr(context, "session_id", None)

    # Create Code Interpreter with runtime session binding
    code_interpreter = AgentCoreCodeInterpreter(region=REGION, auto_create=True)

    agent = Agent(
        model=MODEL_ID,
        system_prompt="""You are a helpful assistant specializing in solving algorithmic problems with code.

Your output will be spoken aloud by text-to-speech, so use plain language without special formatting or characters (for instance, **AVOID NUMBERED OR BULLETED LISTS**).

Think aloud as you work: explain your approach before coding, describe what you're doing as you write code, and analyze the results after execution. Narrate your reasoning throughout to make your process transparent and educational.

Also, try to be as succinct as possible. Avoid unnecessary verbosity.
""",
        tools=[code_interpreter.code_interpreter],
    )

    # Stream the response
    async for event in agent.stream_async(payload.get("prompt", "")):
        if "data" in event:
            chunk = event["data"]
            # Yield chunks as they arrive for real-time streaming
            yield {"response": chunk}
        elif "result" in event:
            # Final result with stop reason
            yield {"done": True}


if __name__ == "__main__":
    app.run()



================================================
FILE: aws-agentcore/agents/dummy_agent.py
================================================
import asyncio

from bedrock_agentcore import BedrockAgentCoreApp

app = BedrockAgentCoreApp()


@app.entrypoint
async def invoke(payload, context):
    prompt = payload.get("prompt")

    yield {"response": f"Handling your request: {prompt}."}

    # Simulate some processing
    await asyncio.sleep(5)

    yield {"response": f" Still working on it..."}

    # Simulate more processing
    await asyncio.sleep(5)

    yield {"response": f" Finished! The answer, as always, is 'who knows?'."}

    # Remove yields above and uncomment the below to test non-streamed response
    # return {"response": f"Finished! The answer, as always, is 'who knows?'."}


if __name__ == "__main__":
    app.run()



================================================
FILE: aws-agentcore/agents/requirements.txt
================================================
bedrock-agentcore
strands-agents
strands-agents-tools



================================================
FILE: aws-agentcore/agents/.dockerignore
================================================
# Build artifacts
build/
dist/
*.egg-info/
*.egg

# Python cache
__pycache__/
__pycache__*
*.py[cod]
*$py.class
*.so
.Python

# Virtual environments
.venv/
.env
venv/
env/
ENV/

# Testing
.pytest_cache/
.coverage
.coverage*
htmlcov/
.tox/
*.cover
.hypothesis/
.mypy_cache/
.ruff_cache/

# Development
*.log
*.bak
*.swp
*.swo
*~
.DS_Store

# IDEs
.vscode/
.idea/

# Version control
.git/
.gitignore
.gitattributes

# Documentation
docs/
*.md
!README.md

# CI/CD
.github/
.gitlab-ci.yml
.travis.yml

# Project specific
tests/

# Bedrock AgentCore specific - keep config but exclude runtime files
.bedrock_agentcore.yaml
.dockerignore
.bedrock_agentcore/

# Keep wheelhouse for offline installations
# wheelhouse/



================================================
FILE: aws-strands/README.md
================================================
# AWS Strands Examples

This folder contains two Python examples demonstrating how to use Pipecat with the AWS Strands agent.

## Overview

These examples show how to delegate complex, multi-step tasks to a Strands agent, which can reason step-by-step and call tools to accomplish user requests.

These examples are intentionally simplified for demonstration, using mock API calls. They work best if you ask it:

> What's the weather where the Golden Gate Bridge is?

## Example Scripts

### `black-box.py`

A minimal example that demonstrates how to use the Strands agent with Pipecat. The agent can handle multi-step queries by calling tools, but does not explain its reasoning out loud.

### `explain-thinking.py`

An enhanced example where the Strands agent explains each step of its reasoning in clear, simple language as it works through a multi-step task.

## Quick Start

1. **Clone the repository and navigate to this example:**

   ```bash
   git clone https://github.com/pipecat-ai/pipecat.git
   cd pipecat/examples/aws-strands
   ```

2. **Set up a virtual environment:**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```
4. **Enable AWS Bedrock models:**
   ⚠️ **Important:** AWS Strands uses Bedrock models by default. You must first activate the required models in your AWS Bedrock console before running these examples. Visit the [AWS Bedrock Model Access documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-permissions.html) to enable model access permissions.


5. **Configure environment variables:**

   Copy the provided `env.example` file to `.env` and fill in the necessary credentials:

   ```bash
   cp env.example .env
   # Then edit .env with your preferred editor
   ```

6. **Run an example:**

   ```bash
   python black-box.py
   # or
   python explain-thinking.py
   # The transport is selected via the --transport or -t command line argument. Choices are daily webrtc and twilio.defaults to    #  webrtc
   ```



================================================
FILE: aws-strands/black-box.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame, TTSSpeakFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams
from strands import Agent, tool
from strands.models import BedrockModel

load_dotenv(override=True)

"""This example demonstrates how to use the Strands agent with Pipecat.

You can delegate complex, multi-step tasks to the Strands agent, which can cycle through LLM-based reasoning and tool calls to accomplish the task.

Try asking: "What's the weather where the Golden Gate Bridge is?"
"""

# Strands agent tools


@tool
def get_location_name_from_landmark(landmark: str) -> str:
    """
    Get the location name from a landmark.

    Args:
        landmark (str): The name of the landmark, e.g. "Golden Gate Bridge".
    """
    # Simulate fetching location
    return "San Francisco, CA"


@tool
def get_lat_long_from_location_name(location: str) -> dict:
    """
    Get the latitude and longitude for a location name.

    Args:
        location (str): The city and state, e.g. "San Francisco, CA".
    """
    # Simulate fetching lat/long from a geocoding service
    return {"lat": 37.7749, "long": -122.4194}


@tool
def get_current_weather_from_lat_long(lat: float, long: float) -> dict:
    """
    Get the current weather for a specific latitude and longitude.

    Args:
        lat (float): The latitude of the location.
        long (float): The longitude of the location.
    """
    # Simulate fetching weather data from a weather service
    return {"conditions": "nice", "temperature": "75"}


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_bot(transport: BaseTransport):
    logger.info(f"Starting bot")

    strands_agent = Agent(
        model=BedrockModel(
            model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0", max_tokens=64000
        ),
        tools=[
            get_location_name_from_landmark,
            get_lat_long_from_location_name,
            get_current_weather_from_lat_long,
        ],
        system_prompt="""
        You are a helpful personal assistant who can look up information about places and weather.

        Your key capabilities:
        1. Look up where landmarks are located.
        2. Find latitude and longitude for a location.
        3. Look up the current weather for a specific latitude and longitude.

        Explain each step of your reasoning in clear, simple, and concise language. Your responses will be converted to audio, so avoid special characters and numbered lists.
        """,
    )

    async def handle_location_or_weather_related_queries(params: FunctionCallParams, query: str):
        """
        Handle location or weather related queries.

        Args:
            query (str): The user's query, e.g. "What's the weather where the Golden Gate Bridge is?".
        """
        # Run in a background thread
        # (Otherwise the agent blocks the event loop; one effect of that is that we don't hear
        # "let me check on that" until the agent finishes)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, strands_agent, query)
        await params.result_callback(result.message)

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    llm.register_direct_function(handle_location_or_weather_related_queries)

    @llm.event_handler("on_function_calls_started")
    async def on_function_calls_started(service, function_calls):
        await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

    tools = ToolsSchema(standard_tools=[handle_location_or_weather_related_queries])

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way. Start by suggesting that the user ask about the weather where the Golden Gate Bridge is.",
        },
    ]

    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: aws-strands/env.example
================================================
OPENAI_API_KEY=
CARTESIA_API_KEY=
DEEPGRAM_API_KEY=
DAILY_API_KEY=
DAILY_SAMPLE_ROOM_URL=
AWS_SECRET_ACCESS_KEY=
AWS_ACCESS_KEY_ID=
AWS_REGION=


================================================
FILE: aws-strands/explain-thinking.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import os
import time

from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame, TTSSpeakFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams
from strands import Agent, tool
from strands.models import BedrockModel

load_dotenv(override=True)

"""This example demonstrates how to use the Strands agent with Pipecat in a way where the agent explains its reasoning step-by-step.

You can delegate complex, multi-step tasks to the Strands agent, which can cycle through LLM-based reasoning and tool calls to accomplish the task.

Try asking: "What's the weather where the Golden Gate Bridge is?"
"""


# Strands agent tools


@tool
def get_location_name_from_landmark(landmark: str) -> str:
    """
    Get the location name from a landmark.

    Args:
        landmark (str): The name of the landmark, e.g. "Golden Gate Bridge".
    """
    # Simulate fetching location (slowly)
    time.sleep(3)
    return "San Francisco, CA"


@tool
def get_lat_long_from_location_name(location: str) -> dict:
    """
    Get the latitude and longitude for a location name.

    Args:
        location (str): The city and state, e.g. "San Francisco, CA".
    """
    # Simulate fetching lat/long from a geocoding service (slowly)
    time.sleep(3)
    return {"lat": 37.7749, "long": -122.4194}


@tool
def get_current_weather_from_lat_long(lat: float, long: float) -> dict:
    """
    Get the current weather for a specific latitude and longitude.

    Args:
        lat (float): The latitude of the location.
        long (float): The longitude of the location.
    """
    # Simulate fetching weather data from a weather service (slowly)
    time.sleep(3)
    return {"conditions": "nice", "temperature": "75"}


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_bot(transport: BaseTransport):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    next_strands_message_is_last = False
    strands_messages_queue = asyncio.Queue()

    def strands_callback_handler(**kwargs):
        """
        Handle events from the Strands agent.
        """
        nonlocal next_strands_message_is_last
        if "event" in kwargs:
            event_obj = kwargs["event"]
            if event_obj and "messageStop" in event_obj:
                message_stop = event_obj["messageStop"]
                if message_stop and "stopReason" in message_stop:
                    stop_reason = message_stop["stopReason"]
                    if stop_reason == "end_turn":
                        next_strands_message_is_last = True
        elif "message" in kwargs:
            message_obj = kwargs["message"]
            if message_obj and "content" in message_obj and "role" in message_obj:
                role = message_obj["role"]
                content = message_obj["content"]
                if role == "assistant" and isinstance(content, list):
                    for content_obj in content:
                        if isinstance(content_obj, dict) and "text" in content_obj:
                            message = content_obj["text"]
                            if not next_strands_message_is_last:
                                strands_messages_queue.put_nowait(message)

    async def process_strands_messages():
        while True:
            message = await strands_messages_queue.get()
            await tts.queue_frame(TTSSpeakFrame(message))
            strands_messages_queue.task_done()

    asyncio.create_task(process_strands_messages())

    strands_agent = Agent(
        model=BedrockModel(
            model_id="us.anthropic.claude-3-7-sonnet-20250219-v1:0", max_tokens=64000
        ),
        tools=[
            get_location_name_from_landmark,
            get_lat_long_from_location_name,
            get_current_weather_from_lat_long,
        ],
        system_prompt="""
        You are a helpful personal assistant who can look up information about places and weather.

        Your key capabilities:
        1. Look up where landmarks are located.
        2. Find latitude and longitude for a location.
        3. Look up the current weather for a specific latitude and longitude.

        Explain each step of your reasoning in clear, simple, and concise language. Your responses will be converted to audio, so avoid special characters and numbered lists.
        """,
        callback_handler=strands_callback_handler,
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    async def handle_location_or_weather_related_queries(params: FunctionCallParams, query: str):
        """
        Handle location or weather related queries.

        Args:
            query (str): The user's query, e.g. "What's the weather where the Golden Gate Bridge is?".
        """
        # Run in a background thread
        # (Otherwise the agent blocks the event loop; one effect of that is that we don't hear
        # the agent's "thinking" messages until the agent finishes)
        loop = asyncio.get_running_loop()
        result = await loop.run_in_executor(None, strands_agent, query)
        await params.result_callback(result.message)

    llm.register_direct_function(handle_location_or_weather_related_queries)

    @llm.event_handler("on_function_calls_started")
    async def on_function_calls_started(service, function_calls):
        await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

    tools = ToolsSchema(standard_tools=[handle_location_or_weather_related_queries])

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way. Start by suggesting that the user ask about the weather where the Golden Gate Bridge is.",
        },
    ]

    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: aws-strands/requirements.txt
================================================
fastapi
uvicorn
python-dotenv
pipecat-ai[webrtc,daily,deepgram,cartesia,silero]>=0.0.82
# -e ../../pipecat/[webrtc,daily,deepgram,cartesia,silero]
pipecat-ai-small-webrtc-prebuilt
strands-agents



================================================
FILE: bot-ready-signalling/README.md
================================================
# Bot ready signaling

A simple Pipecat example demonstrating how to handle signaling between the client and the bot, 
ensuring that the bot starts sending audio only when the client is available, 
thereby avoiding the risk of cutting off the beginning of the audio.

## Quick Start

### First, start the bot server:

1. Navigate to the server directory:
   ```bash
   cd server
   ```
2. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install requirements:
   ```bash
   pip install -r requirements.txt
   ```
4. Copy env.example to .env and configure:
   - Add your API keys
5. Start the server:
   ```bash
   python server.py
   ```

### Next, connect using the client app:

For client-side setup, refer to the [JavaScript Guide](client/javascript/README.md).

## Important Note

Ensure the bot server is running before using any client implementations.

## Requirements

- Python 3.10+
- Node.js 16+ (for JavaScript)
- Daily API key
- Cartesia API key
- Modern web browser with WebRTC support


================================================
FILE: bot-ready-signalling/client/javascript/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the bot server. See the [server README](../../README).

2. Navigate to the `client/javascript` directory:

```bash
cd client/javascript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: bot-ready-signalling/client/javascript/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
  <div class="container">
    <div class="status-bar">
      <div class="status">
        Status: <span id="connection-status">Disconnected</span>
      </div>
      <div class="controls">
        <button id="connect-btn">Connect</button>
        <button id="disconnect-btn" disabled>Disconnect</button>
      </div>
    </div>

    <audio id="bot-audio" autoplay></audio>

    <div class="debug-panel">
      <h3>Debug Info</h3>
      <div id="debug-log"></div>
    </div>
  </div>

  <script type="module" src="/src/app.js"></script>
  <link rel="stylesheet" href="/src/style.css">
</body>

</html>



================================================
FILE: bot-ready-signalling/client/javascript/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@daily-co/daily-js": "0.74.0"
  }
}



================================================
FILE: bot-ready-signalling/client/javascript/vite.config.js
================================================
import { defineConfig } from 'vite';

export default defineConfig({
    server: {
        proxy: {
            // Proxy /api requests to the backend server
            '/connect': {
                target: 'http://0.0.0.0:7860', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: bot-ready-signalling/client/javascript/src/app.js
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

import Daily from "@daily-co/daily-js";

/**
 * ChatbotClient handles the connection and media management for a real-time
 * voice interaction with an AI bot.
 */
class ChatbotClient {
  constructor() {
    // Initialize client state
    this.dailyCallObject = null;
    this.setupDOMElements();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  setupDOMElements() {
    // Get references to UI control elements
    this.connectBtn = document.getElementById('connect-btn');
    this.disconnectBtn = document.getElementById('disconnect-btn');
    this.statusSpan = document.getElementById('connection-status');
    this.debugLog = document.getElementById('debug-log');

    // Create an audio element for bot's voice output
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    this.botAudio.playsInline = true;
    document.body.appendChild(this.botAudio);
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  setupEventListeners() {
    this.connectBtn.addEventListener('click', () => this.connect());
    this.disconnectBtn.addEventListener('click', () => this.disconnect());
  }

  /**
   * Add a timestamped message to the debug log
   */
  log(message) {
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;

    // Add styling based on message type
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3'; // blue for user
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50'; // green for bot
    }

    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  updateStatus(status) {
    this.statusSpan.textContent = status;
    this.log(`Status: ${status}`);
  }

  handleEventToConsole (evt) {
    this.log(`Received event: ${evt.action}`);
  };

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.dailyCallObject) return;

    this.dailyCallObject.on("joined-meeting", () => {
      this.updateStatus('Connected');
      this.connectBtn.disabled = true;
      this.disconnectBtn.disabled = false;
      this.log('Client connected');
    });
    this.dailyCallObject.on("track-started", (evt) => {
      if (evt.track.kind === "audio" && evt.participant.local === false) {
        this.log("Audio track started.")
        this.setupAudioTrack(evt.track);
      }
    });
    this.dailyCallObject.on("track-stopped", this.handleEventToConsole.bind(this));
    this.dailyCallObject.on("participant-joined", this.handleEventToConsole.bind(this));
    this.dailyCallObject.on("participant-updated", this.handleEventToConsole.bind(this));
    this.dailyCallObject.on("participant-left", () => {
      // When the bot leaves, we are also disconnecting from the call
      this.disconnect()
    });
    this.dailyCallObject.on("left-meeting", () => {
      this.updateStatus('Disconnected');
      this.connectBtn.disabled = false;
      this.disconnectBtn.disabled = true;
      this.log('Client disconnected');
    });
    this.dailyCallObject.on("error", this.handleEventToConsole.bind(this));
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  setupAudioTrack(track) {
    this.log(`Setting up audio track, track state: ${track.readyState}, muted: ${track.muted}`);

    // Check if we're already playing this track
    if (this.botAudio.srcObject) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    // Create a new MediaStream with the track and set it as the audio source
    this.botAudio.srcObject = new MediaStream([track]);
    this.botAudio.onplaying = async (event) => {
      this.log("onplaying")
      this.log("Will send the audio message to play the audio at the next tick")
      this.dailyCallObject.sendAppMessage("playable")
    }
  }

  async fetchRoomInfo() {
    let connectUrl = '/connect'
    let res = await fetch(connectUrl, {
      method: "POST",
      mode: "cors",
      headers: new Headers({
        "Content-Type": "application/json"
      }),
    })
    if (res.ok) {
      return res.json();
    }
  }

  /**
   * Initialize and connect to the bot
   * This sets up the RTVI client, initializes devices, and establishes the connection
   */
  async connect() {
    try {
      // Initialize the client
      this.dailyCallObject = Daily.createCallObject({
        subscribeToTracksAutomatically: true,
      });

      // Set up listeners for media track events
      this.setupTrackListeners();

      this.log('Creating the bot...');
      let roomInfo = await this.fetchRoomInfo()

      // Connect to the bot
      this.log('Connecting to bot...');
      // Only for making debugger easier
      window.callObject = this.dailyCallObject;
      await this.dailyCallObject.join({
        url: roomInfo.room_url,
      });

      this.log('Connection complete');
    } catch (error) {
      // Handle any errors during connection
      this.log(`Error connecting: ${error.message}`);
      this.log(`Error stack: ${error.stack}`);
      this.updateStatus('Error');

      // Clean up if there's an error
      if (this.dailyCallObject) {
        try {
          await this.dailyCallObject.leave();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError.message}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  async disconnect() {
    if (this.dailyCallObject) {
      try {
        // Disconnect the RTVI client
        await this.dailyCallObject.leave();
        await this.dailyCallObject.destroy();
        this.dailyCallObject = null;

        // Clean up audio
        if (this.botAudio.srcObject) {
          this.botAudio.srcObject.getTracks().forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }
      } catch (error) {
        this.log(`Error disconnecting: ${error.message}`);
      }
    }
  }
}

// Initialize the client when the page loads
window.addEventListener('DOMContentLoaded', () => {
  new ChatbotClient();
});



================================================
FILE: bot-ready-signalling/client/javascript/src/style.css
================================================
body {
  margin: 0;
  padding: 20px;
  font-family: Arial, sans-serif;
  background-color: #f0f0f0;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
}

.status-bar {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px;
  background-color: #fff;
  border-radius: 8px;
  margin-bottom: 20px;
}

.controls button {
  padding: 8px 16px;
  margin-left: 10px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

#connect-btn {
  background-color: #4caf50;
  color: white;
}

#disconnect-btn {
  background-color: #f44336;
  color: white;
}

button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.main-content {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 20px;
}

.bot-container {
  display: flex;
  flex-direction: column;
  align-items: center;
}

#bot-video-container {
  width: 640px;
  height: 360px;
  background-color: #e0e0e0;
  border-radius: 8px;
  margin: 20px auto;
  overflow: hidden;
  display: flex;
  align-items: center;
  justify-content: center;
}

#bot-video-container video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}

.debug-panel {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
}

.debug-panel h3 {
  margin: 0 0 10px 0;
  font-size: 16px;
  font-weight: bold;
}

#debug-log {
  height: 200px;
  overflow-y: auto;
  background-color: #f8f8f8;
  padding: 10px;
  border-radius: 4px;
  font-family: monospace;
  font-size: 12px;
  line-height: 1.4;
}



================================================
FILE: bot-ready-signalling/client/react-native/README.md
================================================
# React Native Implementation

Basic implementation using the [Pipecat React Native SDK](https://docs.pipecat.ai/client/react-native/introduction).

## Usage

### Expo requirements

This project cannot be used with an [Expo Go](https://docs.expo.dev/workflow/expo-go/) app because [it requires custom native code](https://docs.expo.io/workflow/customizing/).

When a project requires custom native code or a config plugin, we need to transition from using [Expo Go](https://docs.expo.dev/workflow/expo-go/) 
to a [development build](https://docs.expo.dev/development/introduction/).

More details about the custom native code used by this demo can be found in [rn-daily-js-expo-config-plugin](https://github.com/daily-co/rn-daily-js-expo-config-plugin).

### Building remotely

If you do not have experience with Xcode and Android Studio builds or do not have them installed locally on your computer, you will need to follow [this guide from Expo to use EAS Build](https://docs.expo.dev/development/create-development-builds/#create-and-install-eas-build).

### Building locally

You will need to have installed locally on your computer:
- [Xcode](https://developer.apple.com/xcode/) to build for iOS;
- [Android Studio](https://developer.android.com/studio) to build for Android;

#### Install the demo dependencies

```bash
# Use the version of node specified in .nvmrc
nvm i

# Install dependencies
npm i

# Before a native app can be compiled, the native source code must be generated.
npx expo prebuild

# Configure the environment variable to connect to the local server
cp env.example .env
# edit .env and add your local ip address, for example: http://192.168.1.16:7860
```

#### Running on Android

After plugging in an Android device [configured for debugging](https://developer.android.com/studio/debug/dev-options), run the following command:

```
npm run android
```

#### Running on iOS

Run the following command:

```
npm run ios
```

#### Connect to the server
Use the http://localhost:5173 in your app.



================================================
FILE: bot-ready-signalling/client/react-native/app.json
================================================
{
  "expo": {
    "name": "bot-ready-rn",
    "slug": "bot-ready-rn",
    "version": "1.0.0",
    "orientation": "portrait",
    "icon": "./assets/icon.png",
    "userInterfaceStyle": "light",
    "splash": {
      "image": "./assets/splash.png",
      "resizeMode": "contain",
      "backgroundColor": "#ffffff"
    },
    "updates": {
      "fallbackToCacheTimeout": 0
    },
    "assetBundlePatterns": [
      "**/*"
    ],
    "ios": {
      "supportsTablet": true,
      "bitcode": false,
      "bundleIdentifier": "co.daily.expo.BotReady",
      "infoPlist": {
        "UIBackgroundModes": [
          "voip"
        ]
      },
      "appleTeamId": "EEBGKV9N3N"
    },
    "android": {
      "adaptiveIcon": {
        "foregroundImage": "./assets/adaptive-icon.png",
        "backgroundColor": "#FFFFFF"
      },
      "package": "co.daily.expo.BotReady",
      "permissions": [
        "android.permission.ACCESS_NETWORK_STATE",
        "android.permission.BLUETOOTH",
        "android.permission.CAMERA",
        "android.permission.INTERNET",
        "android.permission.MODIFY_AUDIO_SETTINGS",
        "android.permission.RECORD_AUDIO",
        "android.permission.SYSTEM_ALERT_WINDOW",
        "android.permission.WAKE_LOCK",
        "android.permission.FOREGROUND_SERVICE",
        "android.permission.FOREGROUND_SERVICE_CAMERA",
        "android.permission.FOREGROUND_SERVICE_MICROPHONE",
        "android.permission.FOREGROUND_SERVICE_MEDIA_PROJECTION",
        "android.permission.POST_NOTIFICATIONS"
      ]
    },
    "web": {
      "favicon": "./assets/favicon.png"
    },
    "plugins": [
      "@config-plugins/react-native-webrtc",
      "@daily-co/config-plugin-rn-daily-js",
      [
        "expo-build-properties",
        {
          "android": {
            "minSdkVersion": 24,
            "compileSdkVersion": 35,
            "targetSdkVersion": 34,
            "buildToolsVersion": "35.0.0"
          },
          "ios": {
            "deploymentTarget": "15.1"
          }
        }
      ]
    ]
  }
}



================================================
FILE: bot-ready-signalling/client/react-native/babel.config.js
================================================
module.exports = function(api) {
  api.cache(true);
  return {
    presets: ['babel-preset-expo'],
    plugins: [["module:react-native-dotenv"]],
  };
};



================================================
FILE: bot-ready-signalling/client/react-native/env.example
================================================
API_BASE_URL=http://YOUR_LOCAL_IP:7860



================================================
FILE: bot-ready-signalling/client/react-native/index.js
================================================
import { registerRootComponent } from "expo";

import App from "./src/App";

// registerRootComponent calls AppRegistry.registerComponent('main', () => App);
// It also ensures that the environment is set up appropriately
registerRootComponent(App);



================================================
FILE: bot-ready-signalling/client/react-native/metro.config.js
================================================
// Learn more https://docs.expo.io/guides/customizing-metro
const { getDefaultConfig } = require('expo/metro-config');

module.exports = getDefaultConfig(__dirname);



================================================
FILE: bot-ready-signalling/client/react-native/package.json
================================================
{
  "name": "bot-ready-rn",
  "version": "1.0.0",
  "scripts": {
    "start": "expo start --dev-client",
    "android": "expo run:android --device",
    "ios": "expo run:ios --device",
    "web": "expo start --web"
  },
  "dependencies": {
    "@config-plugins/react-native-webrtc": "^10.0.0",
    "@daily-co/config-plugin-rn-daily-js": "0.0.7",
    "@daily-co/react-native-daily-js": "^0.70.0",
    "@daily-co/react-native-webrtc": "^118.0.3-daily.2",
    "@react-native-async-storage/async-storage": "1.23.1",
    "expo": "^52.0.0",
    "expo-build-properties": "~0.13.1",
    "expo-dev-client": "~5.0.5",
    "expo-splash-screen": "~0.29.16",
    "expo-status-bar": "~2.0.0",
    "react": "18.3.1",
    "react-native": "0.76.3",
    "react-native-background-timer": "^2.4.1",
    "react-native-dotenv": "^3.4.11",
    "react-native-get-random-values": "^1.11.0"
  },
  "devDependencies": {
    "@babel/core": "^7.12.9"
  },
  "private": true
}



================================================
FILE: bot-ready-signalling/client/react-native/.nvmrc
================================================
22.14



================================================
FILE: bot-ready-signalling/client/react-native/src/App.js
================================================
import React, { useState, useEffect } from 'react';
import {SafeAreaView, View, Text, Button, StyleSheet, ScrollView} from 'react-native';
import Daily from "@daily-co/react-native-daily-js";
import { API_BASE_URL } from "@env";

const CallScreen = () => {
  const [connectionStatus, setConnectionStatus] = useState('Disconnected');
  const [isConnected, setIsConnected] = useState(false);
  const [callObject, setCallObject] = useState(null);
  const [logs, setLogs] = useState([]);

  useEffect(() => {
    if (callObject) {
      setupTrackListeners(callObject);
    }
  }, [callObject]);

  const log = (message) => {
    setLogs((prevLogs) => [...prevLogs, `${new Date().toISOString()} - ${message}`]);
    console.log(message);
  };

  const setupTrackListeners = (callObject) => {
    callObject.on("joined-meeting", () => {
      setConnectionStatus('Connected');
      setIsConnected(true);
      log('Client connected');
    });
    callObject.on("left-meeting", () => {
      setConnectionStatus('Disconnected');
      setIsConnected(false);
      log('Client disconnected');
    });
    callObject.on("participant-left", () => {
      // When the bot leaves, we are also disconnecting from the call
      disconnect().catch((err) => {
        log(`Failed to disconnect ${err}`);
      })
    });
    // Trigger so the bot can start sending audio
    callObject.on("track-started", (evt) => {
      if (evt.track.kind === "audio" && evt.participant.local === false) {
        handleEventToConsole(evt)
        log("Sending the message that will trigger the bot to play the audio.")
        callObject.sendAppMessage("playable")
      }
    });
    callObject.on("error", (evt) => log(`Error: ${evt.error}`));
    // Other events just for awareness
    callObject.on("track-stopped", handleEventToConsole);
    callObject.on("participant-joined", handleEventToConsole);
    callObject.on("participant-updated", handleEventToConsole);
  };

  const handleEventToConsole = (evt) => {
    log(`Received event: ${evt.action}`);
  };

  const connect = async () => {
    try {
      const callObject = Daily.createCallObject({ subscribeToTracksAutomatically: true });
      setCallObject(callObject);
      const connectionUrl = `${API_BASE_URL}/connect`
      const res = await fetch(connectionUrl, { method: "POST", headers: { "Content-Type": "application/json" } });
      const roomInfo = await res.json();
      await callObject.join({ url: roomInfo.room_url });
    } catch (error) {
      log(`Error connecting: ${error.message}`);
    }
  };

  const disconnect = async () => {
    if (callObject) {
      try {
        await callObject.leave();
        await callObject.destroy();
        setCallObject(null);
      } catch (error) {
        log(`Error disconnecting: ${error.message}`);
      }
    }
  };

  return (
      <SafeAreaView style={styles.safeArea}>
        <View style={styles.container}>
          <View style={styles.statusBar}>
            <Text>Status: <Text style={styles.status}>{connectionStatus}</Text></Text>
            <View style={styles.controls}>
              <Button
                title={isConnected ? "Disconnect" : "Connect"}
                onPress={isConnected ? disconnect : connect}
              />
            </View>
          </View>

          <View style={styles.debugPanel}>
            <Text style={styles.debugTitle}>Debug Info</Text>
            <ScrollView style={styles.debugLog}>
              {logs.map((logEntry, index) => (
                  <Text key={index} style={styles.logText}>{logEntry}</Text>
              ))}
            </ScrollView>
          </View>
        </View>
      </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  safeArea: { flex: 1, backgroundColor: '#f0f0f0', padding: 20 },
  container: { flex: 1, margin: 20 },
  statusBar: { flexDirection: 'row', justifyContent: 'space-between', alignItems: 'center', padding: 10, backgroundColor: '#fff', borderRadius: 8, marginBottom: 20 },
  status: { fontWeight: 'bold' },
  controls: { flexDirection: 'row', gap: 10 },
  debugPanel: { height: '80%', backgroundColor: '#fff', borderRadius: 8, padding: 20},
  debugTitle: { fontSize: 16, fontWeight: 'bold' },
  debugLog: { height: '100%', overflow: 'scroll', backgroundColor: '#f8f8f8', padding: 10, borderRadius: 4, fontFamily: 'monospace', fontSize: 12, lineHeight: 1.4 },
});

export default CallScreen;



================================================
FILE: bot-ready-signalling/server/README.md
================================================
# Bot ready signaling Server

A FastAPI server that manages bot instances and provide endpoint for Pipecat client connections.

## Endpoints

- `POST /connect` - Pipecat client connection endpoint

## Environment Variables

Copy `env.example` to `.env` and configure:

```ini
# Required API Keys
DAILY_API_KEY=           # Your Daily API key
CARTESIA_API_KEY=        # Your Cartesia API key

# Optional Configuration
DAILY_API_URL=           # Optional: Daily API URL (defaults to https://api.daily.co/v1)
DAILY_SAMPLE_ROOM_URL=   # Optional: Fixed room URL for development
HOST=                    # Optional: Host address (defaults to 0.0.0.0)
FAST_API_PORT=           # Optional: Port number (defaults to 7860)
```

## Running the Server

Set up and activate your virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

If you want to use the local version of `pipecat` in this repo rather than the last published version, also run:

```bash
pip install --editable "../../../[daily,cartesia,openai]"
```

Run the server:

```bash
python server.py
```



================================================
FILE: bot-ready-signalling/server/env.example
================================================
DAILY_SAMPLE_ROOM_URL=https://yourdomain.daily.co/yourroom # (for joining the bot to the same room repeatedly for local dev)
DAILY_API_KEY=
CARTESIA_API_KEY=


================================================
FILE: bot-ready-signalling/server/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
pipecat-ai[daily,cartesia,openai]



================================================
FILE: bot-ready-signalling/server/runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os
from typing import Optional

import aiohttp
from pipecat.transports.daily.utils import DailyRESTHelper


async def configure(aiohttp_session: aiohttp.ClientSession):
    (url, token, _) = await configure_with_args(aiohttp_session)
    return (url, token)


async def configure_with_args(
    aiohttp_session: aiohttp.ClientSession, parser: Optional[argparse.ArgumentParser] = None
):
    if not parser:
        parser = argparse.ArgumentParser(description="Daily AI SDK Bot Sample")
    parser.add_argument(
        "-u", "--url", type=str, required=False, help="URL of the Daily room to join"
    )
    parser.add_argument(
        "-k",
        "--apikey",
        type=str,
        required=False,
        help="Daily API Key (needed to create an owner token for the room)",
    )

    args, unknown = parser.parse_known_args()

    url = args.url or os.getenv("DAILY_SAMPLE_ROOM_URL")
    key = args.apikey or os.getenv("DAILY_API_KEY")

    if not url:
        raise Exception(
            "No Daily room specified. use the -u/--url option from the command line, or set DAILY_SAMPLE_ROOM_URL in your environment to specify a Daily room URL."
        )

    if not key:
        raise Exception(
            "No Daily API key specified. use the -k/--apikey option from the command line, or set DAILY_API_KEY in your environment to specify a Daily API key, available from https://dashboard.daily.co/developers."
        )

    daily_rest_helper = DailyRESTHelper(
        daily_api_key=key,
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )

    # Create a meeting token for the given room with an expiration 1 hour in
    # the future.
    expiry_time: float = 60 * 60

    token = await daily_rest_helper.get_token(url, expiry_time)

    return (url, token, args)



================================================
FILE: bot-ready-signalling/server/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os
import subprocess
from contextlib import asynccontextmanager
from typing import Any, Dict

import aiohttp
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pipecat.transports.daily.utils import DailyRESTHelper, DailyRoomParams

# Load environment variables from .env file
load_dotenv(override=True)

# Dictionary to track bot processes: {pid: (process, room_url)}
bot_procs = {}

# Store Daily API helpers
daily_helpers = {}


def cleanup():
    """Cleanup function to terminate all bot processes.

    Called during server shutdown.
    """
    for entry in bot_procs.values():
        proc = entry[0]
        proc.terminate()
        proc.wait()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan manager that handles startup and shutdown tasks.

    - Creates aiohttp session
    - Initializes Daily API helper
    - Cleans up resources on shutdown
    """
    aiohttp_session = aiohttp.ClientSession()
    daily_helpers["rest"] = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )
    yield
    await aiohttp_session.close()
    cleanup()


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


async def create_room_and_token() -> tuple[str, str]:
    """Helper function to create a Daily room and generate an access token.

    Returns:
        tuple[str, str]: A tuple containing (room_url, token)

    Raises:
        HTTPException: If room creation or token generation fails
    """
    room = await daily_helpers["rest"].create_room(DailyRoomParams())
    if not room.url:
        raise HTTPException(status_code=500, detail="Failed to create room")

    token = await daily_helpers["rest"].get_token(room.url)
    if not token:
        raise HTTPException(status_code=500, detail=f"Failed to get token for room: {room.url}")

    return room.url, token


@app.post("/connect")
async def bot_connect(request: Request) -> Dict[Any, Any]:
    """Connect endpoint that creates a room and returns connection credentials.

    This endpoint is called by client to establish a connection.

    Returns:
        Dict[Any, Any]: Authentication bundle containing room_url and token

    Raises:
        HTTPException: If room creation, token generation, or bot startup fails
    """
    print("Creating room for RTVI connection")
    room_url, token = await create_room_and_token()
    print(f"Room URL: {room_url}")

    # Start the bot process
    try:
        bot_file = "signalling_bot"
        proc = subprocess.Popen(
            [f"python3 -m {bot_file} -u {room_url} -t {token}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )
        bot_procs[proc.pid] = (proc, room_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    # Return the authentication bundle in format expected by DailyTransport
    return {"room_url": room_url, "token": token}


if __name__ == "__main__":
    import uvicorn

    # Parse command line arguments for server configuration
    default_host = os.getenv("HOST", "0.0.0.0")
    default_port = int(os.getenv("FAST_API_PORT", "7860"))

    parser = argparse.ArgumentParser(description="Daily Travel Companion FastAPI server")
    parser.add_argument("--host", type=str, default=default_host, help="Host address")
    parser.add_argument("--port", type=int, default=default_port, help="Port number")
    parser.add_argument("--reload", action="store_true", help="Reload code on change")

    config = parser.parse_args()

    # Start the FastAPI server
    uvicorn.run(
        "server:app",
        host=config.host,
        port=config.port,
        reload=config.reload,
    )



================================================
FILE: bot-ready-signalling/server/signalling_bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import os
import sys
from dataclasses import dataclass

import aiohttp
from dotenv import load_dotenv
from loguru import logger
from pipecat.frames.frames import AudioRawFrame, EndFrame, OutputAudioRawFrame, TTSSpeakFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineTask
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from runner import configure

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


@dataclass
class SilenceFrame(OutputAudioRawFrame):
    def __init__(
        self,
        *,
        sample_rate: int,
        duration: float,
    ):
        # Initialize the parent class with the silent frame's data
        super().__init__(
            audio=self.create_silent_audio_frame(sample_rate, 1, duration).audio,
            sample_rate=sample_rate,
            num_channels=1,
        )

    @staticmethod
    def create_silent_audio_frame(
        sample_rate: int, num_channels: int, duration: float
    ) -> AudioRawFrame:
        """Create an AudioRawFrame containing silence."""
        frame_size = num_channels * 2  # 2 bytes per sample for 16-bit audio
        total_frames = int(sample_rate * duration)
        total_bytes = total_frames * frame_size
        silent_audio = bytes(total_bytes)  # Create a byte array filled with zeros
        return AudioRawFrame(audio=silent_audio, sample_rate=sample_rate, num_channels=num_channels)


async def main():
    async with aiohttp.ClientSession() as session:
        (room_url, _) = await configure(session)

        transport = DailyTransport(
            room_url, None, "Say One Thing", DailyParams(audio_out_enabled=True)
        )

        tts = CartesiaTTSService(
            api_key=os.getenv("CARTESIA_API_KEY"),
            voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
        )

        runner = PipelineRunner()

        task = PipelineTask(Pipeline([tts, transport.output()]))

        # Register an event handler so we can play the audio when we receive a specific message
        @transport.event_handler("on_app_message")
        async def on_app_message(transport, message, sender):
            logger.debug(f"Received app message: {message} - {sender}")
            if "playable" not in message:
                return
            await task.queue_frames(
                [
                    SilenceFrame(
                        sample_rate=task.params.audio_out_sample_rate,
                        duration=0.5,
                    ),
                    TTSSpeakFrame(f"Hello there, how are you doing today ?"),
                    EndFrame(),
                ]
            )

        await runner.run(task)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: code-helper/README.md
================================================
# code-helper

This example demonstrates using `LLMTextProcessor` to categorize the LLM's
output text so that the client can easily render different types of output
accordingly, while the TTS speaks these same types in separate but also
custom ways, spelling out credit card numbers, while skipping trying to
read out code snippets or not saying the 'https' part of a url.

This example also includes a text entry box in the client to show how the
bot handles text input and can respond either with audio or not and the
categorization and "bot output" continues seemlessly.

The client in this example will render the user and bot transcripts using
simply the `user-transcript` and `bot-output` messages. The bot output will
render each sentence and then highlight each word as it is said. All code
provided by the bot will be highlighted as such and links will be formatted.

Concepts this example is meant to demonstrate:
- Custom handling of LLM text output for different purposes:
  - For the purpose of having the TTS skip certain outputs or speak certain
    outputs differently
  - For the purpose of supporting a client UI for easier rendering of
    different types of text or for altering for filtering out text before
    sending it to the client.
- Client <-> Bot Communication with RTVI
- Tool calling for sensitive information and custom handling of that
  information for TTS and RTVI purposes.
- Client->Server Text input

## Configuration

- **Bot Type**: Web
- **Transport(s)**: SmallWebRTC
- **Pipeline**: Cascade
  - **STT**: Deepgram
  - **LLM**: OpenAI
  - **TTS**: ElevenLabs

## Setup

### Server

1. **Navigate to server directory**:

   ```bash
   cd server
   ```

2. **Install dependencies**:

   ```bash
   uv sync
   ```

3. **Configure environment variables**:

   ```bash
   cp .env.example .env
   # Edit .env and add your API keys
   ```

4. **Run the bot**:

   - SmallWebRTC: `uv run bot.py`

### Client

1. **Navigate to client directory**:

   ```bash
   cd client
   ```

2. **Install dependencies**:

   ```bash
   npm install
   ```

3. **Run development server**:

   ```bash
   npm run dev
   ```

4. **Open browser**:

   http://localhost:5173

## Project Structure

```
code-helper/
├── server/              # Python bot server
│   ├── bot.py           # Main bot implementation
│   ├── pyproject.toml   # Python dependencies
│   ├── .env.example     # Environment variables template
│   ├── .env             # Your API keys (git-ignored)
│   └── ...
├── client/              # Vanilla application
│   ├── src/             # Client source code
│   ├── package.json     # Node dependencies
│   └── ...
├── .gitignore           # Git ignore patterns
└── README.md            # This file
```
## Observability

This project includes observability tools to help you debug and monitor your bot:

## Learn More

- [Pipecat Documentation](https://docs.pipecat.ai/)
- [Pipecat GitHub](https://github.com/pipecat-ai/pipecat)
- [Pipecat Examples](https://github.com/pipecat-ai/pipecat-examples)
- [Discord Community](https://discord.gg/pipecat)


================================================
FILE: code-helper/client/index.html
================================================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice UI Kit - Vanilla JS</title>
  </head>

  <body>
    <div class="container">
      <!-- Header with controls -->
      <div class="header">
        <div class="transport-selector">
          <label for="transport-select">Transport:</label>
          <select id="transport-select">
            <option value="smallwebrtc">SmallWebRTC</option>
            <option value="daily">Daily</option>
          </select>
        </div>
        <div class="controls">
          <button id="mic-btn" class="control-btn" disabled>
            🎤 <span id="mic-status">Mic is Off</span>
          </button>
          <button id="connect-btn" class="connect-btn">Connect</button>
        </div>
      </div>

      <!-- Conversation area -->
      <div class="conversation-panel">
        <h3>Conversation</h3>
        <div id="conversation-log"></div>
      </div>

      <!-- Text entry area -->
      <div class="text-entry">
        <h3>Text Entry</h3>
        <div class="entry-area">
          <textarea
            id="user-input"
            placeholder="Type your message here..."></textarea>
          <button id="send-btn" class="send-btn" disabled>Send</button>
        </div>
        <div class="audio-response-row">
          <input type="checkbox" id="audio-response" />
          <label for="audio-response">Respond with audio</label>
        </div>
      </div>

      <!-- Events panel -->
      <div class="events-panel">
        <h3>Events</h3>
        <div id="events-log"></div>
      </div>
    </div>

    <script type="module" src="/src/app.js"></script>
    <link rel="stylesheet" href="/src/style.css" />
  </body>
</html>



================================================
FILE: code-helper/client/package.json
================================================
{
  "name": "code-helper",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.5.0",
    "@pipecat-ai/daily-transport": "^1.5.0",
    "@pipecat-ai/small-webrtc-transport": "^1.8.0",
    "highlight.js": "^11.11.1"
  },
  "devDependencies": {
    "vite": "^7.1.7"
  }
}



================================================
FILE: code-helper/client/src/app.js
================================================
import {
  AggregationType,
  PipecatClient,
  RTVIEvent,
} from '@pipecat-ai/client-js';
import {
  AVAILABLE_TRANSPORTS,
  DEFAULT_TRANSPORT,
  TRANSPORT_CONFIG,
  createTransport,
} from './config';

import hljs from 'highlight.js/lib/core';
import 'highlight.js/styles/dark.css';
import javascript from 'highlight.js/lib/languages/javascript';
import python from 'highlight.js/lib/languages/python';
hljs.registerLanguage('javascript', javascript);
hljs.registerLanguage('python', python);

class VoiceChatClient {
  constructor() {
    this.client = null;
    this.transportType = DEFAULT_TRANSPORT;
    this.isConnected = false;

    this.setupDOM();
    this.setupEventListeners();
    this.addEvent('initialized', 'Client initialized');
  }

  setupDOM() {
    this.transportSelect = document.getElementById('transport-select');
    this.connectBtn = document.getElementById('connect-btn');
    this.micBtn = document.getElementById('mic-btn');
    this.micStatus = document.getElementById('mic-status');
    this.conversationLog = document.getElementById('conversation-log');
    this.eventsLog = document.getElementById('events-log');
    this.lastConversationBubble = null;
    this.botSpans = [];
    this.curBotSpan = -1;
    this.sendBtn = document.getElementById('send-btn');

    // Populate transport selector with available transports
    this.transportSelect.innerHTML = '';
    AVAILABLE_TRANSPORTS.forEach((transport) => {
      const option = document.createElement('option');
      option.value = transport;
      option.textContent =
        transport.charAt(0).toUpperCase() + transport.slice(1);
      if (transport === 'smallwebrtc') {
        option.textContent = 'SmallWebRTC';
      } else if (transport === 'daily') {
        option.textContent = 'Daily';
      }
      this.transportSelect.appendChild(option);
    });

    // Hide transport selector if only one transport
    if (AVAILABLE_TRANSPORTS.length === 1) {
      this.transportSelect.parentElement.style.display = 'none';
    }

    // Add placeholder message
    this.addConversationMessage(
      'Connect to start talking with your bot',
      'placeholder'
    );
  }

  setupEventListeners() {
    // Listen and log transport state changes
    this.transportSelect.addEventListener('change', (e) => {
      this.transportType = e.target.value;
      this.addEvent('transport-changed', this.transportType);
    });

    // Setup connect button for connecting/disconnecting
    this.connectBtn.addEventListener('click', () => {
      if (this.isConnected) {
        this.disconnect();
      } else {
        this.connect();
      }
    });

    // Setup mic button for muting/unmuting
    this.micBtn.addEventListener('click', () => {
      if (this.client) {
        const newState = !this.client.isMicEnabled;
        this.client.enableMic(newState);
        this.updateMicButton(newState);
      }
    });

    // Handle sending text input to LLM
    const userInput = document.getElementById('user-input');
    const sendTextToLLM = () => {
      if (this.client && this.isConnected) {
        const text = userInput.value.trim();
        const audioResponse = document.getElementById('audio-response').checked;
        if (text.length > 0) {
          this.client.sendText(text, { audio_response: audioResponse });
          this.addConversationMessage(text, 'user');
        }
        userInput.value = '';
      }
    };

    this.sendBtn.addEventListener('click', sendTextToLLM);

    // Also handle Enter key in the input
    userInput.addEventListener('keyup', (e) => {
      if (e.key === 'Enter') {
        sendTextToLLM();
      }
    });
  }

  async connect() {
    try {
      this.addEvent('connecting', `Using ${this.transportType} transport`);

      // Create transport using config
      const transport = await createTransport(this.transportType);

      // Create client
      this.client = new PipecatClient({
        transport,
        enableMic: true,
        enableCam: false,
        callbacks: {
          onConnected: () => {
            this.onConnected();
          },
          onDisconnected: () => {
            this.onDisconnected();
          },
          onTransportStateChanged: (state) => {
            this.addEvent('transport-state', state);
          },
          onBotReady: () => {
            this.addEvent('bot-ready', 'Bot is ready to talk');
          },
          onUserTranscript: (data) => {
            if (data.final) {
              this.addConversationMessage(data.text, 'user');
            }
          },
          onBotOutput: (data) => {
            // Check the aggregation type. If WORD, embolden the word already rendered
            // in the bot transcript. Otherwise, add to the latest bot message or start
            // a new one.
            if (data.aggregated_by === AggregationType.WORD) {
              this.emboldenBotWord(data.text);
              return;
            } else {
              this.addConversationMessage(data.text, 'bot', data.aggregated_by);
            }
          },
          onError: (error) => {
            this.addEvent('error', error.message);
          },
        },
      });
      window.client = this; // For debugging

      // Setup audio
      this.setupAudio();

      // Connect using config
      const connectParams = TRANSPORT_CONFIG[this.transportType];
      await this.client.connect(connectParams);
    } catch (error) {
      this.addEvent('error', error.message);
      console.error('Connection error:', error);
    }
  }

  async disconnect() {
    if (this.client) {
      await this.client.disconnect();
    }
  }

  setupAudio() {
    // Listen for bot audio tracks and play them
    this.client.on(RTVIEvent.TrackStarted, (track, participant) => {
      if (!participant?.local && track.kind === 'audio') {
        this.addEvent('track-started', 'Bot audio track');
        const audio = document.createElement('audio');
        audio.autoplay = true;
        audio.srcObject = new MediaStream([track]);
        document.body.appendChild(audio);
      }
    });
  }

  onConnected() {
    // Update UI on connection
    this.isConnected = true;
    this.connectBtn.textContent = 'Disconnect';
    this.connectBtn.classList.add('disconnect');
    this.micBtn.disabled = false;
    this.sendBtn.disabled = false;
    this.transportSelect.disabled = true;
    this.updateMicButton(this.client.isMicEnabled);
    this.addEvent('connected', 'Successfully connected to bot');

    // Clear placeholder
    if (this.conversationLog.querySelector('.placeholder')) {
      this.conversationLog.innerHTML = '';
    }
  }

  onDisconnected() {
    // Update UI on disconnection
    this.isConnected = false;
    this.connectBtn.textContent = 'Connect';
    this.connectBtn.classList.remove('disconnect');
    this.micBtn.disabled = true;
    this.sendBtn.disabled = true;
    this.transportSelect.disabled = false;
    this.updateMicButton(false);
    this.addEvent('disconnected', 'Disconnected from bot');
  }

  updateMicButton(enabled) {
    // Update the microphone button UI based on whether the mic is enabled
    this.micStatus.textContent = enabled ? 'Mic is On' : 'Mic is Off';
    this.micBtn.style.backgroundColor = enabled ? '#10b981' : '#1f2937';
  }

  emboldenBotWord(word) {
    // This method does it's best to find the word provided in the rendered bot
    // transcript and embolden it. It keeps track of which bubble and index
    // it's at to avoid searching from the start each time. It simply looks for
    // the next occurrence of the word in the current bubble and emboldens all the
    // text up to that word. This means it may fail if the word does not
    // match exactly what was rendered (e.g., punctuation, casing, etc), but
    // it's a best effort.
    if (this.curBotSpan < 0) return;
    const curSpan = this.botSpans[this.curBotSpan];
    if (!curSpan) return;
    // Get the inner HTML without <strong> tags
    const spanInnards = curSpan.innerHTML.replace(/<\/?strong>/g, '');
    // Split into already spoken (and emboldened) and yet to be spoken (and emboldened)
    const alreadyEmboldened = spanInnards.slice(0, this.lastBotWordIndex || 0);
    const yetToEmbolden = spanInnards.slice(this.lastBotWordIndex || 0);

    // For the yet to embolden part, find the next occurrence of the word
    const wordIndex = yetToEmbolden.indexOf(word);
    if (wordIndex === -1) {
      // If the word is not found, we may have finished this span
      // move to the next span if available
      if (this.botSpans.length > this.curBotSpan + 1) {
        // Once we complete a span, mark it as spoken. This removes the need
        // for inserting <strong> tags and simplifies the innerHTML.
        curSpan.innerHTML = spanInnards;
        curSpan.classList.add('spoken');

        // Move to next bubble
        this.curBotSpan = this.curBotSpan + 1;
        this.lastBotWordIndex = 0;
        // Try again with the next span
        this.emboldenBotWord(word);
        return;
      }
      return;
    }
    // Replace the first occurrence of the word with word</strong>
    // Use word boundaries to match the whole word
    const replaced = yetToEmbolden.replace(word, `${word}</strong>`);

    // Update the inner HTML so that <strong> wraps all text up until
    // and including the current word
    curSpan.innerHTML = '<strong>' + alreadyEmboldened + replaced;
    // Scroll to bottom
    this.conversationLog.scrollTop = this.conversationLog.scrollHeight;

    // Update lastBotWordIndex
    this.lastBotWordIndex =
      (this.lastBotWordIndex || 0) + wordIndex + word.length;
  }

  // Create a new element to add to the bot bubble based on aggregation type
  createBotBubbleElement(text, type) {
    let newElement;
    switch (type) {
      case 'code':
        {
          // Create a code block with syntax highlighting
          newElement = document.createElement('pre');
          const codeDiv = document.createElement('code');
          codeDiv.textContent = text;
          hljs.highlightElement(codeDiv);
          newElement.appendChild(codeDiv);
        }
        break;
      case 'link':
        {
          // Create a link element
          newElement = document.createElement('div');
          const link = document.createElement('a');
          link.href = text;
          link.textContent = text;
          link.target = '_blank';
          newElement.appendChild(link);
        }
        break;
      default:
        {
          // All other text is rendered in a simple span and new lines are converted to <br>
          newElement = document.createElement('span');
          text = text.trim();
          // We add spaces around the <br> to ensure we don't break our emboldening logic
          newElement.innerHTML = text.replace(/\n/g, ' <br> ');
          this.botSpans.push(newElement);
          if (this.curBotSpan === -1) {
            this.curBotSpan = 0;
          }
        }
        break;
    }
    // Attach the aggregation type for later reference
    newElement.type = type;
    return newElement;
  }

  // Add text to the last bubble, handling different types appropriately
  addToLastBubble(text, role, type) {
    const appendText = (element, text) => {
      text = text.trim();
      element.innerHTML += ' ' + text.replace(/\n/g, ' <br> ');
    };
    const typeIsText = (t) => {
      return !['code', 'link'].includes(t);
    };

    if (role === 'user') {
      // If the role is user, always simply append the text and return.
      // There is no special rendering for user messages.
      appendText(this.lastConversationBubble, text);
      return;
    }

    // For bot messages, if the last element is text and the new type is also text,
    // we can simply append to it.
    const lastChild = this.lastConversationBubble.lastChild;
    if (lastChild && typeIsText(lastChild.type) && typeIsText(type)) {
      appendText(lastChild, text);
      return;
    }
    // If we're here, then the text is part of the bot transcript and either not
    // text or a different type than the last element. Create a new element to add
    // to the bot transcript bubble.
    this.lastConversationBubble.appendChild(
      this.createBotBubbleElement(text, type)
    );
  }

  // Entry point for adding text to the conversation log
  addConversationMessage(text, role, type = AggregationType.SENTENCE) {
    // If the role changes, create a new bubble. Otherwise, add to the last bubble.
    if (this.lastConversationBubble?.role === role) {
      this.addToLastBubble(text, role, type);
    } else {
      this.createConversationBubble(text, role, type);
    }
  }

  // Create a new conversation bubble along with its initial text
  createConversationBubble(text, role, type) {
    const messageDiv = document.createElement('div');
    messageDiv.className = `conversation-message ${role} ${type}`;
    this.lastConversationBubble = messageDiv;
    this.lastConversationBubble.role = role;

    if (role === 'placeholder') {
      messageDiv.textContent = text;
    } else {
      this.addToLastBubble(text, role, type);
    }

    this.conversationLog.appendChild(messageDiv);
    this.conversationLog.scrollTop = this.conversationLog.scrollHeight;
  }

  // The client UI also has an event log for debugging and observability.
  // The method below adds entries to that log.
  addEvent(eventName, data) {
    const eventDiv = document.createElement('div');
    eventDiv.className = 'event-entry';

    const timestamp = new Date().toLocaleTimeString();
    const timestampSpan = document.createElement('span');
    timestampSpan.className = 'timestamp';
    timestampSpan.textContent = timestamp;

    const nameSpan = document.createElement('span');
    nameSpan.className = 'event-name';
    nameSpan.textContent = eventName;

    const dataSpan = document.createElement('span');
    dataSpan.className = 'event-data';
    dataSpan.textContent =
      typeof data === 'string' ? data : JSON.stringify(data);

    eventDiv.appendChild(timestampSpan);
    eventDiv.appendChild(nameSpan);
    eventDiv.appendChild(dataSpan);

    this.eventsLog.appendChild(eventDiv);
    this.eventsLog.scrollTop = this.eventsLog.scrollHeight;
  }
}

// Initialize when DOM is loaded
window.addEventListener('DOMContentLoaded', () => {
  new VoiceChatClient();
});



================================================
FILE: code-helper/client/src/config.js
================================================
/**
 * Project configuration
 * This file is auto-generated by pipecat-cli
 */

export const AVAILABLE_TRANSPORTS = ['smallwebrtc'];

export const DEFAULT_TRANSPORT = 'smallwebrtc';

export const TRANSPORT_CONFIG = {
  daily: { endpoint: 'http://localhost:7860/start' },
  smallwebrtc: {
    webrtcRequestParams: { endpoint: 'http://localhost:7860/api/offer' },
  },
};

/**
 * Create a transport instance based on the transport type
 * Uses dynamic imports to only load the required transport library
 */
export async function createTransport(transportType) {
  switch (transportType) {
    case 'smallwebrtc': {
      const { SmallWebRTCTransport } = await import(
        '@pipecat-ai/small-webrtc-transport'
      );
      return new SmallWebRTCTransport();
    }

    default:
      throw new Error(`Unsupported transport type: ${transportType}`);
  }
}



================================================
FILE: code-helper/client/src/style.css
================================================
* {
  box-sizing: border-box;
}

body {
  margin: 0;
  padding: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  background-color: #000;
  color: #e5e7eb;
  height: 100vh;
  overflow: hidden;
}

.container {
  display: flex;
  flex-direction: column;
  width: 100%;
  height: 100vh;
}

/* Header */
.header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 1rem;
  border-bottom: 1px solid #333;
}

.transport-selector {
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.transport-selector label {
  font-weight: 500;
}

#transport-select {
  padding: 0.5rem 1rem;
  background-color: #1f2937;
  color: #e5e7eb;
  border: 1px solid #374151;
  border-radius: 0.375rem;
  cursor: pointer;
  font-size: 0.875rem;
}

#transport-select:focus {
  outline: none;
  border-color: #10b981;
}

.controls {
  display: flex;
  gap: 0.5rem;
  align-items: center;
}

.control-btn,
.connect-btn,
.send-btn {
  padding: 0.5rem 1rem;
  border: none;
  border-radius: 0.375rem;
  font-size: 0.875rem;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.control-btn {
  background-color: #1f2937;
  color: #e5e7eb;
  border: 1px solid #374151;
}

.control-btn:hover:not(:disabled) {
  background-color: #374151;
}

.control-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.connect-btn {
  background-color: #10b981;
  color: #000;
}

.connect-btn:hover {
  background-color: #059669;
}

.connect-btn.disconnect {
  background-color: #ef4444;
  color: #fff;
}

.connect-btn.disconnect:hover {
  background-color: #dc2626;
}

/* Conversation Panel */
.conversation-panel {
  flex: 1;
  padding: 1rem;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.conversation-panel h3 {
  margin: 0 0 1rem 0;
  font-size: 1rem;
  font-weight: 600;
}

#conversation-log {
  flex: 1;
  overflow-y: auto;
  background-color: #111;
  border: 1px solid #333;
  border-radius: 0.5rem;
  padding: 1rem;
  font-size: 0.875rem;
  line-height: 1.5;
}

.conversation-message {
  margin-bottom: 0.75rem;
  padding: 0.5rem;
  border-radius: 0.25rem;
}

.conversation-message.user {
  background-color: #1e40af;
  color: #e0e7ff;
}

.conversation-message.bot {
  background-color: #065f46;
  color: #d1fae5;
}

span.spoken {
  font-weight: bold;
}

.conversation-message pre {
  border-radius: 0.4rem;
  overflow-x: auto;
}

.conversation-message .role {
  font-weight: 600;
  margin-bottom: 0.25rem;
}

.conversation-message.placeholder {
  color: #6b7280;
  font-style: italic;
  background-color: transparent;
}

/* Text Entry Area */
.text-entry {
  display: flex;
  flex-direction: column;
  padding: 1rem;
  border-top: 1px solid #333;
  gap: 0.5rem;
}

.text-entry h3 {
  margin: 1rem 0;
  font-size: 1rem;
  font-weight: 600;
}

.entry-area {
  display: flex;
  gap: 0.5rem;
}

.text-entry textarea {
  flex: 1;
  padding: 0.75rem;
  border: 1px solid #333;
  border-radius: 0.375rem;
  background-color: #111;
  color: #e5e7eb;
  font-size: 0.875rem;
  resize: none;
  height: 3rem;
}

.send-btn {
  background-color: #1f2937;
  color: #e5e7eb;
  border: 1px solid #374151;
}

.send-btn:hover:not(:disabled) {
  background-color: #374151;
}

.send-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.audio-response-row {
  display: flex;
  gap: 0.5rem;
  margin-top: 0.5rem;
}

.audio-response-row label {
  font-size: 0.875rem;
}

/* Events Panel */
.events-panel {
  height: 256px;
  padding: 0 1rem 1rem 1rem;
  border-top: 1px solid #333;
}

.events-panel h3 {
  margin: 1rem 0;
  font-size: 1rem;
  font-weight: 600;
}

#events-log {
  height: calc(100% - 3rem);
  overflow-y: auto;
  background-color: #111;
  border: 1px solid #333;
  border-radius: 0.5rem;
  padding: 0.75rem;
  font-family: 'Courier New', monospace;
  font-size: 0.75rem;
  line-height: 1.4;
}

.event-entry {
  margin-bottom: 0.5rem;
  display: flex;
  gap: 0.5rem;
}

.event-entry .timestamp {
  color: #6b7280;
  flex-shrink: 0;
}

.event-entry .event-name {
  color: #10b981;
  font-weight: 600;
  flex-shrink: 0;
}

.event-entry .event-data {
  color: #9ca3af;
}

/* Scrollbar styling */
#conversation-log::-webkit-scrollbar,
#events-log::-webkit-scrollbar {
  width: 8px;
}

#conversation-log::-webkit-scrollbar-track,
#events-log::-webkit-scrollbar-track {
  background: #1f2937;
  border-radius: 4px;
}

#conversation-log::-webkit-scrollbar-thumb,
#events-log::-webkit-scrollbar-thumb {
  background: #374151;
  border-radius: 4px;
}

#conversation-log::-webkit-scrollbar-thumb:hover,
#events-log::-webkit-scrollbar-thumb:hover {
  background: #4b5563;
}



================================================
FILE: code-helper/server/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""code-helper - Pipecat Voice Agent

This example bot demonstrates using an LLMTextProcessor along with TTS and
RTVI translations to handle special text segments like code snippets,
credit card numbers, and URLs. By prompting the LLM to wrap these segments in
specific tags, we can then process them accordingly for better speech synthesis.

This bot uses a cascade pipeline: Speech-to-Text → LLM → Text-to-Speech

Generated by Pipecat CLI

Required AI services:
- Deepgram (Speech-to-Text)
- Openai (LLM)
- Elevenlabs (Text-to-Speech)

Run the bot using::

    uv run bot.py
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame, TranscriptionMessage, TranscriptionUpdateFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.aggregators.llm_text_processor import LLMTextProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.processors.transcript_processor import TranscriptProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.utils.text.pattern_pair_aggregator import MatchAction, PatternPairAggregator

load_dotenv(override=True)


# Example/stand-in function to exemplify function calling
# and handling sensitive info like credit cards.
# This function simply returns dummy credit card info.
async def fetch_credit_card_info(params: FunctionCallParams):
    await params.result_callback(
        {"card_number": "1234-5678-9012-3456", "expiration_date": "12/24", "cvv": "123"}
    )


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    """Main bot logic."""
    logger.info("Starting bot")

    # LLM system prompt -- instruct the LLM on how to format special segments
    #   code blocks should be wrapped in <code></code> tags
    #   credit card numbers should be wrapped in <card></card> tags
    #   urls should be wrapped in <link></link> tags
    system_prompt = "You are a friendly AI assistant. All code snippets should be wrapped in <code></code> blocks. All credit card numbers should be wrapped in <card></card> blocks. All urls should be wrapped in <link></link> blocks."

    # Set up the LLMTextProcessor with a custom PatternPairAggregator
    # to identify code blocks, credit cards, and urls and aggregate them
    # separately. This allows us to handle them differently downstream in
    # TTS and RTVI processing.
    llm_text_aggregator = PatternPairAggregator()
    llm_text_aggregator.add_pattern(
        type="code",
        start_pattern="<code>",
        end_pattern="</code>",
        action=MatchAction.AGGREGATE,
    )
    llm_text_aggregator.add_pattern(
        type="credit_card",
        start_pattern="<card>",
        end_pattern="</card>",
        action=MatchAction.AGGREGATE,
    )
    llm_text_aggregator.add_pattern(
        type="link",
        start_pattern="<link>",
        end_pattern="</link>",
        action=MatchAction.AGGREGATE,
    )
    llm_text_processor = LLMTextProcessor(text_aggregator=llm_text_aggregator)

    # Speech-to-Text service
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    # Text-to-Speech service
    #   using skip_aggregator_types to avoid having code blocks spoken out loud
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
        skip_aggregator_types=["code"],  # Skip code blocks in TTS speech
    )

    # Text transformers for TTS
    # This will insert Cartesia's spell tags around the provided text.
    async def spell_out_text(text: str, type: str) -> str:
        return CartesiaTTSService.SPELL(text)

    # This will strip URL protocols for cleaner speech.
    async def strip_url_protocol(text: str, type: str) -> str:
        if text.startswith("http://"):
            text = text[len("http://") :]
        elif text.startswith("https://"):
            text = text[len("https://") :]
        if text.startswith("www."):
            text = text[len("www.") :]
        return text

    # Setup the text transformers in TTS to strip protocols from all
    # links and spell out credit card numbers. The strings below match
    # the types defined in the PatternPairAggregator above so that whenever
    # those segments are encountered, these transformers will be applied.
    tts.add_text_transformer(strip_url_protocol, "link")
    tts.add_text_transformer(spell_out_text, "credit_card")

    # LLM service with a function call to retrieve credit card info
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    llm.register_function("get_credit_card_info", fetch_credit_card_info)

    # LLM aggregator context
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        },
    ]

    credit_card_function = FunctionSchema(
        name="get_credit_card_info",
        description="Get credit card information for the user.",
        properties={},
        required=[],
    )
    tools = ToolsSchema(standard_tools=[credit_card_function])
    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(
        context=context,
    )

    # Transcription processor
    transcript_processor = TranscriptProcessor()

    # RTVI processor and observer with a text transformer to obfuscate
    # credit card numbers in the bot's output.
    async def obfuscate_credit_card(text: str, type: str) -> str:
        return "XXXX-XXXX-XXXX-" + text[-4:]

    rtvi = RTVIProcessor()
    rtvi_observer = RTVIObserver(rtvi)
    rtvi_observer.add_bot_output_transformer(obfuscate_credit_card, "credit_card")

    # Pipeline - The following pipeline is typical for a STT->LLM->TTS bot + RTVI
    #            with the addition of the LLMTextProcessor to handle special text segments.
    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            transcript_processor.user(),
            context_aggregator.user(),
            llm,
            llm_text_processor,  # Pre-aggregate LLMTextFrames for custom segment handling
            tts,
            transport.output(),
            transcript_processor.assistant(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[rtvi_observer],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Say hello and briefly introduce yourself."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    @transcript_processor.event_handler("on_transcript_update")
    async def on_transcript_update(processor: TranscriptProcessor, frame: TranscriptionUpdateFrame):
        for msg in frame.messages:
            if isinstance(msg, TranscriptionMessage):
                timestamp = f"[{msg.timestamp}] " if msg.timestamp else ""
                line = f"{timestamp}{msg.role}: {msg.content}"
                logger.info(f"Transcript: {line}")

    @rtvi.event_handler("on_client_message")
    async def on_message(rtvi, msg):
        logger.info(f"Received unknown message from client: {msg.type} | {msg.data}")

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    # We store functions so objects (e.g. SileroVADAnalyzer) don't get
    # instantiated. The function will be called when the desired transport gets
    # selected.
    transport_params = {
        "webrtc": lambda: TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV3(),
        ),
    }

    transport = await create_transport(runner_args, transport_params)

    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: code-helper/server/pyproject.toml
================================================
[project]
name = "code-helper"
version = "0.1.0"
description = "Voice AI bot built with Pipecat"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[deepgram,cartesia, elevenlabs,local-smart-turn-v3,openai,runner,silero,webrtc]>=0.0.96",
]

[dependency-groups]
dev = [
    "pyright>=1.1.404,<2",
    "ruff>=0.12.11,<1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]



================================================
FILE: code-helper/server/.env.example
================================================
# code-helper - Environment Variables
# Copy this file to .env and fill in your API keys

# Deepgram (STT/TTS)
DEEPGRAM_API_KEY=

# ElevenLabs (STT/TTS)
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=

# OpenAI (STT/LLM/TTS/Realtime)
OPENAI_API_KEY=
OPENAI_MODEL=




================================================
FILE: daily-custom-tracks/README.md
================================================
# Daily Custom Tracks

This example shows how to send and receive Daily custom tracks. We will run a simple `daily-python` application to send an audio file with a custom track (named "pipecat") to a room. Then, the Pipecat bot will mirror that custom track into another custom track (named "pipecat-mirror") in the same room.

## Get started

```python
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

## Run the bot

Start the bot by giving it a Daily room URL.

```bash
python bot.py -u ROOM_URL
```

The bot will wait for the first participant to join. Then, it will mirror a custom track named "pipecat" into a new custom track named "pipecat-mirror".

## Run the sender

Now, run the custom track sender. This is a simple `daily-python` application that opens and audio file and sends it as a custom track to the same Daily room.

```bash
python custom_track_sender.py -u ROOM_URL -i office-ambience-mono-16000.mp3
```

## Open client

Finally, open the client so you can hear both custom tracks.

```bash
open index.html
```

Once the client is opened, copy the URL of the Daily room and join it. You should be able to select which custom track you want to hear.



================================================
FILE: daily-custom-tracks/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import sys

import aiohttp
from loguru import logger
from pipecat.frames.frames import Frame, InputAudioRawFrame, OutputAudioRawFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from runner import configure

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


class CustomTrackMirrorProcessor(FrameProcessor):
    def __init__(self, transport_destination: str, **kwargs):
        super().__init__(**kwargs)
        self._transport_destination = transport_destination

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, InputAudioRawFrame) and frame.transport_source:
            output_frame = OutputAudioRawFrame(
                audio=frame.audio,
                sample_rate=frame.sample_rate,
                num_channels=frame.num_channels,
            )
            output_frame.transport_destination = self._transport_destination
            await self.push_frame(output_frame)
        else:
            await self.push_frame(frame, direction)


async def main():
    async with aiohttp.ClientSession() as session:
        (room_url, _) = await configure(session)

        transport = DailyTransport(
            room_url,
            None,
            "Custom tracks mirror",
            DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                microphone_out_enabled=False,  # Disable since we just use custom tracks
                audio_out_destinations=["pipecat-mirror"],
            ),
        )

        pipeline = Pipeline(
            [
                transport.input(),  # Transport user input
                CustomTrackMirrorProcessor("pipecat-mirror"),
                transport.output(),  # Transport bot output
            ]
        )

        task = PipelineTask(
            pipeline,
            params=PipelineParams(
                audio_in_sample_rate=16000,
                audio_out_sample_rate=16000,
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
        )

        @transport.event_handler("on_first_participant_joined")
        async def on_first_participant_joined(transport, participant):
            await transport.capture_participant_audio(participant["id"], audio_source="pipecat")

        runner = PipelineRunner()

        await runner.run(task)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: daily-custom-tracks/custom_track_sender.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import time

from daily import CallClient, CustomAudioSource, Daily
from pydub import AudioSegment

parser = argparse.ArgumentParser(description="Daily AI SDK Bot Sample")
parser.add_argument("-u", "--url", type=str, required=True, help="URL of the Daily room to join")
parser.add_argument(
    "-i", "--input", type=str, required=True, help="Input audio file (needs 16000 sample rate)"
)

args, _ = parser.parse_known_args()

audio = AudioSegment.from_mp3(args.input)

raw_bytes = audio.raw_data
sample_rate = audio.frame_rate
channels = audio.channels

print(f"Length: {len(raw_bytes)} bytes")
print(f"Sample rate: {sample_rate}, Channels: {channels}")

# Initialize the Daily context & create call client
Daily.init()

client = CallClient()

# Join the room and indicate we have a custom track named "pipecat".
client.join(
    args.url,
    client_settings={
        "publishing": {
            "camera": False,
            "microphone": False,
            "customAudio": {"pipecat": True},
        },
    },
)

# Just sleep for a couple of seconds. To do this well we should really use
# completions.
time.sleep(2)

# Create the custom audio source. This is where we will write our audio.
audio_source = CustomAudioSource(sample_rate, channels)

# Create an audio track and assign it our audio source.
client.add_custom_audio_track("pipecat", audio_source)

# Just sleep for a second. To do this well we should really use completions.
time.sleep(1)

try:
    # Just write one second of audio until we have read all the file.
    chunk_size = sample_rate * channels * 2
    while len(raw_bytes) > 0:
        chunk = raw_bytes[:chunk_size]
        raw_bytes = raw_bytes[chunk_size:]
        audio_source.write_frames(chunk)

except KeyboardInterrupt:
    client.leave()

# Just sleep for a second. To do this well we should really use completions.
time.sleep(1)

client.release()



================================================
FILE: daily-custom-tracks/index.html
================================================
<html>
  <head>
    <title>daily custom tracks</title>
  </head>
  <script crossorigin src="https://unpkg.com/@daily-co/daily-js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fomantic-ui/2.8.6/semantic.min.js"></script>
  <link
    rel="stylesheet"
    type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/fomantic-ui/2.8.6/semantic.min.css"
    />
  <script>
    function enableButton(buttonId, enable) {
        const button = document.getElementById(buttonId);
        button.disabled = !enable;
    }

    function enableJoinButton(enable) {
        enableButton("join-button", enable);
    }

    function enableLeaveButton(enable) {
        enableButton("leave-button", enable);
    }

    function destroyPlayers(query) {
        const items = document.querySelectorAll(query);
        if (items) {
            for (const item of items) {
                item.remove();
            }
        }
    }

    function destroyParticipantPlayers(participantId) {
        destroyPlayers(`audio[data-participant-id="${participantId}"]`);
        destroyPlayers(`button[data-participant-id="${participantId}"]`);
    }

    async function startPlayer(player, track) {
        player.muted = false;
        player.autoplay = true;
        if (track != null) {
            player.srcObject = new MediaStream([track]);
        }
    }

    async function buildAudioPlayer(track, participantId) {
        const audioContainer = document.getElementById("audio-container");
        const player = document.createElement("audio");
        player.dataset.participantId = participantId;

        // Create a new button for controlling audio
        const audioControlButton = document.createElement("button");
        audioControlButton.className = "ui primary green button"
        audioControlButton.innerText = track._mediaTag == "cam-audio" ? "english" : track._mediaTag;
        audioControlButton.dataset.participantId = participantId;
        audioControlButton.onclick = () => {
            if (player.paused) {

                player.play();
                audioControlButton.className = "ui primary red button"
            } else {
                player.pause();
                audioControlButton.className = "ui primary green button"
            }
        };

        audioContainer.appendChild(player);
        audioContainer.appendChild(audioControlButton);

        await startPlayer(player, track);
        player.pause()

        return player;
    }

    function subscribeToTracks(participantId) {
        console.log(`subscribing to track`);

        if (participantId === "local") {
            return;
        }

        callObject.updateParticipant(participantId, {
            setSubscribedTracks: {
                audio: true,
                video: false,
                custom: true,
            },
        });
    }

    function startDaily() {
        enableJoinButton(true);
        enableLeaveButton(false);

        window.callObject = window.DailyIframe.createCallObject({});

        callObject.on("participant-joined", (e) => {
            if (!e.participant.local) {
                console.log("participant-joined", e.participant);
               subscribeToTracks(e.participant.session_id);
            }
        });

        callObject.on("participant-left", (e) => {
            console.log("participant-left", e.participant.session_id);
            destroyParticipantPlayers(e.participant.session_id);
        });

        callObject.on("track-started", async (e) => {
            console.log("track-started", e.track);
            if (e.track.kind === "audio") {
                await buildAudioPlayer(e.track, e.participant.session_id);
            }
        });
    }

    async function joinRoom() {
        enableJoinButton(false);
        enableLeaveButton(true);

        const meetingUrl = document.getElementById("meeting-url").value;

        callObject.join({
            url: meetingUrl,
            startVideoOff: true,
            startAudioOff: true,
            subscribeToTracksAutomatically: false,
            receiveSettings: {
                base: { video: { layer: 0 } },
            },
        });
    }

    async function leaveRoom() {
        enableJoinButton(true);
        enableLeaveButton(false);

        callObject.leave();

        const audioContainer = document.getElementById("audio-container");
        audioContainer.replaceChildren();
    }
  </script>

  <body onload="startDaily()">
    <div class="ui centered page grid" style="margin-top: 30px">
      <div class="ten wide column">
        <div class="ui form" style="margin-top: 30px">
          <div class="field">
            <label>Meeting URL</label>
            <input id="meeting-url" value="" />
          </div>
        </div>
      </div>
    </div>
    <div class="ui centered aligned header" style="margin-top: 30px">
      <button id="join-button" class="ui primary button" onclick="joinRoom()">
        Join
      </button>
      <button id="leave-button" class="ui button" onclick="leaveRoom()">
        Leave
      </button>
    </div>
    <div id="tile" class="ui container" style="margin-top: 30px">
      <div id="tile" class="ui center aligned grid">
        <div id="audio-container"></div><br/>
      </div>
    </div>
  </body>
</html>



================================================
FILE: daily-custom-tracks/requirements.txt
================================================
pydub
pipecat-ai[daily]



================================================
FILE: daily-custom-tracks/runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os

import aiohttp
from pipecat.transports.daily.utils import DailyRESTHelper


async def configure(aiohttp_session: aiohttp.ClientSession):
    parser = argparse.ArgumentParser(description="Daily AI SDK Bot Sample")
    parser.add_argument(
        "-u", "--url", type=str, required=False, help="URL of the Daily room to join"
    )
    parser.add_argument(
        "-k",
        "--apikey",
        type=str,
        required=False,
        help="Daily API Key (needed to create an owner token for the room)",
    )

    args, unknown = parser.parse_known_args()

    url = args.url or os.getenv("DAILY_SAMPLE_ROOM_URL")
    key = args.apikey or os.getenv("DAILY_API_KEY")

    if not url:
        raise Exception(
            "No Daily room specified. use the -u/--url option from the command line, or set DAILY_SAMPLE_ROOM_URL in your environment to specify a Daily room URL."
        )

    if not key:
        raise Exception(
            "No Daily API key specified. use the -k/--apikey option from the command line, or set DAILY_API_KEY in your environment to specify a Daily API key, available from https://dashboard.daily.co/developers."
        )

    daily_rest_helper = DailyRESTHelper(
        daily_api_key=key,
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )

    # Create a meeting token for the given room with an expiration 1 hour in
    # the future.
    expiry_time: float = 60 * 60

    token = await daily_rest_helper.get_token(url, expiry_time)

    return (url, token)



================================================
FILE: daily-multi-translation/README.md
================================================
# Daily Multi Translation

This example shows how to use Daily to stream multiple simultaneous translations using a single transport. Daily provides custom tracks and in this example we will simultaneously translate incoming audio in English to Spanish, French and German, each of them being sent to a custom track.

## Get started

Create your virtual environment and install dependencies:

```bash
uv sync
```

Set up environment variables:

```bash
cp env.example .env # and add your credentials
```

## Run the bot

Start your bot using the DailyTransport:

```bash
uv run bot.py -t daily
```

Then, visit `http://localhost:7860/` in your browser. This will open a Daily Prebuilt room where you will speak in English (make sure you are not muted).

## Open client

Next, you need to open the client that will listen to the translations.

```bash
open index.html
```

Once the client is opened, copy the URL of the Daily room created above and join it. You should be able to select which translation you want to hear.

## Build and test the Docker image

```
docker build -t daily-multi-translation .
docker run --env-file .env -p 7860:7860 daily-multi-translation
```



================================================
FILE: daily-multi-translation/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.mixers.soundfile_mixer import SoundfileMixer
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.observers.loggers.transcription_log_observer import TranscriptionLogObserver
from pipecat.pipeline.parallel_pipeline import ParallelPipeline
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

BACKGROUND_SOUND_FILE = "office-ambience-mono-16000.mp3"


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts_spanish = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="cefcb124-080b-4655-b31f-932f3ee743de",
        transport_destination="spanish",
    )
    tts_french = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="8832a0b5-47b2-4751-bb22-6a8e2149303d",
        transport_destination="french",
    )
    tts_german = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="38aabb6a-f52b-4fb0-a3d1-988518f4dc06",
        transport_destination="german",
    )

    messages_spanish = [
        {
            "role": "system",
            "content": "You will be provided with a sentence in English, and your task is to only translate it into Spanish.",
        },
    ]
    messages_french = [
        {
            "role": "system",
            "content": "You will be provided with a sentence in English, and your task is to only translate it into French.",
        },
    ]
    messages_german = [
        {
            "role": "system",
            "content": "You will be provided with a sentence in English, and your task is to only translate it into German.",
        },
    ]

    llm_spanish = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    llm_french = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    llm_german = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    context_spanish = LLMContext(messages_spanish)
    context_aggregator_spanish = LLMContextAggregatorPair(context_spanish)

    context_french = LLMContext(messages_french)
    context_aggregator_french = LLMContextAggregatorPair(context_french)

    context_german = LLMContext(messages_german)
    context_aggregator_german = LLMContextAggregatorPair(context_german)

    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            stt,
            ParallelPipeline(
                # Spanish pipeline.
                [
                    context_aggregator_spanish.user(),
                    llm_spanish,
                    tts_spanish,
                    context_aggregator_spanish.assistant(),
                ],
                # French pipeline.
                [
                    context_aggregator_french.user(),
                    llm_french,
                    tts_french,
                    context_aggregator_french.assistant(),
                ],
                # German pipeline.
                [
                    context_aggregator_german.user(),
                    llm_german,
                    tts_german,
                    context_aggregator_german.assistant(),
                ],
            ),
            transport.output(),  # Transport bot output
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=16000,
            audio_out_sample_rate=16000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[TranscriptionLogObserver()],
    )

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = DailyTransport(
        runner_args.room_url,
        runner_args.token,
        "Multi translation bot",
        DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            audio_out_mixer={
                "spanish": SoundfileMixer(
                    sound_files={"office": BACKGROUND_SOUND_FILE}, default_sound="office"
                ),
                "french": SoundfileMixer(
                    sound_files={"office": BACKGROUND_SOUND_FILE}, default_sound="office"
                ),
                "german": SoundfileMixer(
                    sound_files={"office": BACKGROUND_SOUND_FILE}, default_sound="office"
                ),
            },
            audio_out_destinations=["spanish", "french", "german"],
            microphone_out_enabled=False,  # Disable since we just use custom tracks
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: daily-multi-translation/Dockerfile
================================================
FROM python:3.10-bullseye

RUN mkdir /app
RUN mkdir /app/assets
RUN mkdir /app/utils
COPY *.py /app/
COPY requirements.txt /app/


WORKDIR /app
RUN pip3 install -r requirements.txt

EXPOSE 7860

CMD ["python3", "server.py"]



================================================
FILE: daily-multi-translation/env.example
================================================
DAILY_SAMPLE_ROOM_URL=https://yourdomain.daily.co/yourroom # (for joining the bot to the same room repeatedly for local dev)
DAILY_API_KEY=7df...
OPENAI_API_KEY=sk-PL...
DEEPGRAM_API_KEY=efb...
CARTESIA_API_KEY=aeb...



================================================
FILE: daily-multi-translation/index.html
================================================
<html>
  <head>
    <title>daily multi translation</title>
  </head>
  <script crossorigin src="https://unpkg.com/@daily-co/daily-js"></script>
  <script
    src="https://code.jquery.com/jquery-3.1.1.min.js"
    integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8="
    crossorigin="anonymous"
    ></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fomantic-ui/2.8.6/semantic.min.js"></script>
  <link
    rel="stylesheet"
    type="text/css"
    href="https://cdnjs.cloudflare.com/ajax/libs/fomantic-ui/2.8.6/semantic.min.css"
    />
  <script>
    function enableButton(buttonId, enable) {
        const button = document.getElementById(buttonId);
        button.disabled = !enable;
    }

    function enableJoinButton(enable) {
        enableButton("join-button", enable);
    }

    function enableLeaveButton(enable) {
        enableButton("leave-button", enable);
    }

    function destroyPlayers(query) {
        const items = document.querySelectorAll(query);
        if (items) {
            for (const item of items) {
                item.remove();
            }
        }
    }

    function destroyParticipantPlayers(participantId) {
        destroyPlayers(`video[data-participant-id="${participantId}"]`);
        destroyPlayers(`audio[data-participant-id="${participantId}"]`);
        destroyPlayers(`button[data-participant-id="${participantId}"]`);
    }

    async function startPlayer(player, track) {
        player.muted = false;
        player.autoplay = true;
        if (track != null) {
            player.srcObject = new MediaStream([track]);
        }
    }

    async function buildVideoPlayer(track, participantId) {
        const videoContainer = document.getElementById("video-container");
        const player = document.createElement("video");
        player.dataset.participantId = participantId;

        videoContainer.appendChild(player);

        await startPlayer(player, track);
        await player.play();

        return player;
    }

    async function buildAudioPlayer(track, participantId) {
        const audioContainer = document.getElementById("audio-container");
        const player = document.createElement("audio");
        player.dataset.participantId = participantId;

        // Create a new button for controlling audio
        const audioControlButton = document.createElement("button");
        audioControlButton.className = "ui primary green button"
        audioControlButton.innerText = track._mediaTag == "cam-audio" ? "english" : track._mediaTag;
        audioControlButton.dataset.participantId = participantId;
        audioControlButton.onclick = () => {
            if (player.paused) {

                player.play();
                audioControlButton.className = "ui primary red button"
            } else {
                player.pause();
                audioControlButton.className = "ui primary green button"
            }
        };

        audioContainer.appendChild(player);
        audioContainer.appendChild(audioControlButton);

        await startPlayer(player, track);
        player.pause()

        return player;
    }

    function subscribeToTracks(participantId) {
        console.log(`subscribing to track`);

        if (participantId === "local") {
            return;
        }

        callObject.updateParticipant(participantId, {
            setSubscribedTracks: {
                audio: true,
                video: true,
                custom: true,
            },
        });
    }

    function startDaily() {
        enableJoinButton(true);
        enableLeaveButton(false);

        window.callObject = window.DailyIframe.createCallObject({});

        callObject.on("participant-joined", (e) => {
            if (!e.participant.local) {
                console.log("participant-joined", e.participant);
               subscribeToTracks(e.participant.session_id);
            }
        });

        callObject.on("participant-left", (e) => {
            console.log("participant-left", e.participant.session_id);
            destroyParticipantPlayers(e.participant.session_id);
        });

        callObject.on("track-started", async (e) => {
            console.log("track-started", e.track);
            if (e.track.kind === "video") {
                await buildVideoPlayer(e.track, e.participant.session_id);
            } else if (e.track.kind === "audio") {
                await buildAudioPlayer(e.track, e.participant.session_id);
            }
        });
    }

    async function joinRoom() {
        enableJoinButton(false);
        enableLeaveButton(true);

        const meetingUrl = document.getElementById("meeting-url").value;

        callObject.join({
            url: meetingUrl,
            startVideoOff: true,
            startAudioOff: true,
            subscribeToTracksAutomatically: false,
            receiveSettings: {
                base: { video: { layer: 0 } },
            },
        });
    }

    async function leaveRoom() {
        enableJoinButton(true);
        enableLeaveButton(false);

        callObject.leave();

        const videoContainer = document.getElementById("video-container");
        videoContainer.replaceChildren();

        const audioContainer = document.getElementById("audio-container");
        audioContainer.replaceChildren();
    }
  </script>

  <body onload="startDaily()">
    <div class="ui centered page grid" style="margin-top: 30px">
      <div class="ten wide column">
        <div class="ui form" style="margin-top: 30px">
          <div class="field">
            <label>Meeting URL</label>
            <input id="meeting-url" value="" />
          </div>
        </div>
      </div>
    </div>
    <div class="ui centered aligned header" style="margin-top: 30px">
      <button id="join-button" class="ui primary button" onclick="joinRoom()">
        Join
      </button>
      <button id="leave-button" class="ui button" onclick="leaveRoom()">
        Leave
      </button>
    </div>
    <div id="tile" class="ui container" style="margin-top: 30px">
      <div id="tile" class="ui center aligned grid">
        <div id="audio-container"></div><br/>
      </div>
    </div>
    <div id="tile" class="ui container" style="margin-top: 30px">
      <div id="tile" class="ui center aligned grid">
        <div id="video-container" class="ui segment"></div>
      </div>
    </div>
  </body>
</html>



================================================
FILE: daily-multi-translation/pyproject.toml
================================================
[project]
name = "daily-multitranslation"
version = "0.1.0"
description = "Daily Multitranslation bot"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[daily,webrtc,websocket,cartesia,google,silero,deepgram,runner,local-smart-turn-v3,soundfile]>=0.0.91",
  "pipecatcloud>=0.2.7",
  "aiofiles"
]



================================================
FILE: deployment/aws-agentcore-webrtc/README.md
================================================
# Amazon Bedrock AgentCore Runtime WebRTC Example

This example demonstrates how to deploy a Pipecat bot to **Amazon Bedrock AgentCore Runtime** using SmallWebRTC for communication.

## Prerequisites

- Accounts with:
  - AWS
  - Deepgram
  - Cartesia
- Python 3.10 or higher
- `uv` package manager

## Set Up the Environment

### IAM Configuration

Configure your IAM user with the necessary policies for AgentCore usage. Start with these:

- `BedrockAgentCoreFullAccess`
- A new policy (maybe named `BedrockAgentCoreCLI`) configured [like this](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html#runtime-permissions-starter-toolkit)

You can also choose to specify more granular permissions; see [Amazon Bedrock AgentCore docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html) for more information.

To authenticate with AWS, you have two options:

1. Export environment variables:

   ```bash
   export AWS_SECRET_ACCESS_KEY=your_secret_key
   export AWS_ACCESS_KEY_ID=your_access_key
   export AWS_REGION=your_region
   export AWS_DEFAULT_REGION=your_default_region
   ```

2. Or use AWS CLI configuration:
   ```bash
   aws configure
   ```
   This will create/update your AWS credentials file (~/.aws/credentials).

### Virtual Environment Setup

Create and activate a virtual environment:

```bash
uv sync
```

### Environment Variables Configuration

1. For the agent:

   ```bash
   cd agent
   cp env.example .env
   ```

   Add your API keys:

   - `AWS_ACCESS_KEY_ID`: Your AWS access key ID for the Amazon Bedrock LLM used by the agent
   - `AWS_SECRET_ACCESS_KEY`: Your AWS secret access key for the Amazon Bedrock LLM used by the agent
   - `AWS_REGION`: The AWS region for the Amazon Bedrock LLM used by the agent
   - `DEEPGRAM_API_KEY`: Your Deepgram API key
   - `CARTESIA_API_KEY`: Your Cartesia API key
   - `ICE_SERVER_URLS`: Your TCP TURN server urls
   - `ICE_SERVER_USERNAME`: Your TURN server username
   - `ICE_SERVER_CREDENTIAL`: Your TURN server credential

   > Important Notes about TURN Server Configuration:
   >
   > - You must use TURN servers that support TCP connections
   > - UDP connections are not supported within AgentCore runtime environment
   > - If your TURN server only supports UDP, your WebRTC connection will fail
   > - Consider using a service like Twilio's TURN servers which support TCP

2. For the server:
   ```bash
   cd server
   cp env.example .env
   ```
   The server configuration is minimal - the `AGENT_RUNTIME_ARN` will be automatically set during agent deployment.

## Agent Configuration

Configure your bot as an AgentCore agent:

```bash
./scripts/configure.sh
```

This script:

1. Configures deployment type as "Container" (required by Pipecat)
2. Applies necessary patches to the Dockerfile
3. Adds dependencies required by SmallWebRTC (`libgl1` and `libglib2.0-0`)

Follow the prompts to complete the configuration.

> Technical Note:
> Direct Code Deploy isn't used because some dependencies (like `numba`) lack `aarch64_manylinux2014` wheels.

## ⚠️ Before Proceeding

Just in case you've previously deployed other agents to AgentCore, ensure that you have the desired agent selected as "default" in the `agentcore` tool:

```
# Check
uv run agentcore configure list
# Set
uv run agentcore configure set-default <agent-name>
```

The following steps act on `agentcore`'s default agent.

## Deployment to AgentCore Runtime

Deploy your bot:

```bash
./scripts/launch.sh
```

This script:

1. Reads environment variables from `agent/.env`
2. Deploys to AgentCore
3. Updates the server's configuration with the agent ARN
4. Displays log-tailing commands for monitoring

## Running on AgentCore Runtime

1. Start the server:

   ```bash
   cd server
   uv run python server.py
   ```

2. Access the UI:
   - Open http://localhost:7860 in your browser
   - Or use your configured custom port

## Monitoring and Troubleshooting

### View Agent Logs

Use the log-tailing command provided during deployment:

```bash
# Replace with your actual command
aws logs tail /aws/bedrock-agentcore/runtimes/bot1-0uJkkT7QHC-DEFAULT --log-stream-name-prefix "2025/11/19/[runtime-logs]" --follow
```

## Test Agent Manually

Test the agent using the AWS CLI:

```bash
uv run agentcore invoke \
  --session-id user-123456-conversation-12345679 \
  '{
  "sdp": "YOUR_OFFER",
  "type": "offer"
}'
```

> This will only allow you to see that the Pipecat agent has started, but you won’t be able to hear or send audio. So it is only useful for troubleshooting.

## Cleanup

Remove your agent:

```bash
./scripts/destroy.sh
```

## Local Development

Run your bot locally for testing:

```bash
PIPECAT_LOCAL_DEV=1 uv run python pipecat-agent.py
```

## Additional Resources

- [Amazon Bedrock AgentCore Developer Guide](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html)
- [TURN Server Configuration Guide](https://webrtc.org/getting-started/turn-server)



================================================
FILE: deployment/aws-agentcore-webrtc/pyproject.toml
================================================
[project]
name = "agentcore-pipecat-webrtc"
version = "0.1.0"
description = "Example for building Pipecat bots deployable to Amazon Bedrock AgentCore using WebRTC for communication"
requires-python = ">=3.10"
dependencies = [
    "aioice",
    "aiohttp",
    "aioboto3",
    "bedrock-agentcore",
    "python-dotenv",
    "pipecat-ai[aws,webrtc,daily,silero,deepgram,openai,cartesia,runner]",
    "pipecat-ai-small-webrtc-prebuilt>=2.0.0"
]

[dependency-groups]
dev = [
    "bedrock-agentcore-starter-toolkit",
    "pyright>=1.1.404,<2",
    "ruff>=0.12.11,<1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: deployment/aws-agentcore-webrtc/agent/env.example
================================================
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=...
DEEPGRAM_API_KEY=...
CARTESIA_API_KEY=...

# Credentials for the TCP TURN servers
ICE_SERVER_URLS=turn:turn.cloudflare.com:3478?transport=tcp,turn:turn.cloudflare.com:80?transport=tcp,turns:turn.cloudflare.com:5349?transport=tcp,turns:turn.cloudflare.com:443?transport=tcp
ICE_SERVER_USERNAME=
ICE_SERVER_CREDENTIAL=



================================================
FILE: deployment/aws-agentcore-webrtc/agent/pipecat-agent.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from bedrock_agentcore import BedrockAgentCoreApp
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.aws import AWSBedrockLLMService
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.connection import IceServer, SmallWebRTCConnection
from pipecat.transports.smallwebrtc.request_handler import (
    IceCandidate,
    SmallWebRTCPatchRequest,
    SmallWebRTCRequest,
    SmallWebRTCRequestHandler,
)
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

app = BedrockAgentCoreApp()

request_handler: SmallWebRTCRequestHandler = None

load_dotenv(override=True)


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    yield {"status": "initializing bot"}

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    # Automatically uses AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION env vars.
    llm = AWSBedrockLLMService(
        model="us.amazon.nova-2-lite-v1:0",
        params=AWSBedrockLLMService.InputParams(temperature=0.8),
    )

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be spoken aloud, so avoid special characters that can't easily be spoken, such as emojis or bullet points. Respond to what the user said in a creative and helpful way.",
        },
        {"role": "user", "content": "Say hello and briefly introduce yourself."},
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        idle_timeout_secs=runner_args.pipeline_idle_timeout_secs,
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.info(f"Client ready")
        await rtvi.set_bot_ready()
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    task_id = app.add_async_task("voice_agent")

    await runner.run(task)

    app.complete_async_task(task_id)

    yield {"status": "completed"}


async def initialize_connection_and_run_bot(request: SmallWebRTCRequest):
    """Handle initial WebRTC connection setup and run the bot."""

    raw_urls = os.getenv("ICE_SERVER_URLS")
    urls = [u.strip() for u in raw_urls.split(",") if u.strip()]
    ice_servers = [
        IceServer(
            urls=urls,
            username=os.getenv("ICE_SERVER_USERNAME"),
            credential=os.getenv("ICE_SERVER_CREDENTIAL"),
        )
    ]

    transport = None
    runner_args = None

    async def webrtc_connection_callback(connection: SmallWebRTCConnection):
        nonlocal transport, runner_args
        runner_args = SmallWebRTCRunnerArguments(
            webrtc_connection=connection, body=request.request_data
        )
        transport = await create_transport(runner_args, transport_params)

    yield {"status": "initializing connection"}
    global request_handler
    request_handler = SmallWebRTCRequestHandler(ice_servers=ice_servers)
    answer = await request_handler.handle_web_request(
        request=request, webrtc_connection_callback=webrtc_connection_callback
    )
    yield {"status": "ANSWER:START"}
    yield {"answer": answer}
    yield {"status": "ANSWER:END"}

    async for result in run_bot(transport, runner_args):
        yield result


async def add_ice_candidates(patch_request: SmallWebRTCPatchRequest):
    """Handle ICE candidate additions for existing connections."""
    await request_handler.handle_patch_request(patch_request)
    yield {"status": "success"}


@app.entrypoint
async def agentcore_bot(payload, context):
    """Bot entry point for running on Amazon Bedrock AgentCore Runtime."""
    request_type = payload.get("type", "unknown")
    logger.info(f"Received request of type: {request_type}")

    data = payload.get("data")
    if not data:
        logger.error("No data found in payload")
        yield {"status": "error", "message": "No data found in payload"}
        return

    match request_type:
        case "offer":
            # Initial connection setup
            try:
                request = SmallWebRTCRequest.from_dict(data)
            except Exception as e:
                logger.error(f"Failed to deserialize SmallWebRTCRequest: {e}")
                yield {"status": "error", "message": f"Invalid request payload: {str(e)}"}
                return
            async for result in initialize_connection_and_run_bot(request):
                yield result
        case "ice-candidates":
            # ICE candidate additions
            try:
                if "candidates" in data:
                    data["candidates"] = [IceCandidate(**c) for c in data["candidates"]]
                patch_request = SmallWebRTCPatchRequest(**data)
            except Exception as e:
                logger.error(f"Failed to deserialize SmallWebRTCPatchRequest: {e}")
                yield {"status": "error", "message": f"Invalid request payload: {str(e)}"}
                return
            async for result in add_ice_candidates(patch_request):
                yield result
        case _:
            logger.error(f"Unknown request type: {request_type}")
            yield {"status": "error", "message": f"Unknown request type: {request_type}"}
            return


# Used for local development
async def bot(runner_args: RunnerArguments):
    """Bot entry point for running locally and on Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    async for result in run_bot(transport, runner_args):
        pass  # Consume the stream


if __name__ == "__main__":
    # NOTE: ideally we shouldn't have to branch for local dev vs AgentCore, but
    # local AgentCore container-based dev doesn't seem to be working, or at
    # least not for this project.
    if os.getenv("PIPECAT_LOCAL_DEV") == "1":
        # Running locally
        from pipecat.runner.run import main

        main()
    else:
        # Running on AgentCore Runtime
        app.run()



================================================
FILE: deployment/aws-agentcore-webrtc/agent/pyproject.toml
================================================
[project]
name = "agentcore-pipecat-webrtc-agent"
version = "0.1.0"
description = "Pipecat bot agent for Amazon Bedrock AgentCore"
requires-python = ">=3.10"
dependencies = [
    "aiohttp",
    "bedrock-agentcore",
    "pipecat-ai[aws,webrtc,daily,silero,deepgram,openai,cartesia,runner]",
]



================================================
FILE: deployment/aws-agentcore-webrtc/scripts/configure.sh
================================================
#!/bin/bash

# Script to configure the bot, patch Dockerfile and sync AGENT_RUNTIME_ARN

DOCKERFILE=".bedrock_agentcore/pipecat_agent/Dockerfile"
TARGET_LINE="RUN cd . && uv pip install ."
# Extra dependencies needed by SmallWebRTC
INSERT_LINE="RUN apt update && apt install -y libgl1 libglib2.0-0 && apt clean"

###############################################
# STEP 1 — Configure agentcore
# Already configuring to use Docker as it is required by Pipecat
# Disabling memory by default since it is not needed by this example
###############################################
uv run agentcore configure -e ./agent/pipecat-agent.py --name pipecat_agent --container-runtime docker --disable-memory

###############################################
# STEP 2 — Wait until Dockerfile exists
###############################################
while [ ! -s "$DOCKERFILE" ]; do
    sleep 0.2
done

###############################################
# STEP 3 — Patch Dockerfile
###############################################
cp "$DOCKERFILE" "$DOCKERFILE.bak"

awk -v target="$TARGET_LINE" -v insert="$INSERT_LINE" '
{
    print $0
    if ($0 ~ target) {
        print insert
    }
}
' "$DOCKERFILE.bak" > "$DOCKERFILE"

echo "Dockerfile patched successfully!"



================================================
FILE: deployment/aws-agentcore-webrtc/scripts/destroy.sh
================================================
#!/bin/bash

# Script to destroy the agent

uv run agentcore destroy


================================================
FILE: deployment/aws-agentcore-webrtc/scripts/launch.sh
================================================
#!/bin/bash

# Script to dynamically read all variables from .env file and launch agentcore
AGENT_ENV_FILE="./agent/.env"
SERVER_ENV_FILE="./server/.env"

###############################################
# STEP 1 — Launch the new agent
###############################################

# Check if the local .env file exists
if [ ! -f "$AGENT_ENV_FILE" ]; then
    echo "Error: $AGENT_ENV_FILE file not found"
    echo "Please create an agent .env file with your environment variables"
    exit 1
fi

# Start building the agentcore launch command
LAUNCH_CMD="uv run agentcore launch --auto-update-on-conflict"
FOUND_ENV_VARS=false

echo "Loading environment variables from agent .env file..."

# Read each line from agent .env file and process it
while IFS= read -r line || [ -n "$line" ]; do
    # Skip empty lines & comments
    [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue

    # Ensure line contains KEY=value
    if [[ "$line" =~ ^[^=]+=(.*)$ ]]; then
        VAR_NAME="${line%%=*}"
        VAR_VALUE="${line#*=}"

        # Remove surrounding whitespace
        VAR_NAME="$(echo "$VAR_NAME" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"
        VAR_VALUE="$(echo "$VAR_VALUE" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"

        # Skip PIPECAT_LOCAL_DEV
        if [[ "$VAR_NAME" == "PIPECAT_LOCAL_DEV" ]]; then
            echo "  Skipping: $VAR_NAME (ignored for deployment)"
            continue
        fi

        # Skip if variable name or value is empty
        if [[ -n "$VAR_NAME" && -n "$VAR_VALUE" ]]; then
            # Always quote the value so special characters are preserved
            LAUNCH_CMD+=" --env $VAR_NAME=\"$VAR_VALUE\""
            FOUND_ENV_VARS=true
            echo "  Added: $VAR_NAME"
        fi
    fi
done < "$AGENT_ENV_FILE"

# Check if any environment variables were added
if ! $FOUND_ENV_VARS; then
    echo "Warning: No valid environment variables found in agent .env file"
    echo "Make sure your agent .env file contains variables in the format: KEY=value"
    exit 1
fi

# Execute the command
echo ""
echo "Executing: $LAUNCH_CMD"
eval "$LAUNCH_CMD"


###############################################
# STEP 2 — Read AGENT ARN from agentcore status
###############################################
echo "Reading Agent ARN from agentcore status..."

# Extract Agent ARN from status output (removing box formatting characters and spaces)
AGENT_ARN=$(uv run agentcore status | grep "Agent ARN:" | sed 's/.*Agent ARN: //' | sed 's/│//g' | xargs)

echo "Agent ARN: $AGENT_ARN"

###############################################
# STEP 3 — Update server .env
###############################################
if [ ! -f "$SERVER_ENV_FILE" ]; then
    echo "ERROR: $SERVER_ENV_FILE not found!"
    exit 1
fi

# If AGENT_RUNTIME_ARN already exists → replace
# If not → append
if grep -q "^AGENT_RUNTIME_ARN=" "$SERVER_ENV_FILE"; then
    sed -i.bak "s|^AGENT_RUNTIME_ARN=.*|AGENT_RUNTIME_ARN=$AGENT_ARN|" "$SERVER_ENV_FILE"
else
    echo "AGENT_RUNTIME_ARN=$AGENT_ARN" >> "$SERVER_ENV_FILE"
fi

echo ".env updated successfully!"
echo "AGENT_RUNTIME_ARN is now set to:"
echo "$AGENT_ARN"


================================================
FILE: deployment/aws-agentcore-webrtc/server/env.example
================================================
# This variable is automatically populated when you run the script that launches the new agent.
# If you're curious, the value is stored in .bedrock_agentcore.yaml.
AGENT_RUNTIME_ARN=

# Credentials for STUN/TURN servers.
# Username and credential fields are optional, since not all ICE servers require them.
# For example, Google’s public STUN server does not require authentication.
ICE_SERVER_URLS=stun:stun.l.google.com:19302
ICE_SERVER_USERNAME=
ICE_SERVER_CREDENTIAL=


================================================
FILE: deployment/aws-agentcore-webrtc/server/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import json
import os
import sys
import uuid
from contextlib import asynccontextmanager
from http import HTTPMethod
from typing import Any, Dict, List, Optional, TypedDict, Union

import boto3
import uvicorn
from botocore.response import StreamingBody
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, HTTPException, Request, Response
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse
from loguru import logger
from pipecat_ai_small_webrtc_prebuilt.frontend import SmallWebRTCPrebuiltUI

load_dotenv(override=True)

app = FastAPI()

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Add your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# In-memory store of active sessions: session_id -> session info
active_sessions: Dict[str, Dict[str, Any]] = {}

# Initialize Bedrock client
bedrock = boto3.client("bedrock-agentcore")

# You can find this inside .bedrock_agentcore.yaml
AGENT_RUNTIME_ARN = os.getenv("AGENT_RUNTIME_ARN")

# Mount the frontend at /
app.mount("/client", SmallWebRTCPrebuiltUI)


# Ice servers
class IceServer(TypedDict, total=False):
    urls: Union[str, List[str]]
    username: Optional[str]
    credential: Optional[str]


raw_urls = os.getenv("ICE_SERVER_URLS")
urls = [u.strip() for u in raw_urls.split(",") if u.strip()]
ice_servers = [
    IceServer(
        urls=urls,
        username=os.getenv("ICE_SERVER_USERNAME"),
        credential=os.getenv("ICE_SERVER_CREDENTIAL"),
    )
]

logger.info(f"Ice servers: {ice_servers}")


@app.get("/", include_in_schema=False)
async def root_redirect():
    return RedirectResponse(url="/client/")


async def post_offer(request: Request, session_id: str):
    """Handle WebRTC offer requests."""

    data = await request.json()
    request = {"type": "offer", "data": data}

    response = bedrock.invoke_agent_runtime(
        agentRuntimeArn=AGENT_RUNTIME_ARN,
        contentType="application/json",
        payload=json.dumps(request),
        runtimeSessionId=session_id,
    )

    answer_sdp = None

    if "text/event-stream" in response.get("contentType", ""):
        # Handle streaming response
        streaming_body: StreamingBody = response["response"]
        for line in streaming_body.iter_lines(chunk_size=1):
            if line:
                line = line.decode("utf-8")
                if line.startswith("data: "):
                    line = line[6:]
                    print(f"Received line: {line}")
                    try:
                        event = json.loads(line)
                        print("Received event:", event)

                        # 4. Check for the 'answer' key
                        if "answer" in event:
                            payload = event["answer"]

                            if payload.get("type") == "answer":
                                answer_sdp = payload
                                print("WebRTC answer found. Stopping stream processing.")
                                # Break the line loop immediately
                                break

                    except json.JSONDecodeError:
                        print(f"Failed to parse extracted SSE payload as JSON: {line}")
                        pass

    if answer_sdp is None:
        raise HTTPException(500, "Did not find WebRTC answer in agent output")

    return answer_sdp


async def patch_offer(request: Request, session_id: str):
    """Handle WebRTC new ice candidate requests."""

    data = await request.json()
    request = {"type": "ice-candidates", "data": data}

    response = bedrock.invoke_agent_runtime(
        agentRuntimeArn=AGENT_RUNTIME_ARN,
        contentType="application/json",
        payload=json.dumps(request),
        runtimeSessionId=session_id,
    )

    result = None

    if "text/event-stream" in response.get("contentType", ""):
        # Handle streaming response
        streaming_body: StreamingBody = response["response"]
        for line in streaming_body.iter_lines(chunk_size=1):
            if line:
                line = line.decode("utf-8")
                if line.startswith("data: "):
                    line = line[6:]
                    print(f"Received line: {line}")
                    try:
                        # Assume the first valid JSON line is the result
                        result = json.loads(line)
                        print("Received event:", result)
                        break
                    except json.JSONDecodeError:
                        print(f"Failed to parse extracted SSE payload as JSON: {line}")
                        pass

    if result is None:
        raise HTTPException(500, "Did not get ICE candidate ack from agent")

    return result


@app.post("/start")
async def rtvi_start(request: Request):
    """Mimic Pipecat Cloud's /start endpoint."""

    class IceConfig(TypedDict):
        iceServers: List[IceServer]

    class StartBotResult(TypedDict, total=False):
        sessionId: str
        iceConfig: Optional[IceConfig]

    # Parse the request body
    try:
        request_data = await request.json()
        logger.debug(f"Received request: {request_data}")
    except Exception as e:
        logger.error(f"Failed to parse request body: {e}")
        request_data = {}

    # Store session info immediately in memory, replicate the behavior expected on Pipecat Cloud
    session_id = str(uuid.uuid4())
    active_sessions[session_id] = request_data

    result: StartBotResult = {"sessionId": session_id}
    if request_data.get("enableDefaultIceServers"):
        result["iceConfig"] = IceConfig(iceServers=ice_servers)

    return result


@app.api_route(
    "/sessions/{session_id}/{path:path}",
    methods=["GET", "POST", "PUT", "PATCH", "DELETE"],
)
async def proxy_request(
    session_id: str, path: str, request: Request, background_tasks: BackgroundTasks
):
    """Mimic Pipecat Cloud's proxy."""
    active_session = active_sessions.get(session_id)
    if active_session is None:
        return Response(content="Invalid or not-yet-ready session_id", status_code=404)

    if path.endswith("api/offer"):
        try:
            if request.method == HTTPMethod.POST.value:
                return await post_offer(request, session_id)
            elif request.method == HTTPMethod.PATCH.value:
                return await patch_offer(request, session_id)
        except Exception as e:
            logger.error(f"Failed to parse WebRTC request: {e}")
            return Response(content="Invalid WebRTC request", status_code=400)

    logger.info(f"Received request for path: {path}")
    return Response(status_code=200)


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WebRTC demo")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="TRACE")
    else:
        logger.add(sys.stderr, level="DEBUG")

    uvicorn.run(app, host=args.host, port=args.port)



================================================
FILE: deployment/aws-agentcore-websocket/README.md
================================================
# Amazon Bedrock AgentCore Runtime WebSocket Example

This example demonstrates how to deploy a Pipecat bot to **Amazon Bedrock AgentCore Runtime** using WebSockets for communication.

## Prerequisites

- Accounts with:
  - AWS
  - Deepgram
  - Cartesia
- Python 3.10 or higher
- `uv` package manager

## Environment Setup

### IAM Configuration

Configure your IAM user with the necessary policies for AgentCore usage. Start with these:

- `BedrockAgentCoreFullAccess`
- A new policy (maybe named `BedrockAgentCoreCLI`) configured [like this](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html#runtime-permissions-starter-toolkit)

You can also choose to specify more granular permissions; see [Amazon Bedrock AgentCore docs](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/runtime-permissions.html) for more information.

### Environment Variable Setup

1. For agent management (configuring, deploying, etc.):

   Either export your AWS credentials and configuration as environment variables:

   ```bash
   export AWS_SECRET_ACCESS_KEY=...
   export AWS_ACCESS_KEY_ID=...
   export AWS_REGION=...
   ```

   Or use AWS CLI configuration:

   ```bash
   aws configure
   ```

2. For the agent itself:

   ```bash
   cd agent
   cp env.example .env
   ```

   Add your API keys:

   - `AWS_ACCESS_KEY_ID`: Your AWS access key ID for the Amazon Bedrock LLM used by the agent
   - `AWS_SECRET_ACCESS_KEY`: Your AWS secret access key for the Amazon Bedrock LLM used by the agent
   - `AWS_REGION`: The AWS region for the Amazon Bedrock LLM used by the agent
   - `DEEPGRAM_API_KEY`: Your Deepgram API key
   - `CARTESIA_API_KEY`: Your Cartesia API key

3. For the server:

   ```bash
   cd server
   cp env.example .env
   ```

   Add your AWS credentials and configuration, for generating a signed WebSocket URL in the `/start` endpoint:

   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
   - `AWS_REGION`

### Virtual Environment Setup

Create and activate a virtual environment for managing the agent:

```bash
uv sync
```

## Agent Configuration

Configure your Pipecat agent as an AgentCore agent:

```bash
uv run agentcore configure -e agent/agent.py
```

Follow the prompts to complete the configuration. It's fine to just accept all defaults.

## ⚠️ Before Proceeding

Just in case you've previously deployed other agents to AgentCore, ensure that you have the desired agent selected as "default" in the `agentcore` tool:

```
# Check
uv run agentcore configure list
# Set
uv run agentcore configure set-default <agent-name>
```

The following steps act on `agentcore`'s default agent.

## Deploying to AgentCore

Deploy your configured agent to Amazon Bedrock AgentCore Runtime for production hosting.

```bash
./scripts/launch.sh
```

You should see commands related to tailing logs printed to the console. Copy and save them for later use.

This is also the command you need to run after you've updated your agent code.

## Running the Server

The server provides a `/start` endpoint that generates WebSocket URLs for the client to connect to the agent.

See [the server README](./server/README.md) for setup and run instructions.

## Running the Client

Once the server is running, you can run the client to connect to your AgentCore-hosted agent.

See [the client README](./client/README.md) for setup and run instructions.

## Testing Locally

To test agent logic locally before deploying to AgentCore Runtime, do the following.

First, run the agent locally:

```bash
cd agent
uv run python agent.py
```

This will make the agent reachable at "ws://localhost:8080/ws".

Then, run the server as usual, but with the `LOCAL_AGENT=1` environment variable:

```bash
LOCAL_AGENT=1 uv run python server.py
```

You can then [run your client as usual](#running-the-client).

## Observation

Paste one of the log tailing commands you received when deploying your agent to AgentCore Runtime. It should look something like:

```bash
# Replace with your actual command
aws logs tail /aws/bedrock-agentcore/runtimes/foo-0uJkkT7QHC-DEFAULT --log-stream-name-prefix "2025/11/19/[runtime-logs]" --follow
```

If you don't have that command handy, no worries. Just run:

```bash
uv run agentcore status
```

## Agent Deletion

Delete your agent from AgentCore:

```bash
uv run agentcore destroy
```

## Additional Resources

For a comprehensive guide to getting started with Amazon Bedrock AgentCore, including detailed setup instructions, see the [Amazon Bedrock AgentCore Developer Guide](https://docs.aws.amazon.com/bedrock-agentcore/latest/devguide/what-is-bedrock-agentcore.html).



================================================
FILE: deployment/aws-agentcore-websocket/pyproject.toml
================================================
[project]
name = "agentcore-pipecat-websocket"
version = "0.1.0"
description = "Example for building Pipecat bots deployable to Amazon Bedrock AgentCore using WebSockets for communication"
requires-python = ">=3.10"
dependencies = [
]

[dependency-groups]
dev = [
    "bedrock-agentcore-starter-toolkit",
    "pyright>=1.1.404,<2",
    "ruff>=0.12.11,<1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]



================================================
FILE: deployment/aws-agentcore-websocket/agent/agent.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from bedrock_agentcore import BedrockAgentCoreApp
from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame, TTSSpeakFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.services.aws.llm import AWSBedrockLLMService
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams, FastAPIWebsocketTransport

app = BedrockAgentCoreApp()

load_dotenv(override=True)


async def fetch_weather_from_api(params: FunctionCallParams):
    await params.result_callback({"conditions": "nice", "temperature": "75"})


async def fetch_restaurant_recommendation(params: FunctionCallParams):
    await params.result_callback({"name": "The Golden Dragon"})


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    # Automatically uses AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION env vars.
    llm = AWSBedrockLLMService(
        model="us.amazon.nova-2-lite-v1:0",
        params=AWSBedrockLLMService.InputParams(temperature=0.8),
    )

    # You can also register a function_name of None to get all functions
    # sent to the same callback with an additional function_name parameter.
    llm.register_function("get_current_weather", fetch_weather_from_api)
    llm.register_function("get_restaurant_recommendation", fetch_restaurant_recommendation)

    @llm.event_handler("on_function_calls_started")
    async def on_function_calls_started(service, function_calls):
        await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

    weather_function = FunctionSchema(
        name="get_current_weather",
        description="Get the current weather",
        properties={
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
            "format": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "The temperature unit to use. Infer this from the user's location.",
            },
        },
        required=["location", "format"],
    )
    restaurant_function = FunctionSchema(
        name="get_restaurant_recommendation",
        description="Get a restaurant recommendation",
        properties={
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
        },
        required=["location"],
    )
    tools = ToolsSchema(standard_tools=[weather_function, restaurant_function])

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a voice call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be spoken aloud, so avoid special characters that can't easily be spoken, such as emojis or bullet points. Respond to what the user said in a creative and helpful way.",
        },
        {"role": "user", "content": "Say hello and briefly introduce yourself."},
    ]

    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor()

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        idle_timeout_secs=runner_args.pipeline_idle_timeout_secs,
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.info(f"Client ready")
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


@app.websocket
async def agentcore_bot(websocket, context):
    """Bot entry point for running on Amazon Bedrock AgentCore Runtime."""
    await websocket.accept()

    transport = FastAPIWebsocketTransport(
        websocket=websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV3(),
            serializer=ProtobufFrameSerializer(),
        ),
    )

    await run_bot(transport, RunnerArguments())


if __name__ == "__main__":
    # Running on AgentCore Runtime
    app.run()



================================================
FILE: deployment/aws-agentcore-websocket/agent/env.example
================================================
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=...
DEEPGRAM_API_KEY=...
CARTESIA_API_KEY=...



================================================
FILE: deployment/aws-agentcore-websocket/agent/pyproject.toml
================================================
[project]
name = "agentcore-pipecat-websocket-agent"
version = "0.1.0"
description = "Pipecat bot agent for Amazon Bedrock AgentCore"
requires-python = ">=3.10"
dependencies = [
    "bedrock-agentcore",
    "pipecat-ai[aws,cartesia,daily,deepgram,runner,silero,websocket,local-smart-turn-v3]",
]



================================================
FILE: deployment/aws-agentcore-websocket/client/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Deploy the agent. See the [README](../README).

2. Navigate to the `client` directory:

```bash
cd client
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: deployment/aws-agentcore-websocket/client/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
<div class="container">
  <div class="status-bar">
    <div class="status">
      Transport: <span id="connection-status">Disconnected</span>
    </div>
    <div class="controls">
      <button id="connect-btn">Connect</button>
      <button id="disconnect-btn" disabled>Disconnect</button>
    </div>
  </div>

  <audio id="bot-audio" autoplay></audio>

  <div class="debug-panel">
    <h3>Debug Info</h3>
    <div id="debug-log"></div>
  </div>
</div>

<script type="module" src="/src/app.ts"></script>
<link rel="stylesheet" href="/src/style.css">
</body>

</html>



================================================
FILE: deployment/aws-agentcore-websocket/client/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.15.30",
    "@types/protobufjs": "^6.0.0",
    "typescript": "^5.8.3",
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.5.0",
    "@pipecat-ai/websocket-transport": "^1.5.0",
    "protobufjs": "^7.4.0"
  }
}



================================================
FILE: deployment/aws-agentcore-websocket/client/tsconfig.json
================================================
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */

    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */

    /* Language and Environment */
    "target": "es2016",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for legacy experimental decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */

    /* Modules */
    "module": "commonjs",                                /* Specify what module code is generated. */
    // "rootDir": "./",                                  /* Specify the root folder within your source files. */
    // "moduleResolution": "node10",                     /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "allowImportingTsExtensions": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */
    // "rewriteRelativeImportExtensions": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */
    // "resolvePackageJsonExports": true,                /* Use the package.json 'exports' field when resolving package imports. */
    // "resolvePackageJsonImports": true,                /* Use the package.json 'imports' field when resolving imports. */
    // "customConditions": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */
    // "noUncheckedSideEffectImports": true,             /* Check side effect imports. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "allowArbitraryExtensions": true,                 /* Enable importing files with any extension, provided a declaration file is present. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */

    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */

    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    // "outDir": "./",                                   /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */

    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "verbatimModuleSyntax": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */
    // "isolatedDeclarations": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true,            /* Ensure that casing is correct in imports. */

    /* Type Checking */
    "strict": true,                                      /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "strictBuiltinIteratorReturn": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */

    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true                                 /* Skip type checking all .d.ts files. */
  }
}



================================================
FILE: deployment/aws-agentcore-websocket/client/src/app.ts
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

/**
 * Pipecat Client Implementation
 *
 * This client connects to an RTVI-compatible bot server using WebSocket.
 *
 * Requirements:
 * - A running RTVI bot server (defaults to http://localhost:7860)
 */

import {
  PipecatClient,
  PipecatClientOptions,
  RTVIEvent,
} from "@pipecat-ai/client-js";
import { WebSocketTransport } from "@pipecat-ai/websocket-transport";

class WebsocketClientApp {
  private pcClient: PipecatClient | null = null;
  private connectBtn: HTMLButtonElement | null = null;
  private disconnectBtn: HTMLButtonElement | null = null;
  private statusSpan: HTMLElement | null = null;
  private debugLog: HTMLElement | null = null;
  private botAudio: HTMLAudioElement;

  constructor() {
    console.log("WebsocketClientApp");
    this.botAudio = document.createElement("audio");
    this.botAudio.autoplay = true;
    //this.botAudio.playsInline = true;
    document.body.appendChild(this.botAudio);

    this.setupDOMElements();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      "connect-btn"
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      "disconnect-btn"
    ) as HTMLButtonElement;
    this.statusSpan = document.getElementById("connection-status");
    this.debugLog = document.getElementById("debug-log");
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  private setupEventListeners(): void {
    this.connectBtn?.addEventListener("click", () => this.connect());
    this.disconnectBtn?.addEventListener("click", () => this.disconnect());
  }

  /**
   * Add a timestamped message to the debug log
   */
  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement("div");
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith("User: ")) {
      entry.style.color = "#2196F3";
    } else if (message.startsWith("Bot: ")) {
      entry.style.color = "#4CAF50";
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.pcClient) return;
    const tracks = this.pcClient.tracks();
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.pcClient) return;

    // Listen for new tracks starting
    this.pcClient.on(RTVIEvent.TrackStarted, (track, participant) => {
      // Only handle non-local (bot) tracks
      if (!participant?.local && track.kind === "audio") {
        this.setupAudioTrack(track);
      }
    });

    // Listen for tracks stopping
    this.pcClient.on(RTVIEvent.TrackStopped, (track, participant) => {
      this.log(
        `Track stopped: ${track.kind} from ${participant?.name || "unknown"}`
      );
    });
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  private setupAudioTrack(track: MediaStreamTrack): void {
    this.log("Setting up audio track");
    if (
      this.botAudio.srcObject &&
      "getAudioTracks" in this.botAudio.srcObject
    ) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the Pipecat client, initializes devices, and establishes the connection
   */
  public async connect(): Promise<void> {
    try {
      const startTime = Date.now();

      //const transport = new DailyTransport();
      const PipecatConfig: PipecatClientOptions = {
        transport: new WebSocketTransport(),
        enableMic: true,
        enableCam: false,
        callbacks: {
          onConnected: () => {
            this.updateStatus("Connected");
            if (this.connectBtn) this.connectBtn.disabled = true;
            if (this.disconnectBtn) this.disconnectBtn.disabled = false;
          },
          onDisconnected: () => {
            this.updateStatus("Disconnected");
            if (this.connectBtn) this.connectBtn.disabled = false;
            if (this.disconnectBtn) this.disconnectBtn.disabled = true;
            this.log("Client disconnected");
          },
          onBotReady: (data) => {
            this.log(`Bot ready: ${JSON.stringify(data)}`);
            this.setupMediaTracks();
          },
          onUserTranscript: (data) => {
            if (data.final) {
              this.log(`User: ${data.text}`);
            }
          },
          onBotTranscript: (data) => this.log(`Bot: ${data.text}`),
          onMessageError: (error) => console.error("Message error:", error),
          onError: (error) => console.error("Error:", error),
        },
      };
      this.pcClient = new PipecatClient(PipecatConfig);
      // @ts-ignore
      window.pcClient = this.pcClient; // Expose for debugging
      this.setupTrackListeners();

      this.log("Initializing devices...");
      await this.pcClient.initDevices();

      this.log("Connecting to bot...");
      await this.pcClient.startBotAndConnect({
        // The baseURL and endpoint of your bot server that the client will connect to
        endpoint: "http://localhost:7860/start",
      });

      const timeTaken = Date.now() - startTime;
      this.log(`Connection complete, timeTaken: ${timeTaken}`);
    } catch (error) {
      this.log(`Error connecting: ${(error as Error).message}`);
      this.updateStatus("Error");
      // Clean up if there's an error
      if (this.pcClient) {
        try {
          await this.pcClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  public async disconnect(): Promise<void> {
    if (this.pcClient) {
      try {
        await this.pcClient.disconnect();
        this.pcClient = null;
        if (
          this.botAudio.srcObject &&
          "getAudioTracks" in this.botAudio.srcObject
        ) {
          this.botAudio.srcObject
            .getAudioTracks()
            .forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }
      } catch (error) {
        this.log(`Error disconnecting: ${(error as Error).message}`);
      }
    }
  }
}

declare global {
  interface Window {
    WebsocketClientApp: typeof WebsocketClientApp;
  }
}

window.addEventListener("DOMContentLoaded", () => {
  window.WebsocketClientApp = WebsocketClientApp;
  new WebsocketClientApp();
});



================================================
FILE: deployment/aws-agentcore-websocket/client/src/style.css
================================================
body {
    margin: 0;
    padding: 20px;
    font-family: Arial, sans-serif;
    background-color: #f0f0f0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
}

.status-bar {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px;
    background-color: #fff;
    border-radius: 8px;
    margin-bottom: 20px;
}

.controls button {
    padding: 8px 16px;
    margin-left: 10px;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}

#connect-btn {
    background-color: #4caf50;
    color: white;
}

#disconnect-btn {
    background-color: #f44336;
    color: white;
}

button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.main-content {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.bot-container {
    display: flex;
    flex-direction: column;
    align-items: center;
}

#bot-video-container {
    width: 640px;
    height: 360px;
    background-color: #e0e0e0;
    border-radius: 8px;
    margin: 20px auto;
    overflow: hidden;
    display: flex;
    align-items: center;
    justify-content: center;
}

#bot-video-container video {
    width: 100%;
    height: 100%;
    object-fit: cover;
}

.debug-panel {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
}

.debug-panel h3 {
    margin: 0 0 10px 0;
    font-size: 16px;
    font-weight: bold;
}

#debug-log {
    height: 500px;
    overflow-y: auto;
    background-color: #f8f8f8;
    padding: 10px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 12px;
    line-height: 1.4;
}



================================================
FILE: deployment/aws-agentcore-websocket/scripts/launch.sh
================================================
#!/bin/bash

# Script to dynamically read all variables from .env file and launch agentcore
AGENT_ENV_FILE="./agent/.env"
SERVER_ENV_FILE="./server/.env"

###############################################
# STEP 1 — Launch the new agent
###############################################

# Check if the local .env file exists
if [ ! -f "$AGENT_ENV_FILE" ]; then
    echo "Error: $AGENT_ENV_FILE file not found"
    echo "Please create an agent .env file with your environment variables"
    exit 1
fi

# Start building the agentcore launch command
LAUNCH_CMD="uv run agentcore launch --auto-update-on-conflict"
FOUND_ENV_VARS=false

echo "Loading environment variables from agent .env file..."

# Read each line from agent .env file and process it
while IFS= read -r line || [ -n "$line" ]; do
    # Skip empty lines & comments
    [[ -z "$line" || "$line" =~ ^[[:space:]]*# ]] && continue

    # Ensure line contains KEY=value
    if [[ "$line" =~ ^[^=]+=(.*)$ ]]; then
        VAR_NAME="${line%%=*}"
        VAR_VALUE="${line#*=}"

        # Remove surrounding whitespace
        VAR_NAME="$(echo "$VAR_NAME" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"
        VAR_VALUE="$(echo "$VAR_VALUE" | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')"

        # Skip PIPECAT_LOCAL_DEV
        if [[ "$VAR_NAME" == "PIPECAT_LOCAL_DEV" ]]; then
            echo "  Skipping: $VAR_NAME (ignored for deployment)"
            continue
        fi

        # Skip if variable name or value is empty
        if [[ -n "$VAR_NAME" && -n "$VAR_VALUE" ]]; then
            # Always quote the value so special characters are preserved
            LAUNCH_CMD+=" --env $VAR_NAME=\"$VAR_VALUE\""
            FOUND_ENV_VARS=true
            echo "  Added: $VAR_NAME"
        fi
    fi
done < "$AGENT_ENV_FILE"

# Check if any environment variables were added
if ! $FOUND_ENV_VARS; then
    echo "Warning: No valid environment variables found in agent .env file"
    echo "Make sure your agent .env file contains variables in the format: KEY=value"
    exit 1
fi

# Execute the command
echo ""
echo "Executing: $LAUNCH_CMD"
eval "$LAUNCH_CMD"


###############################################
# STEP 2 — Read AGENT ARN from agentcore status
###############################################
echo "Reading Agent ARN from agentcore status..."

# Extract Agent ARN from status output (removing box formatting characters and spaces)
AGENT_ARN=$(uv run agentcore status | grep "Agent ARN:" | sed 's/.*Agent ARN: //' | sed 's/│//g' | xargs)

echo "Agent ARN: $AGENT_ARN"

###############################################
# STEP 3 — Update server .env
###############################################
if [ ! -f "$SERVER_ENV_FILE" ]; then
    echo "ERROR: $SERVER_ENV_FILE not found!"
    exit 1
fi

# If AGENT_RUNTIME_ARN already exists → replace
# If not → append
if grep -q "^AGENT_RUNTIME_ARN=" "$SERVER_ENV_FILE"; then
    sed -i.bak "s|^AGENT_RUNTIME_ARN=.*|AGENT_RUNTIME_ARN=$AGENT_ARN|" "$SERVER_ENV_FILE"
else
    echo "AGENT_RUNTIME_ARN=$AGENT_ARN" >> "$SERVER_ENV_FILE"
fi

echo ".env updated successfully!"
echo "AGENT_RUNTIME_ARN is now set to:"
echo "$AGENT_ARN"


================================================
FILE: deployment/aws-agentcore-websocket/server/README.md
================================================
# Server

This server provides a `/start` endpoint that generates WebSocket URLs for connecting to the agent running on Amazon Bedrock AgentCore.

## Prerequisites

Before deploying your agent, configure your environment variables:

1. Copy the environment example file:

   ```bash
   cp env.example .env
   ```

2. Edit `.env` and fill in your AWS credentials and configuration:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
   - `AWS_REGION`

## Setup

Install dependencies:

```bash
uv sync
```

## Running the Server

Start the server on port 7860:

```bash
uv run python server.py
```

The server will be available at `http://localhost:7860`.

## Running the Server in Local Agent Mode

If you want to test a locally-running agent (reachable at "ws://localhost:8080/ws"), start the server like this:

```bash
LOCAL_AGENT=1 uv run python server.py
```

## Endpoint

### POST /start

Returns a WebSocket URL for the client to connect to the agent running on Amazon Bedrock AgentCore.

**Response:**

```json
{
  "ws_url": "wss://bedrock-agentcore.us-west-2.amazonaws.com/runtimes/..."
}
```



================================================
FILE: deployment/aws-agentcore-websocket/server/env.example
================================================
# AWS Credentials
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=...

# Agent Runtime ARN from your AgentCore deployment.
# No need to edit this by hand; it will be filled in automatically by launch.sh, the agent deployment script.
AGENT_RUNTIME_ARN=...



================================================
FILE: deployment/aws-agentcore-websocket/server/pyproject.toml
================================================
[project]
name = "aws-agentcore-websocket-server"
version = "0.1.0"
description = "Server for AWS AgentCore WebSocket example"
requires-python = ">=3.10"
dependencies = [
    "fastapi>=0.115.6",
    "uvicorn>=0.34.0",
    "python-dotenv>=1.0.1",
    "boto3>=1.35.85",
]



================================================
FILE: deployment/aws-agentcore-websocket/server/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
from contextlib import asynccontextmanager
from typing import Any, Dict
from urllib.parse import quote

import uvicorn
from botocore.auth import SigV4QueryAuth
from botocore.awsrequest import AWSRequest
from botocore.credentials import Credentials
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware

load_dotenv(override=True)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handles FastAPI startup and shutdown."""
    yield


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/start")
async def start_bot(request: Request) -> Dict[Any, Any]:
    """
    Start endpoint that generates a signed WebSocket URL for connecting to the agent on AWS Bedrock AgentCore.

    Returns:
        Dict[Any, Any]: Contains the signed WebSocket URL for the client to connect
    """
    # Check if LOCAL_AGENT mode is enabled
    if os.getenv("LOCAL_AGENT") == "1":
        return {"ws_url": "ws://localhost:8080/ws"}

    # Get required environment variables
    access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
    secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
    agent_runtime_arn = os.getenv("AGENT_RUNTIME_ARN")
    region = os.getenv("AWS_REGION")

    if not access_key_id or not secret_access_key or not agent_runtime_arn or not region:
        raise HTTPException(
            status_code=500,
            detail="Missing required environment variables: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AGENT_RUNTIME_ARN, or AWS_REGION",
        )

    try:
        # Construct the WebSocket URL
        ws_url = f"wss://bedrock-agentcore.{region}.amazonaws.com/runtimes/{quote(agent_runtime_arn, safe='')}/ws"

        # Create AWS credentials
        credentials = Credentials(access_key_id, secret_access_key)

        # Create an AWS request for signing
        aws_request = AWSRequest(method="GET", url=ws_url)

        # Sign the request using SigV4QueryAuth (adds signature to query string)
        SigV4QueryAuth(credentials, "bedrock-agentcore", region).add_auth(aws_request)

        # Get the signed URL
        signed_url = aws_request.url

        return {"ws_url": signed_url}

    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Failed to generate signed WebSocket URL: {str(e)}"
        )


if __name__ == "__main__":
    config = uvicorn.Config(app, host="0.0.0.0", port=7860)
    server = uvicorn.Server(config)
    server.run()



================================================
FILE: deployment/flyio-example/README.md
================================================
# Fly.io deployment example

This project modifies the `bot_runner.py` server to launch a new machine for each user session. This is a recommended approach for production vs. running shell processess as your deployment will quickly run out of system resources under load.

For this example, we are using Daily as a WebRTC transport and provisioning a new room and token for each session. You can use another transport, such as WebSockets, by modifying the `bot.py` and `bot_runner.py` files accordingly.

## Setting up your fly.io deployment

### Create your fly.toml file

You can copy the `example-fly.toml` as a reference. Be sure to change the app name to something unique.

### Create your .env file

Copy the base `env.example` to `.env` and enter the necessary API keys.

`FLY_APP_NAME` should match that in the `fly.toml` file.

### Launch a new fly.io project

`fly launch` or `fly launch --org your-org-name`

### Set the necessary app secrets from your .env

Note: you can do this manually via the fly.io dashboard under the "secrets" sub-section of your deployment (e.g. "https://fly.io/apps/fly-app-name/secrets") or run the following terminal command:

`cat .env | tr '\n' ' ' | xargs flyctl secrets set`

### Deploy your machine

`fly deploy`

## Connecting to your bot

Send a post request to your running fly.io instance:

`curl --location --request POST 'https://YOUR_FLY_APP_NAME/'`

This request will wait until the machine enters into a `starting` state, before returning the a room URL and token to join.



================================================
FILE: deployment/flyio-example/__init__.py
================================================
[Empty file]


================================================
FILE: deployment/flyio-example/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import asyncio
import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import EndFrame, LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

daily_api_key = os.getenv("DAILY_API_KEY", "")
daily_api_url = os.getenv("DAILY_API_URL", "https://api.daily.co/v1")


async def main(room_url: str, token: str):
    transport = DailyTransport(
        room_url,
        token,
        "Chatbot",
        DailyParams(
            api_url=daily_api_url,
            api_key=daily_api_key,
            audio_in_enabled=True,
            audio_out_enabled=True,
            video_out_enabled=False,
            vad_analyzer=SileroVADAnalyzer(),
            transcription_enabled=True,
        ),
    )

    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY", ""),
        voice_id=os.getenv("ELEVENLABS_VOICE_ID", ""),
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are Chatbot, a friendly, helpful robot. Your output will be converted to audio so don't include special characters other than '!' or '?' in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by saying hello.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        await transport.capture_participant_transcription(participant["id"])
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        await task.cancel()

    @transport.event_handler("on_call_state_updated")
    async def on_call_state_updated(transport, state):
        if state == "left":
            # Here we don't want to cancel, we just want to finish sending
            # whatever is queued, so we use an EndFrame().
            await task.queue_frame(EndFrame())

    runner = PipelineRunner()

    await runner.run(task)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipecat Bot")
    parser.add_argument("-u", type=str, help="Room URL")
    parser.add_argument("-t", type=str, help="Token")
    config = parser.parse_args()

    asyncio.run(main(config.u, config.t))



================================================
FILE: deployment/flyio-example/bot_runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os
import subprocess
from contextlib import asynccontextmanager

import aiohttp
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pipecat.transports.daily.utils import (
    DailyRESTHelper,
    DailyRoomObject,
    DailyRoomParams,
    DailyRoomProperties,
)

load_dotenv(override=True)


# ------------ Configuration ------------ #

MAX_SESSION_TIME = 5 * 60  # 5 minutes
REQUIRED_ENV_VARS = [
    "DAILY_API_KEY",
    "OPENAI_API_KEY",
    "ELEVENLABS_API_KEY",
    "ELEVENLABS_VOICE_ID",
    "FLY_API_KEY",
    "FLY_APP_NAME",
]

FLY_API_HOST = os.getenv("FLY_API_HOST", "https://api.machines.dev/v1")
FLY_APP_NAME = os.getenv("FLY_APP_NAME", "pipecat-fly-example")
FLY_API_KEY = os.getenv("FLY_API_KEY", "")
FLY_HEADERS = {"Authorization": f"Bearer {FLY_API_KEY}", "Content-Type": "application/json"}

daily_helpers = {}


# ----------------- API ----------------- #


@asynccontextmanager
async def lifespan(app: FastAPI):
    aiohttp_session = aiohttp.ClientSession()
    daily_helpers["rest"] = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )
    yield
    await aiohttp_session.close()


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ----------------- Main ----------------- #


async def spawn_fly_machine(room_url: str, token: str):
    async with aiohttp.ClientSession() as session:
        # Use the same image as the bot runner
        async with session.get(
            f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines", headers=FLY_HEADERS
        ) as r:
            if r.status != 200:
                text = await r.text()
                raise Exception(f"Unable to get machine info from Fly: {text}")

            data = await r.json()
            image = data[0]["config"]["image"]

        # Machine configuration
        cmd = f"python3 bot.py -u {room_url} -t {token}"
        cmd = cmd.split()
        worker_props = {
            "config": {
                "image": image,
                "auto_destroy": True,
                "init": {"cmd": cmd},
                "restart": {"policy": "no"},
                "guest": {"cpu_kind": "shared", "cpus": 1, "memory_mb": 1024},
            },
        }

        # Spawn a new machine instance
        async with session.post(
            f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines", headers=FLY_HEADERS, json=worker_props
        ) as r:
            if r.status != 200:
                text = await r.text()
                raise Exception(f"Problem starting a bot worker: {text}")

            data = await r.json()
            # Wait for the machine to enter the started state
            vm_id = data["id"]

        async with session.get(
            f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines/{vm_id}/wait?state=started",
            headers=FLY_HEADERS,
        ) as r:
            if r.status != 200:
                text = await r.text()
                raise Exception(f"Bot was unable to enter started state: {text}")

    print(f"Machine joined room: {room_url}")


@app.post("/")
async def start_bot(request: Request) -> JSONResponse:
    try:
        data = await request.json()
        # Is this a webhook creation request?
        if "test" in data:
            return JSONResponse({"test": True})
    except Exception as e:
        pass

    # Use specified room URL, or create a new one if not specified
    room_url = os.getenv("DAILY_SAMPLE_ROOM_URL", "")

    if not room_url:
        params = DailyRoomParams(properties=DailyRoomProperties())
        try:
            room: DailyRoomObject = await daily_helpers["rest"].create_room(params=params)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Unable to provision room {e}")
    else:
        # Check passed room URL exists, we should assume that it already has a sip set up
        try:
            room: DailyRoomObject = await daily_helpers["rest"].get_room_from_url(room_url)
        except Exception:
            raise HTTPException(status_code=500, detail=f"Room not found: {room_url}")

    # Give the agent a token to join the session
    token = await daily_helpers["rest"].get_token(room.url, MAX_SESSION_TIME)

    if not room or not token:
        raise HTTPException(status_code=500, detail=f"Failed to get token for room: {room_url}")

    # Launch a new fly.io machine, or run as a shell process (not recommended)
    run_as_process = os.getenv("RUN_AS_PROCESS", False)

    if run_as_process:
        try:
            subprocess.Popen(
                [f"python3 -m bot -u {room.url} -t {token}"],
                shell=True,
                bufsize=1,
                cwd=os.path.dirname(os.path.abspath(__file__)),
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")
    else:
        try:
            await spawn_fly_machine(room.url, token)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to spawn VM: {e}")

    # Grab a token for the user to join with
    user_token = await daily_helpers["rest"].get_token(room.url, MAX_SESSION_TIME)

    return JSONResponse(
        {
            "room_url": room.url,
            "token": user_token,
        }
    )


if __name__ == "__main__":
    # Check environment variables
    for env_var in REQUIRED_ENV_VARS:
        if env_var not in os.environ:
            raise Exception(f"Missing environment variable: {env_var}.")

    parser = argparse.ArgumentParser(description="Pipecat Bot Runner")
    parser.add_argument(
        "--host", type=str, default=os.getenv("HOST", "0.0.0.0"), help="Host address"
    )
    parser.add_argument("--port", type=int, default=os.getenv("PORT", 7860), help="Port number")
    parser.add_argument(
        "--reload", action="store_true", default=False, help="Reload code on change"
    )

    config = parser.parse_args()

    try:
        import uvicorn

        uvicorn.run("bot_runner:app", host=config.host, port=config.port, reload=config.reload)
    except KeyboardInterrupt:
        print("Pipecat runner shutting down...")



================================================
FILE: deployment/flyio-example/Dockerfile
================================================
FROM python:3.11-bullseye

# Open port 7860 for http service
ENV FAST_API_PORT=7860
EXPOSE 7860

# Install Python dependencies
COPY *.py .
COPY ./requirements.txt requirements.txt
RUN pip3 install --no-cache-dir --upgrade -r requirements.txt

# Start the FastAPI server
CMD python3 bot_runner.py --port ${FAST_API_PORT}


================================================
FILE: deployment/flyio-example/env.example
================================================
DAILY_API_KEY=
DAILY_SAMPLE_ROOM_URL= # Enter a Daily room URL to use a set room URL each time (useful for local testing)
OPENAI_API_KEY=
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=
FLY_API_KEY=
FLY_APP_NAME=
RUN_AS_PROCESS= # Spawn fly.io machine for each session or run as local process


================================================
FILE: deployment/flyio-example/example-fly.toml
================================================
# fly.toml app configuration file generated for pipecat-fly-example on 2024-07-01T15:04:53+01:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'pipecat-fly-example'
primary_region = 'sjc'

[build]

[env]
  FLY_APP_NAME = 'pipecat-fly-example'

[http_service]
  internal_port = 7860
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

[[vm]]
  memory = 512
  cpu_kind = 'shared'
  cpus = 1



================================================
FILE: deployment/flyio-example/requirements.txt
================================================
pipecat-ai[daily,openai,silero]>=0.0.82
fastapi
uvicorn
python-dotenv
loguru



================================================
FILE: deployment/modal-example/README.md
================================================
# Deploying Pipecat to Modal.com

Deployment example for [modal.com](https://www.modal.com). This example demonstrates how to deploy a FastAPI webapp to Modal with an RTVI compatible `/connect` endpoint that launches a Pipecat pipeline in a separate Modal container and returns a room/token for the client to join. This example also supports providing a parameter to the `/connect` endpoint for specifying which Pipecat pipeline to launch; openai, gemini, or vllm. The vllm pipeline points to a self-hosted OpenAI compatible LLM, using a llama model (neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16), deployed to Modal.

![](diagram.jpg)

# Running this Example

## Install the Modal CLI

Setup a Modal account and install it on your machine if you have not already, following their easy 3 steps in their [Getting Started Guide](https://modal.com/docs/guide#getting-started)

## Deploy a self-serve LLM

1. Deploy Modal's OpenAI-compatible LLM service:

   ```bash
   git clone https://github.com/modal-labs/modal-examples
   cd modal-examples
   modal deploy 06_gpu_and_ml/llm-serving/vllm_inference.py
   ```

   Refer to Modal's guide and example for [Deploying an OpenAI-compatible LLM service with vLLM](https://modal.com/docs/examples/vllm_inference) for more details.

2. Take note of the endpoint URL from the previous step, which will look like:
   ```
   https://{your-workspace}--example-vllm-openai-compatible-serve.modal.run
   ```
   You'll need this for the `bot_vllm.py` file in the next section.

    **Note:**  The default Modal LLM example uses Llama-3.1 and will shut down after 15 minutes of inactivity. Cold starts take 5-10 minutes. To prepare the service, we recommend visiting the `/docs` endpoint (`https://<Modal workspace>--example-vllm-openai-compatible-serve.modal.run/docs`) for your deployed LLM and wait for it to fully load before connecting your client.

## Deploy FastAPI App and Pipecat pipeline to Modal 

1. Setup environment variables

   ```bash
   cd server
   cp env.example .env
   # Modify .env to provide your service API Keys
   ```

   Alternatively, you can configure your Modal app to use [secrets](https://modal.com/docs/guide/secrets)

2. Update the `modal_url` in `server/src/bot_vllm.py` to point to the url produced from the self-serve llm deploy, mentioned above.

3. From within the `server` directory, test the app locally:

   ```bash
   modal serve app.py
   ```

4. Deploy to production

   ```bash
   modal deploy app.py
   ```

5. Note the endpoint URL produced from this deployment. It will look like:

   ```bash
   https://{your-workspace}--pipecat-modal-fastapi-app.modal.run
   ```

   You'll need this URL for the client's `app.js` configuration mentioned in its README.

## Launch your bots on Modal

### Option 1: Direct Link

Simply click on the url displayed after running the server or deploy step to launch an agent and be redirected to a Daily room to talk with the launched bot. This will use the OpenAI pipeline.

### Option 2: Connect via an RTVI Client

Follow the instructions provided in the [client folder's README](client/javascript/README.md) for building and running a custom client that connects to your Modal endpoint. The provided client provides a dropdown for choosing which bot pipeline to run.

# Navigating your llm, server, and Pipecat logs

In your [Modal dashboard](https://modal.com/apps), you should have two Apps listed under Live Apps:

1. `example-vllm-openai-compatible`: This App contains the containers and logs used to run your self-hosted LLM. There will be just one App Function listed: `serve`. Click on this function to view logs for your LLM.
2. `pipecat-modal`: This App contains the containers and logs used to run your `connect` endpoints and Pipecat pipelines. It will list two App Functions:
    1. `fastapi_app`: This function is running the endpoints that your client will interact with and initiate starting a new pipeline (`/`, `/connect`, `/status`). Click on this function to see logs for each endpoint hit.
    2. `bot_runner`: This function handles launching and running a bot pipeline. Click on this function to get a list of all pipeline runs and access each run's logs.

# Modal + Pipecat Tips

- In most other Pipecat examples, we use `Popen` to launch the pipeline process from the `/connect` endpoint. In this example, we use a Modal function instead. This allows us to run the pipelines using a separately defined Modal image as well as run each pipeline in an isolated container.
- For the FastAPI and most common Pipecat Pipeline containers, a default `debian_slim` CPU-only should be all that's required to run. GPU containers are needed for self-hosted services.
- To minimize cold starts of the pipeline and reduce latency for users, set `min_containers=1` on the Modal Function that launches the pipeline to ensure at least one warm instance of your function is always available.
- For next steps on running a self-hosted llm and reducing latency, check out all of [Modal's LLM examples](https://modal.com/docs/examples/vllm_inference).



================================================
FILE: deployment/modal-example/client/javascript/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Deploy the Modal server. See the main [README](../../README).

2. Navigate to the `client/javascript` directory:

```bash
cd client/javascript
```

3. Modify the baseUrl in src/app.js to point to your deployed Modal endpoint

4. Install dependencies:

```bash
npm install
```

5. Run the client app:

```
npm run dev
```

6. Visit http://localhost:5173 in your browser.



================================================
FILE: deployment/modal-example/client/javascript/index.html
================================================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>AI Chatbot</title>
  </head>

  <body>
    <div class="container">
      <div class="status-bar">
        <div class="status">
          Status: <span id="connection-status">Disconnected</span>
        </div>
        <div class="controls">
          <select id="bot-selector">
            <option value="openai">OpenAI</option>
            <option value="gemini">Gemini</option>
            <option value="vllm">Llama</option>
          </select>
          <button id="connect-btn">Connect</button>
          <button id="disconnect-btn" disabled>Disconnect</button>
        </div>
      </div>

      <div class="main-content">
        <div class="bot-container">
          <div id="bot-video-container"></div>
          <audio id="bot-audio" autoplay></audio>
        </div>
      </div>

      <div class="device-bar">
        <div class="device-controls">
          <select id="device-selector"></select>
          <button id="mic-toggle-btn">Mute Mic</button>
        </div>
      </div>

      <div class="debug-panel">
        <h3>Debug Info</h3>
        <div id="debug-log"></div>
      </div>
    </div>

    <script type="module" src="/src/app.js"></script>
    <link rel="stylesheet" href="/src/style.css" />
  </body>
</html>



================================================
FILE: deployment/modal-example/client/javascript/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/daily-transport": "^1.2.0"
  }
}



================================================
FILE: deployment/modal-example/client/javascript/src/app.js
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

/**
 * Pipecat Client Implementation
 *
 * This client connects to an RTVI-compatible bot server using WebRTC (via Daily).
 * It handles audio/video streaming and manages the connection lifecycle.
 *
 * Requirements:
 * - A running RTVI bot server (defaults to http://localhost:7860)
 * - The server must implement the /connect endpoint that returns Daily.co room credentials
 * - Browser with WebRTC support
 */

import { PipecatClient, RTVIEvent } from '@pipecat-ai/client-js';
import { DailyTransport } from '@pipecat-ai/daily-transport';

/**
 * ChatbotClient handles the connection and media management for a real-time
 * voice and video interaction with an AI bot.
 */
class ChatbotClient {
  constructor() {
    // Initialize client state
    this.pcClient = null;
    this.setupDOMElements();
    this.initializeClientAndTransport();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  setupDOMElements() {
    // Get references to UI control elements
    this.connectBtn = document.getElementById('connect-btn');
    this.disconnectBtn = document.getElementById('disconnect-btn');
    this.statusSpan = document.getElementById('connection-status');
    this.debugLog = document.getElementById('debug-log');
    this.botVideoContainer = document.getElementById('bot-video-container');
    this.deviceSelector = document.getElementById('device-selector');

    // Create an audio element for bot's voice output
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    this.botAudio.playsInline = true;
    document.body.appendChild(this.botAudio);
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  setupEventListeners() {
    this.connectBtn.addEventListener('click', () => this.connect());
    this.disconnectBtn.addEventListener('click', () => this.disconnect());

    // Populate device selector
    this.pcClient.getAllMics().then((mics) => {
      console.log('Available mics:', mics);
      mics.forEach((device) => {
        const option = document.createElement('option');
        option.value = device.deviceId;
        option.textContent = device.label || `Microphone ${device.deviceId}`;
        this.deviceSelector.appendChild(option);
      });
    });
    this.deviceSelector.addEventListener('change', (event) => {
      const selectedDeviceId = event.target.value;
      console.log('Selected device ID:', selectedDeviceId);
      this.pcClient.updateMic(selectedDeviceId);
    });

    // Handle mic mute/unmute toggle
    const micToggleBtn = document.getElementById('mic-toggle-btn');

    micToggleBtn.addEventListener('click', () => {
      let micEnabled = this.pcClient.isMicEnabled;
      micToggleBtn.textContent = micEnabled ? 'Unmute Mic' : 'Mute Mic';
      this.pcClient.enableMic(!micEnabled);
      // Add logic to mute/unmute the mic
      if (micEnabled) {
        console.log('Mic muted');
        // Add code to mute the mic
      } else {
        console.log('Mic unmuted');
        // Add code to unmute the mic
      }
    });
  }

  /**
   * Set up the Pipecat client and Daily transport
   */
  async initializeClientAndTransport() {
    // Initialize the Pipecat client with a DailyTransport and our configuration
    this.pcClient = new PipecatClient({
      transport: new DailyTransport(),
      enableMic: true, // Enable microphone for user input
      enableCam: false,
      callbacks: {
        // Handle connection state changes
        onConnected: () => {
          this.updateStatus('Connected');
          this.connectBtn.disabled = true;
          this.disconnectBtn.disabled = false;
          this.log('Client connected');
        },
        onDisconnected: () => {
          this.updateStatus('Disconnected');
          this.connectBtn.disabled = false;
          this.disconnectBtn.disabled = true;
          this.log('Client disconnected');
        },
        // Handle transport state changes
        onTransportStateChanged: (state) => {
          this.updateStatus(`Transport: ${state}`);
          this.log(`Transport state changed: ${state}`);
          if (state === 'connecting') {
            window.startTime = Date.now();
          }
          if (state === 'ready') {
            this.setupMediaTracks();
            console.warn('TIME TO BOT READY:', Date.now() - window.startTime);
          }
        },
        // Handle bot connection events
        onBotConnected: (participant) => {
          this.log(`Bot connected: ${JSON.stringify(participant)}`);
        },
        onBotDisconnected: (participant) => {
          this.log(`Bot disconnected: ${JSON.stringify(participant)}`);
        },
        onBotReady: (data) => {
          this.log(`Bot ready: ${JSON.stringify(data)}`);
          this.setupMediaTracks();
        },
        // Transcript events
        onUserTranscript: (data) => {
          // Only log final transcripts
          if (data.final) {
            this.log(`User: ${data.text}`);
          }
        },
        onBotTranscript: (data) => {
          this.log(`Bot: ${data.text}`);
        },
        // Error handling
        onMessageError: (error) => {
          console.log('Message error:', error);
        },
        onMicUpdated: (data) => {
          console.log('Mic updated:', data);
          this.deviceSelector.value = data.deviceId;
        },
        onError: (error) => {
          console.log('Error:', JSON.stringify(error));
        },
      },
    });

    // Set up listeners for media track events
    this.setupTrackListeners();

    await this.pcClient.initDevices();
    window.client = this.pcClient;
  }

  /**
   * Add a timestamped message to the debug log
   */
  log(message) {
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;

    // Add styling based on message type
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3'; // blue for user
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50'; // green for bot
    }

    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  updateStatus(status) {
    this.statusSpan.textContent = status;
    this.log(`Status: ${status}`);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.pcClient) return;

    // Get current tracks from the client
    const tracks = this.pcClient.tracks();

    // Set up any available bot tracks
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
    if (tracks.bot?.video) {
      this.setupVideoTrack(tracks.bot.video);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.pcClient) return;

    // Listen for new tracks starting
    this.pcClient.on(RTVIEvent.TrackStarted, (track, participant) => {
      // Only handle non-local (bot) tracks
      if (!participant?.local) {
        if (track.kind === 'audio') {
          this.setupAudioTrack(track);
        } else if (track.kind === 'video') {
          this.setupVideoTrack(track);
        }
        this.log(
          `Track started event: ${track.kind} from ${
            participant?.name || 'unknown'
          }`
        );
      } else {
        this.log('Local mic unmuted');
      }
    });

    // Listen for tracks stopping
    this.pcClient.on(RTVIEvent.TrackStopped, (track, participant) => {
      if (participant.local) {
        this.log('Local mic muted');
        return;
      }
      this.log(
        `Track stopped event: ${track.kind} from ${
          participant?.name || 'unknown'
        }`
      );
    });
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  setupAudioTrack(track) {
    this.log('Setting up audio track');
    // Check if we're already playing this track
    if (this.botAudio.srcObject) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    // Create a new MediaStream with the track and set it as the audio source
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Set up a video track for display
   * Handles both initial setup and track updates
   */
  setupVideoTrack(track) {
    this.log('Setting up video track');
    const videoEl = document.createElement('video');
    videoEl.autoplay = true;
    videoEl.playsInline = true;
    videoEl.muted = true;
    videoEl.style.width = '100%';
    videoEl.style.height = '100%';
    videoEl.style.objectFit = 'cover';

    // Check if we're already displaying this track
    if (this.botVideoContainer.querySelector('video')?.srcObject) {
      const oldTrack = this.botVideoContainer
        .querySelector('video')
        .srcObject.getVideoTracks()[0];
      if (oldTrack?.id === track.id) return;
    }

    // Create a new MediaStream with the track and set it as the video source
    videoEl.srcObject = new MediaStream([track]);
    this.botVideoContainer.innerHTML = '';
    this.botVideoContainer.appendChild(videoEl);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the Pipecat client, initializes devices, and establishes the connection
   */
  async connect() {
    try {
      const botSelector = document.getElementById('bot-selector');
      const selectedBot = botSelector.value;

      // Initialize audio/video devices
      this.log('Initializing devices...');
      await this.pcClient.initDevices();

      // Connect to the bot
      this.log(`Connecting to bot: ${selectedBot}`);
      await this.pcClient.startBotAndConnect({
        // REPLACE WITH YOUR MODAL URL ENDPOINT
        endpoint:
          'https://<your-workspace>--pipecat-modal-fastapi-app.modal.run/connect',
        requestData: {
          bot_name: selectedBot,
        },
      });

      this.log('Connection complete');
    } catch (error) {
      // Handle any errors during connection
      console.error('Connection error:', error);
      this.log(`Error connecting: ${JSON.stringify(error.message)}`);
      this.log(`Error stack: ${error.stack}`);
      this.updateStatus('Error');

      // Clean up if there's an error
      if (this.pcClient) {
        try {
          await this.pcClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError.message}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  async disconnect() {
    if (this.pcClient) {
      try {
        // Disconnect the Pipecat client
        await this.pcClient.disconnect();

        // Clean up audio
        if (this.botAudio.srcObject) {
          this.botAudio.srcObject.getTracks().forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }

        // Clean up video
        if (this.botVideoContainer.querySelector('video')?.srcObject) {
          const video = this.botVideoContainer.querySelector('video');
          video.srcObject.getTracks().forEach((track) => track.stop());
          video.srcObject = null;
        }
        this.botVideoContainer.innerHTML = '';
      } catch (error) {
        this.log(`Error disconnecting: ${error.message}`);
      }
    }
  }
}

// Initialize the client when the page loads
window.addEventListener('DOMContentLoaded', () => {
  new ChatbotClient();
});



================================================
FILE: deployment/modal-example/client/javascript/src/style.css
================================================
body {
  margin: 0;
  padding: 20px;
  font-family: Arial, sans-serif;
  background-color: #f0f0f0;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
}

.status-bar,
.device-bar {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px;
  background-color: #fff;
  border-radius: 8px;
  margin-bottom: 20px;
}

.controls,
.device-controls {
  display: flex;
  align-items: center;
  gap: 10px; /* Adds spacing between elements */
}

.device-controls {
  margin-left: auto;
}

.controls button,
.device-controls button {
  padding: 8px 16px;
  margin-left: 10px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

#bot-selector,
#device-selector {
  padding: 8px 16px;
  padding-right: 40px;
  border: none;
  border-radius: 4px;
  background-color: #6c757d; /* Gray background */
  color: white; /* White text */
  cursor: pointer;
  appearance: none; /* Removes default browser styling for dropdowns */
  background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24' fill='white'%3E%3Cpath d='M7 10l5 5 5-5z'/%3E%3C/svg%3E"); /* Custom arrow */
  background-repeat: no-repeat;
  background-position: right 8px center; /* Position the arrow */
}

#bot-selector:focus,
#device-selector:focus {
  outline: none;
  box-shadow: 0 0 4px rgba(0, 0, 0, 0.3); /* Add a subtle focus effect */
}

#connect-btn {
  background-color: #4caf50;
  color: white;
}

#disconnect-btn {
  background-color: #f44336;
  color: white;
}

#mic-toggle-btn {
}

button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.main-content {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 20px;
}

.bot-container {
  display: flex;
  flex-direction: column;
  align-items: center;
}

#bot-video-container {
  width: 640px;
  height: 360px;
  background-color: #e0e0e0;
  border-radius: 8px;
  margin: 20px auto;
  overflow: hidden;
  display: flex;
  align-items: center;
  justify-content: center;
}

#bot-video-container video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}

.debug-panel {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
}

.debug-panel h3 {
  margin: 0 0 10px 0;
  font-size: 16px;
  font-weight: bold;
}

#debug-log {
  height: 200px;
  overflow-y: auto;
  background-color: #f8f8f8;
  padding: 10px;
  border-radius: 4px;
  font-family: monospace;
  font-size: 12px;
  line-height: 1.4;
}



================================================
FILE: deployment/modal-example/server/__init__.py
================================================
[Empty file]


================================================
FILE: deployment/modal-example/server/app.py
================================================
"""modal_example.

This module shows a simple example of how to deploy a bot using Modal and FastAPI.

It includes:
- FastAPI endpoints for starting agents and checking bot statuses.
- Dynamic loading of bot implementations.
- Use of a Daily transport for bot communication.
"""

#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import importlib
import os
from contextlib import asynccontextmanager
from typing import Any, Dict, Literal

import aiohttp
import modal
from fastapi import APIRouter, FastAPI, HTTPException
from fastapi.responses import JSONResponse, RedirectResponse
from pydantic import BaseModel

# container specifications for the FastAPI web server
web_image = (
    modal.Image.debian_slim(python_version="3.13")
    .pip_install_from_requirements("requirements.txt")
    .pip_install("pipecat-ai[daily]")
    .add_local_dir("src", remote_path="/root/src")
)

# container specifications for the Pipecat pipeline
bot_image = (
    modal.Image.debian_slim(python_version="3.13")
    .apt_install("ffmpeg")
    .pip_install_from_requirements("requirements.txt")
    .pip_install("pipecat-ai[daily,elevenlabs,openai,silero,google]")
    .add_local_dir("src", remote_path="/root/src")
)

app = modal.App("pipecat-modal", secrets=[modal.Secret.from_dotenv()])

router = APIRouter()

bot_jobs = {}
daily_helpers = {}

# Names of all supported bot implementations
# These correspond to the bot files in the src directory
BotName = Literal["openai", "gemini", "vllm"]


def cleanup():
    """Cleanup function to terminate all bot processes.

    Called during server shutdown.
    """
    for entry in bot_jobs.values():
        func = modal.FunctionCall.from_id(entry[0])
        if func:
            func.cancel()


def get_bot_file(bot_name: BotName) -> str:
    """Retrieve the bot file name corresponding to the provided bot_name.

    Args:
        bot_name (BotName): The name of the bot (e.g., 'openai', 'gemini', 'vllm').

    Returns:
        str: The file name corresponding to the bot implementation.

    Raises:
        ValueError: If the bot name is invalid or not supported.
    """
    # bot_implementation = os.getenv("BOT_IMPLEMENTATION", "openai").lower().strip()
    bot_implementation = bot_name.lower().strip()
    if not bot_implementation:
        bot_implementation = "openai"
    if bot_implementation not in ["openai", "gemini", "vllm"]:
        raise ValueError(
            f"Invalid BOT_IMPLEMENTATION: {bot_implementation}. Must be 'openai' or 'gemini' or 'vllm'"
        )

    return f"bot_{bot_implementation}"


def get_runner(path: str, bot_file: str) -> callable:
    """Dynamically import the run_bot function based on the bot name.

    Args:
        path (str): The path to the bot files (e.g., 'src').
        bot_file (str): The file name of the bot implementation (e.g., 'openai', 'gemini', 'vllm').

    Returns:
        function: The run_bot function from the specified bot module.

    Raises:
        ImportError: If the specified bot module or run_bot function is not found.
    """
    try:
        # Dynamically construct the module name
        module_name = f"{path}.{bot_file}"
        # Import the module
        module = importlib.import_module(module_name)
        # Get the run_bot function from the module
        return getattr(module, "run_bot")
    except (ImportError, AttributeError) as e:
        raise ImportError(f"Failed to import run_bot from {module_name}: {e}")


async def create_room_and_token() -> tuple[str, str]:
    """Create a Daily room and generate an authentication token.

    This function checks for existing room URL and token in the environment variables.
    If not found, it creates a new room using the Daily API and generates a token for it.

    Returns:
        tuple[str, str]: A tuple containing the room URL and the authentication token.

    Raises:
        HTTPException: If room creation or token generation fails.
    """
    from pipecat.transports.daily.utils import DailyRoomParams

    room_url = os.getenv("DAILY_SAMPLE_ROOM_URL", None)
    token = os.getenv("DAILY_SAMPLE_ROOM_TOKEN", None)
    if not room_url:
        room = await daily_helpers["rest"].create_room(DailyRoomParams())
        if not room.url:
            raise HTTPException(status_code=500, detail="Failed to create room")
        room_url = room.url

        token = await daily_helpers["rest"].get_token(room_url)
        if not token:
            raise HTTPException(status_code=500, detail=f"Failed to get token for room: {room_url}")

    return room_url, token


@app.function(image=bot_image, min_containers=1)
async def bot_runner(room_url, token, bot_name: BotName = "openai"):
    """Launch the provided bot process, providing the given room URL and token for the bot to join.

    Args:
        room_url (str): The URL of the Daily room where the bot and client will communicate.
        token (str): The authentication token for the room.
        bot_name (BotName): The name of the bot implementation to use. Defaults to "openai".

    Raises:
        HTTPException: If the bot pipeline fails to start.
    """
    try:
        path = "src"
        bot_file = get_bot_file(bot_name)
        run_bot = get_runner(path, bot_file)

        print(f"Starting bot process: {bot_file} -u {room_url} -t {token}")
        await run_bot(room_url, token)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start bot pipeline: {e}")


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan manager that handles startup and shutdown tasks.

    - Creates aiohttp session
    - Initializes Daily API helper
    - Cleans up resources on shutdown
    """
    from pipecat.transports.daily.utils import DailyRESTHelper

    aiohttp_session = aiohttp.ClientSession()
    daily_helpers["rest"] = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )
    yield
    await aiohttp_session.close()
    cleanup()


class ConnectData(BaseModel):
    """Data provided by client to specify the bot pipeline.

    Attributes:
        bot_name (BotName): The name of the bot to connect to. Defaults to "openai".
    """

    bot_name: BotName = "openai"


async def start(data: ConnectData):
    """Internal method to start a bot agent and return the room URL and token.

    Args:
        data (ConnectData): The data containing the bot name to use.

    Returns:
        tuple[str, str]: A tuple containing the room URL and token.
    """
    room_url, token = await create_room_and_token()
    launch_bot_func = modal.Function.from_name("pipecat-modal", "bot_runner")
    function_id = launch_bot_func.spawn(room_url, token, data.bot_name)
    bot_jobs[function_id] = (function_id, room_url)

    return room_url, token


@router.get("/")
async def start_agent():
    """A user endpoint for launching a bot agent and redirecting to the created room URL.

    This function retrieves the bot implementation from the environment,
    starts the bot agent, and redirects the user to the room URL to
    interact with the bot through a Daily Prebuilt Interface.

    Returns:
        RedirectResponse: A response that redirects to the room URL.
    """
    bot_name = os.getenv("BOT_IMPLEMENTATION", "openai").lower().strip()
    print(f"Starting bot: {bot_name}")
    room_url, token = await start(ConnectData(bot_name=bot_name))

    return RedirectResponse(room_url)


@router.post("/connect")
async def rtvi_connect(data: ConnectData) -> Dict[Any, Any]:
    """A user endpoint for launching a bot agent and retrieving the room/token credentials.

    This function retrieves the bot implementation from the request, if provided,
    starts the bot agent, and returns the room URL and token for the bot. This allows the
    client to then connect to the bot using their own RTVI interface.

    Args:
        data (ConnectData): Optional. The data containing the bot name to use.

    Returns:
        Dict[Any, Any]: A dictionary containing the room URL and token.
    """
    print(f"Starting bot: {data.bot_name}")
    if data is None or not data.bot_name:
        data.bot_name = os.getenv("BOT_IMPLEMENTATION", "openai").lower().strip()
    room_url, token = await start(data)

    return {"room_url": room_url, "token": token}


@router.get("/status/{fid}")
def get_status(fid: str):
    """Retrieve the status of a bot process by its function ID.

    Args:
        fid (str): The function ID of the bot process.

    Returns:
        JSONResponse: A JSON response containing the bot's status and result code.

    Raises:
        HTTPException: If the bot process with the given ID is not found.
    """
    func = modal.FunctionCall.from_id(fid)
    if not func:
        raise HTTPException(status_code=404, detail=f"Bot with process id: {fid} not found")

    try:
        result = func.get(timeout=0)
        return JSONResponse({"bot_id": fid, "status": "finished", "code": result})
    except modal.exception.OutputExpiredError:
        return JSONResponse({"bot_id": fid, "status": "finished", "code": 404})
    except TimeoutError:
        return JSONResponse({"bot_id": fid, "status": "running", "code": 202})


@app.function(image=web_image, min_containers=1)
@modal.concurrent(max_inputs=1)
@modal.asgi_app()
def fastapi_app():
    """Create and configure the FastAPI application.

    This function initializes the FastAPI app with middleware, routes, and lifespan management.
    It is decorated to be used as a Modal ASGI app.
    """
    from fastapi.middleware.cors import CORSMiddleware

    # Initialize FastAPI app
    web_app = FastAPI(lifespan=lifespan)

    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Include the endpoints from this file
    web_app.include_router(router)

    return web_app



================================================
FILE: deployment/modal-example/server/env.example
================================================
DAILY_API_KEY=

# determines which bot file to default to: 'openai', 'gemini', or 'vllm'
BOT_IMPLEMENTATION=openai

# needed for the openai bot pipeline
OPENAI_API_KEY=
ELEVENLABS_API_KEY=

# needed for the gemini live bot pipeline
GOOGLE_API_KEY=

# needed if you modified the API Key for your self-hosted LLM
VLLM_API_KEY=


================================================
FILE: deployment/modal-example/server/requirements.txt
================================================
python-dotenv==1.0.1
modal==1.0.5
fastapi[all]



================================================
FILE: deployment/modal-example/server/src/__init__.py
================================================
[Empty file]


================================================
FILE: deployment/modal-example/server/src/bot_gemini.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Gemini Bot Implementation.

This module implements a chatbot using Google's Gemini Live model.
It includes:
- Real-time audio/video interaction through Daily
- Animated robot avatar
- Speech-to-speech model

The bot runs as part of a pipeline that processes audio/video frames and manages
the conversation flow using Gemini's streaming capabilities.
"""

import os
import sys

from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    BotStartedSpeakingFrame,
    BotStoppedSpeakingFrame,
    Frame,
    LLMRunFrame,
    OutputImageRawFrame,
    SpriteFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

try:
    logger.remove(0)
    logger.add(sys.stderr, level="DEBUG")
except ValueError:
    # Handle the case where logger is already initialized
    pass

sprites = []
script_dir = os.path.dirname(__file__)

for i in range(1, 26):
    # Build the full path to the image file
    full_path = os.path.join(script_dir, f"assets/robot0{i}.png")
    # Get the filename without the extension to use as the dictionary key
    # Open the image and convert it to bytes
    with Image.open(full_path) as img:
        sprites.append(OutputImageRawFrame(image=img.tobytes(), size=img.size, format=img.format))

# Create a smooth animation by adding reversed frames
flipped = sprites[::-1]
sprites.extend(flipped)

# Define static and animated states
quiet_frame = sprites[0]  # Static frame for when bot is listening
talking_frame = SpriteFrame(images=sprites)  # Animation sequence for when bot is talking


class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states.

    Switches between static (listening) and animated (talking) states based on
    the bot's current speaking status.
    """

    def __init__(self):
        super().__init__()
        self._is_talking = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and update animation state.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Switch to talking animation when bot starts speaking
        if isinstance(frame, BotStartedSpeakingFrame):
            if not self._is_talking:
                await self.push_frame(talking_frame)
                self._is_talking = True
        # Return to static frame when bot stops speaking
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
            self._is_talking = False

        await self.push_frame(frame, direction)


async def run_bot(room_url: str, token: str):
    """Main bot execution function.

    Sets up and runs the bot pipeline including:
    - Daily video transport with specific audio parameters
    - Gemini Live model integration
    - Voice activity detection
    - Animation processing
    - RTVI event handling
    """
    # Set up Daily transport with specific audio/video parameters for Gemini
    transport = DailyTransport(
        room_url,
        token,
        "Chatbot",
        DailyParams(
            audio_out_enabled=True,
            camera_out_enabled=True,
            camera_out_width=1024,
            camera_out_height=576,
            vad_enabled=True,
            vad_audio_passthrough=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.5)),
        ),
    )

    # Initialize the Gemini Live model
    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_user_audio=True,
    )

    messages = [
        {
            "role": "user",
            "content": "You are Chatbot, a friendly, helpful robot. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by introducing yourself.",
        },
    ]

    # Set up conversation context and management
    # The context_aggregator will automatically collect conversation context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    ta = TalkingAnimation()

    #
    # RTVI events for Pipecat client UI
    #
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            context_aggregator.user(),
            llm,
            ta,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )
    await task.queue_frame(quiet_frame)

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        await transport.capture_participant_transcription(participant["id"])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        print(f"Participant left: {participant}")
        await task.cancel()

    runner = PipelineRunner()

    await runner.run(task)



================================================
FILE: deployment/modal-example/server/src/bot_openai.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""OpenAI Bot Implementation.

This module implements a chatbot using OpenAI's GPT-4 model for natural language
processing. It includes:
- Real-time audio/video interaction through Daily
- Animated robot avatar
- Text-to-speech using ElevenLabs
- Support for both English and Spanish

The bot runs as part of a pipeline that processes audio/video frames and manages
the conversation flow.
"""

import os
import sys

from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import (
    BotStartedSpeakingFrame,
    BotStoppedSpeakingFrame,
    Frame,
    LLMRunFrame,
    OutputImageRawFrame,
    SpriteFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

try:
    logger.remove(0)
    logger.add(sys.stderr, level="DEBUG")
except ValueError:
    # Handle the case where logger is already initialized
    pass

sprites = []
script_dir = os.path.dirname(__file__)

# Load sequential animation frames
for i in range(1, 26):
    # Build the full path to the image file
    full_path = os.path.join(script_dir, f"assets/robot0{i}.png")
    # Get the filename without the extension to use as the dictionary key
    # Open the image and convert it to bytes
    with Image.open(full_path) as img:
        sprites.append(OutputImageRawFrame(image=img.tobytes(), size=img.size, format=img.format))

# Create a smooth animation by adding reversed frames
flipped = sprites[::-1]
sprites.extend(flipped)

# Define static and animated states
quiet_frame = sprites[0]  # Static frame for when bot is listening
talking_frame = SpriteFrame(images=sprites)  # Animation sequence for when bot is talking


class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states.

    Switches between static (listening) and animated (talking) states based on
    the bot's current speaking status.
    """

    def __init__(self):
        super().__init__()
        self._is_talking = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and update animation state.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Switch to talking animation when bot starts speaking
        if isinstance(frame, BotStartedSpeakingFrame):
            if not self._is_talking:
                await self.push_frame(talking_frame)
                self._is_talking = True
        # Return to static frame when bot stops speaking
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
            self._is_talking = False

        await self.push_frame(frame, direction)


async def run_bot(room_url: str, token: str):
    """Main bot execution function.

    Sets up and runs the bot pipeline including:
    - Daily video transport
    - Speech-to-text and text-to-speech services
    - Language model integration
    - Animation processing
    - RTVI event handling
    """
    # Set up Daily transport with video/audio parameters
    transport = DailyTransport(
        room_url,
        token,
        "Chatbot",
        DailyParams(
            audio_out_enabled=True,
            camera_out_enabled=True,
            camera_out_width=1024,
            camera_out_height=576,
            vad_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            transcription_enabled=True,
            #
            # Spanish
            #
            # transcription_settings=DailyTranscriptionSettings(
            #     language="es",
            #     tier="nova",
            #     model="2-general"
            # )
        ),
    )

    # Initialize text-to-speech service
    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY"),
        #
        # English
        #
        voice_id="SAz9YHcvj6GT2YYXdXww",
        #
        # Spanish
        #
        # model="eleven_multilingual_v2",
        # voice_id="gD1IexrzCvsXPHUuT0s3",
    )

    # Initialize LLM service
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            #
            # English
            #
            "content": "You are an incessant one-upper. Start by asking the user how their day is going.",
            #
            # Spanish
            #
            # "content": "Eres Chatbot, un amigable y útil robot. Tu objetivo es demostrar tus capacidades de una manera breve. Tus respuestas se convertiran a audio así que nunca no debes incluir caracteres especiales. Contesta a lo que el usuario pregunte de una manera creativa, útil y breve. Empieza por presentarte a ti mismo.",
        },
    ]

    # Set up conversation context and management
    # The context_aggregator will automatically collect conversation context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    ta = TalkingAnimation()

    #
    # RTVI events for Pipecat client UI
    #
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            context_aggregator.user(),
            llm,
            tts,
            ta,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )
    await task.queue_frame(quiet_frame)

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        await transport.capture_participant_transcription(participant["id"])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        print(f"Participant left: {participant}")
        await task.cancel()

    runner = PipelineRunner()

    await runner.run(task)



================================================
FILE: deployment/modal-example/server/src/bot_vllm.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""OpenAI Bot Implementation.

This module implements a chatbot using OpenAI's GPT-4 model for natural language
processing. It includes:
- Real-time audio/video interaction through Daily
- Animated robot avatar
- Text-to-speech using ElevenLabs
- Support for both English and Spanish

The bot runs as part of a pipeline that processes audio/video frames and manages
the conversation flow.
"""

import os
import sys

from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import (
    BotStartedSpeakingFrame,
    BotStoppedSpeakingFrame,
    Frame,
    LLMRunFrame,
    OutputImageRawFrame,
    SpriteFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

try:
    logger.remove(0)
    logger.add(sys.stderr, level="DEBUG")
except ValueError:
    # Handle the case where logger is already initialized
    pass

# REPLACE WITH YOUR MODAL URL ENDPOINT
modal_url = "https://<Modal workspace>--example-vllm-openai-compatible-serve.modal.run"
api_key = os.getenv("VLLM_API_KEY", "super-secret-key")


sprites = []
script_dir = os.path.dirname(__file__)

# Load sequential animation frames
for i in range(1, 26):
    # Build the full path to the image file
    full_path = os.path.join(script_dir, f"assets/robot0{i}.png")
    # Get the filename without the extension to use as the dictionary key
    # Open the image and convert it to bytes
    with Image.open(full_path) as img:
        sprites.append(OutputImageRawFrame(image=img.tobytes(), size=img.size, format=img.format))

# Create a smooth animation by adding reversed frames
flipped = sprites[::-1]
sprites.extend(flipped)

# Define static and animated states
quiet_frame = sprites[0]  # Static frame for when bot is listening
talking_frame = SpriteFrame(images=sprites)  # Animation sequence for when bot is talking


class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states.

    Switches between static (listening) and animated (talking) states based on
    the bot's current speaking status.
    """

    def __init__(self):
        super().__init__()
        self._is_talking = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and update animation state.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Switch to talking animation when bot starts speaking
        if isinstance(frame, BotStartedSpeakingFrame):
            if not self._is_talking:
                await self.push_frame(talking_frame)
                self._is_talking = True
        # Return to static frame when bot stops speaking
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
            self._is_talking = False

        await self.push_frame(frame, direction)


async def run_bot(room_url: str, token: str):
    """Main bot execution function.

    Sets up and runs the bot pipeline including:
    - Daily video transport
    - Speech-to-text and text-to-speech services
    - Language model integration
    - Animation processing
    - RTVI event handling
    """
    # Set up Daily transport with video/audio parameters
    transport = DailyTransport(
        room_url,
        token,
        "Chatbot",
        DailyParams(
            audio_out_enabled=True,
            camera_out_enabled=True,
            camera_out_width=1024,
            camera_out_height=576,
            vad_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            transcription_enabled=True,
            #
            # Spanish
            #
            # transcription_settings=DailyTranscriptionSettings(
            #     language="es",
            #     tier="nova",
            #     model="2-general"
            # )
        ),
    )

    # Initialize text-to-speech service
    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY"),
        #
        # English
        #
        voice_id="D38z5RcWu1voky8WS1ja",
        #
        # Spanish
        #
        # model="eleven_multilingual_v2",
        # voice_id="gD1IexrzCvsXPHUuT0s3",
    )

    # Initialize LLM service
    llm = OpenAILLMService(
        # To use OpenAI
        api_key=api_key,
        # Or, to use a local vLLM (or similar) api server
        model="neuralmagic/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",
        base_url=f"{modal_url}/v1",
    )

    messages = [
        {
            "role": "system",
            #
            # English
            #
            "content": "You are a salesman for Modal, the cloud-native serverless Python computing platform.",
            #
            # Spanish
            #
            # "content": "Eres Chatbot, un amigable y útil robot. Tu objetivo es demostrar tus capacidades de una manera breve. Tus respuestas se convertiran a audio así que nunca no debes incluir caracteres especiales. Contesta a lo que el usuario pregunte de una manera creativa, útil y breve. Empieza por presentarte a ti mismo.",
        },
    ]

    # Set up conversation context and management
    # The context_aggregator will automatically collect conversation context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    ta = TalkingAnimation()

    #
    # RTVI events for Pipecat client UI
    #
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            context_aggregator.user(),
            llm,
            tts,
            ta,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )
    await task.queue_frame(quiet_frame)

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        await transport.capture_participant_transcription(participant["id"])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        print(f"Participant left: {participant}")
        await task.cancel()

    runner = PipelineRunner()

    await runner.run(task)



================================================
FILE: deployment/modal-example/server/src/runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import asyncio
import importlib
import os


def get_bot_file(arg_bot: str | None) -> str:
    bot_implementation = arg_bot or os.getenv("BOT_IMPLEMENTATION", "openai").lower().strip()
    if not bot_implementation:
        bot_implementation = "openai"
    if bot_implementation not in ["openai", "gemini", "vllm"]:
        raise ValueError(
            f"Invalid BOT_IMPLEMENTATION: {bot_implementation}. Must be 'openai' or 'gemini'"
        )
    return f"bot_{bot_implementation}"


def get_runner(bot_file: str):
    """Dynamically import the run_bot function based on the bot name.

    Args:
        bot_name (str): The name of the bot implementation (e.g., 'openai', 'gemini').

    Returns:
        function: The run_bot function from the specified bot module.

    Raises:
        ImportError: If the specified bot module or run_bot function is not found.
    """
    try:
        # Dynamically construct the module name
        module_name = f"{bot_file}"
        # Import the module
        module = importlib.import_module(module_name)
        # Get the run_bot function from the module
        return getattr(module, "run_bot")
    except (ImportError, AttributeError) as e:
        raise ImportError(f"Failed to import run_bot from {module_name}: {e}")


def main():
    """Parse the args to launch the appropriate bot using the given room/token."""
    parser = argparse.ArgumentParser(description="Daily AI SDK Bot Sample")
    parser.add_argument(
        "-u", "--url", type=str, required=False, help="URL of the Daily room to join"
    )
    parser.add_argument(
        "-t",
        "--token",
        type=str,
        required=False,
        help="Daily room token",
    )
    parser.add_argument(
        "-b",
        "--bot",
        type=str,
        required=False,
        help="Bot runner to use (e.g., openai, gemini)",
    )

    args, unknown = parser.parse_known_args()

    url = args.url or os.getenv("DAILY_SAMPLE_ROOM_URL")
    token = args.token or os.getenv("DAILY_SAMPLE_ROOM_TOKEN")
    bot_file = get_bot_file(args.bot)

    if not url:
        raise Exception(
            "No Daily room specified. use the -u/--url option from the command line, or set DAILY_SAMPLE_ROOM_URL in your environment to specify a Daily room URL."
        )

    run_bot = get_runner(bot_file)
    asyncio.run(run_bot(url, token))


if __name__ == "__main__":
    main()



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/README.md
================================================
# Handling PSTN/SIP Dial-in on Pipecat Cloud

This repository contains two server implementations for handling
the pinless dial-in workflow in Pipecat Cloud. This is the companion to the
Pipecat Cloud [pstn_sip starter image](https://github.com/daily-co/pipecat-cloud-images/tree/main/pipecat-starters/pstn_sip).
In addition you can use `/api/dial` to trigger dial-out, and
eventually, call-transfers.

1. [FastAPI Server](fastapi-webhook-server/README.md) -
   A FastAPI implementation that handles PSTN (Public Switched Telephone
   Network) and SIP (Session Initiation Protocol) calls using the Daily API.

2. [Next.js Serverless](nextjs-webhook-server/README.md) -
   A Next.js API implementation designed for deployment on Vercel's
   serverless platform.

Both implementations provide:

- HMAC signature validation for pinless webhook
- Structured logging
- Support for dial-in and dial-out settings
- Voicemail detection and call transfer functionality (coming soon)
- Test request handling

## Choosing an Implementation

- Use the **FastAPI Server** if you:

  - Need a standalone server
  - Prefer Python and FastAPI
  - Want to deploy to traditional hosting platforms

- Use the **Next.js Serverless** implementation if you:
  - Want serverless deployment
  - Prefer JavaScript/TypeScript
  - Already use Next.js and Vercel for other projects
  - Need quick scaling and zero maintenance

## Prerequisites

### Environment Variables

Both implementations require similar environment variables:

- `PIPECAT_CLOUD_API_KEY`: Pipecat Cloud API Key, begins with pk\_\*
- `AGENT_NAME`: Your Daily agent name
- `PINLESS_HMAC_SECRET`: Your HMAC secret for request verification
- `LOG_LEVEL`: (Optional) Logging level (defaults to 'info')

See the individual README files in each implementation directory for
specific setup instructions.

### Phone number setup

You can buy a phone number through the Pipecat Cloud Dashboard:

1. Go to `Settings` > `Telephony`
2. Follow the UI to purchase a phone number
3. Configure the webhook URL to receive incoming calls (e.g. `https://my-webhook-url.com/api/dial`)

Or purchase the number using Daily's
[PhoneNumbers API](https://docs.daily.co/reference/rest-api/phone-numbers).

```bash
curl --request POST \
--url https://api.daily.co/v1/domain-dialin-config \
--header 'Authorization: Bearer $TOKEN' \
--header 'Content-Type: application/json' \
--data-raw '{
	"type": "pinless_dialin",
	"name_prefix": "Customer1",
    "phone_number": "+1PURCHASED_NUM",
	"room_creation_api": "https://example.com/api/dial",
    "hold_music_url": "https://example.com/static/ringtone.mp3",
	"timeout_config": {
		"message": "No agent is available right now"
	}
}'
```

The API will return a static SIP URI (`sip_uri`) that can be called
from other SIP services.

### `room_creation_api`

To make and receive calls currently you have to host a server that
handles incoming calls. In the coming weeks, incoming calls will be
directly handled within Daily and we will expose an endpoint similar
to `{service}/start` that will manage this for you.

In the meantime, the server described below serves as the webhook
handler for the `room_creation_api`. Configure your pinless phone
number or SIP interconnect to the `ngrok` tunnel or
the actual server URL, append `/api/dial` to the webhook URL.

## Example curl commands

Note: Replace `http://localhost:3000` with your actual server URL and
phone numbers with valid values for your use case.

### Dialin Request

The server will receive a request when a call is received from Daily. 
The payload that the webhook received is as follows:
```json
{
  // for dial-in from webhook
  "To": "+14152251493",
  "From": "+14158483432",
  "callId": "string-contains-uuid",
  "callDomain": "string-contains-uuid",
  "sipHeaders": {
    "X-My-Custom-Header": "value",
    "x-caller": "+1234567890",
    "x-called": "+1987654321", 
   },
}
```
The `To`, `From`, `callId`, `callDomain` fields are converted to 
`snake_case` and mapped to `dialin_settings`. In addition, `sipHeader` 
contains any custom SIP headers received by Daily on the SIP 
interconnect address (`sip_uri`). These are headers sent from 
Twilio or other external SIP platforms, for example, to send the 
caller's phone number.

### Dialout Request

Dial a number, will use any purchased number

```bash
curl -X POST http://localhost:3000/api/dial \
  -H "Content-Type: application/json" \
  -d '{
    "dialout_settings": [
      {
        "phoneNumber": "+1234567890",
      }
    ]
  }'
```

Dial a number with callerId, which is the UUID of a purchased number.

```bash
curl -X POST http://localhost:3000/api/dial \
  -H "Content-Type: application/json" \
  -d '{
    "dialout_settings": [
      {
        "phoneNumber": "+1234567890",
        "callerId": "purchased_phone_uuid"
      }
    ]
  }'
```

Dial a number

```bash
curl -X POST http://localhost:3000/api/dial \
  -H "Content-Type: application/json" \
  -d '{
    "dialout_settings": [
      {
        "phoneNumber": "+1234567890",
        "callerId": "purchased_phone_uuid"
      }
    ]
  }'
```

### Advanced Request with Voicemail Detection

```bash
curl -X POST http://localhost:3000/api/dial \
  -H "Content-Type: application/json" \
  -d '{
    "To": "+1234567890",
    "From": "+1987654321",
    "callId": "call-uuid-123",
    "callDomain": "domain-uuid-456",
    "sipHeader": {},
    "dialout_settings": [
      {
        "phoneNumber": "+1234567890",
        "callerId": "purchased_phone_uuid"
      }
    ],
    "voicemail_detection": {
      "testInPrebuilt": true
    },
    "call_transfer": {
      "mode": "dialout",
      "speakSummary": true,
      "storeSummary": true,
      "operatorNumber": "+1234567890",
      "testInPrebuilt": true
    }
  }'
```



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/fastapi-webhook-server/README.md
================================================
# FastAPI server for handling Daily PSTN/SIP Webhook

A FastAPI server that handles PSTN (Public Switched Telephone Network) and SIP (Session Initiation Protocol) calls using the Daily API.

## Setup

1. Clone the repository

2. Navigate to the `fastapi-webhook-server` directory:

   ```bash
   cd fastapi-webhook-server
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Copy `env.example` to `.env`:

   ```bash
   cp env.example .env
   ```

5. Update `.env` with your credentials:

   - `AGENT_NAME`: Your Daily agent name
   - `PIPECAT_CLOUD_API_KEY`: Your Daily API key
   - `PINLESS_HMAC_SECRET`: Your HMAC secret for request verification

## Running the Server

Start the server:

```bash
python server.py
```

The server will run on `http://localhost:7860` and you can expose it via ngrok for testing:

```bash
`ngrok http 7860`
```

> Tip: Use a subdomain for a consistent URL (e.g. `ngrok http -subdomain=mydomain http://localhost:7860`)

## API Endpoints

### GET /

Health check endpoint that returns a "Hello, World!" message.

### POST /api/dial

Initiates a PSTN/SIP call with the following request body format:

```json
{
  "To": "+14152251493",
  "From": "+14158483432",
  "callId": "string-contains-uuid",
  "callDomain": "string-contains-uuid",
  "dialout_settings": [
    {
      "phoneNumber": "+14158483432",
      "callerId": "+14152251493"
    }
  ],
  "voicemail_detection": {
    "testInPrebuilt": true
  },
  "call_transfer": {
    "mode": "dialout",
    "speakSummary": true,
    "storeSummary": true,
    "operatorNumber": "+14152250006",
    "testInPrebuilt": true
  }
}
```

#### Response

Returns a JSON object containing:

- `status`: Success/failure status
- `data`: Response from Daily API
- `room_properties`: Properties of the created Daily room

## Error Handling

- 401: Invalid signature
- 400: Invalid authorization header (e.g. missing Daily API key in bot.py)
- 405: Method not allowed (e.g. incorrect route on the webhook URL)
- 500: Server errors (missing API key, network issues)
- Other status codes are passed through from the Daily API



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/fastapi-webhook-server/env.example
================================================
AGENT_NAME="your-agent-name"
PIPECAT_CLOUD_API_KEY="your-daily-api-key"
PINLESS_HMAC_SECRET="hmac-secret-pinless-dialin"


================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/fastapi-webhook-server/requirements.txt
================================================
fastapi
uvicorn
python-dotenv
requests
pydantic
loguru


================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/fastapi-webhook-server/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

# server.py


import base64  # for calculating hmac signature
import hmac
import os  # for accessing environment variables
import time  # for setting expiration time
from typing import Any, Dict, List, Optional

import requests
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from loguru import logger
from pydantic import BaseModel, Field

load_dotenv(override=True)

app = FastAPI()


class RoomRequest(BaseModel):
    test: Optional[str] = Field(None, alias="Test", description="Test field")
    To: Optional[str] = Field(None, alias="to", description="Destination phone number")
    From: Optional[str] = Field(None, alias="from", description="Source phone number")
    callId: Optional[str] = Field(None, alias="call_id", description="Unique call identifier")
    callDomain: Optional[str] = Field(
        None, alias="call_domain", description="Call domain identifier"
    )
    dialout_settings: Optional[List[Dict[str, Any]]] = Field(
        None, description="An array of phone numbers or SIP URIs to dialout to"
    )
    voicemail_detection: Optional[Dict[str, Any]] = Field(
        None, description="A flag to perform voicemail or answeing-machine detection"
    )
    call_transfer: Optional[Dict[str, Any]] = Field(None, description="to initiate a call transfer")
    sipHeaders: Optional[Dict[str, Any]] = Field(
        None,
        alias="sip_headers",
        description="Custom SIP headers received from the external SIP provider",
    )

    class Config:
        populate_by_name = True
        alias_generator = None


"""
    body can contain any fields, but for handling PSTN/SIP, 
    we recommend sending the following custom values:
    dialin, dialout, voicemail detection, and call transfer
    
    
    "To": "+14152251493",
    "From": "+14158483432",
    "callId": "string-contains-uuid",
    "callDomain": "string-contains-uuid"
    These need to be remapped to dialin_settings

    In addition, we may receive in the body that can be 
    sent to the bot as a custom field, sip_headers
    "sipHeaders": {
        "X-My-Custom-Header": "value",
        "x-caller": "+14158483432",
        "x-called": "+14152251493",
    },

    "dialout_settings": [
        {"phoneNumber": "+14158483432", "callerId": "+14152251493"}, 
        {"sipUri": "sip:username@sip.hostname"}
        ],
    },

    voicemail_detection:{
        testInPrebuilt: true
    },

    "call_transfer": {
        "mode": "dialout",
        "speakSummary": true,
        "storeSummary": true,
        "operatorNumber": "+14152250006",
        "testInPrebuilt": true
    }
"""


@app.get("/")
async def read_root():
    return {"message": "Hello, World!"}


@app.post("/api/dial")
async def dial(request: RoomRequest, raw_request: Request):
    logger.info("Incoming request to /dial:")
    logger.info(f"Headers: {dict(raw_request.headers)}")
    raw_body = await raw_request.body()
    raw_body_str = raw_body.decode()
    logger.info(f"Raw body: {raw_body_str}")
    logger.info(f"Parsed body: {request.dict()}")

    # calculate signature and compare/verify
    hmac_secret = os.getenv("PINLESS_HMAC_SECRET")
    timestamp = raw_request.headers.get("x-pinless-timestamp")
    signature = raw_request.headers.get("x-pinless-signature")

    if not hmac_secret:
        logger.debug("Skipping HMAC validation - PINLESS_HMAC_SECRET not set")
    elif timestamp and signature:
        message = timestamp + "." + raw_body_str

        base64_decoded_secret = base64.b64decode(hmac_secret)
        computed_signature = base64.b64encode(
            hmac.new(base64_decoded_secret, message.encode(), "sha256").digest()
        ).decode()

        if computed_signature != signature:
            logger.error(f"Invalid signature. Expected {signature}, got {computed_signature}")
            raise HTTPException(status_code=401, detail="Invalid signature")
    else:
        logger.debug("Skipping HMAC validation - no signature headers present")

    if request.test == "test":
        logger.debug("Test request received")
        return {"status": "success", "message": "Test request received"}

    dialin_settings = None
    # these fields are camelCase in the request
    required_fields = ["To", "From", "callId", "callDomain"]
    if all(
        field in request.dict() and request.dict()[field] is not None for field in required_fields
    ):
        # transform from camelCase to snake_case because daily-python expects snake_case
        dialin_settings = {
            "From": request.From,
            "To": request.To,
            "call_id": request.callId,
            "call_domain": request.callDomain,
            # transform from camelCase to snake_case
        }
        logger.debug(f"Populated dialin_settings from request: {dialin_settings}")

    daily_room_properties = {
        "enable_dialout": request.dialout_settings is not None,
    }

    if dialin_settings is not None:
        sip_config = {
            "display_name": request.From,
            "sip_mode": "dial-in",
            "num_endpoints": 2 if request.call_transfer is not None else 1,
            "codecs": {"audio": ["OPUS"]},
        }
        daily_room_properties["sip"] = sip_config

    # Setting default expiry to 5 minutes from now
    daily_room_properties["exp"] = int(time.time()) + (5 * 60)

    logger.debug(f"Daily room properties: {daily_room_properties}")
    payload = {
        "createDailyRoom": True,
        "dailyRoomProperties": daily_room_properties,
        "body": {
            "dialin_settings": dialin_settings,
            "dialout_settings": request.dialout_settings,
            "voicemail_detection": request.voicemail_detection,
            "call_transfer": request.call_transfer,
            "sip_headers": request.sipHeaders,  # passing the SIP headers to the bot
        },
    }

    pcc_api_key = os.getenv("PIPECAT_CLOUD_API_KEY")
    agent_name = os.getenv("AGENT_NAME", "my-first-agent")

    if not pcc_api_key:
        raise HTTPException(status_code=500, detail="DAILY_API_KEY environment variable is not set")

    headers = {"Authorization": f"Bearer {pcc_api_key}", "Content-Type": "application/json"}

    url = f"https://api.pipecat.daily.co/v1/public/{agent_name}/start"

    logger.debug(f"Making API call to Daily: {url} {headers} {payload}")

    try:
        response = requests.post(url, json=payload, headers=headers)
        response.raise_for_status()
        response_data = response.json()
        logger.debug(f"Response: {response_data}")
        return {
            "status": "success",
            "data": response_data,
            "room_properties": daily_room_properties,
        }
    except requests.exceptions.HTTPError as e:
        # Pass through the status code and error details from the Daily API
        status_code = e.response.status_code
        error_detail = e.response.json() if e.response.content else str(e)
        logger.error(f"HTTP error: {error_detail}")
        raise HTTPException(status_code=status_code, detail=error_detail)
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    try:
        import uvicorn

        uvicorn.run(app, host="0.0.0.0", port=7860)
    except KeyboardInterrupt:
        logger.info("Server stopped manually")



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/nextjs-webhook-server/README.md
================================================
# Next.js server for handling Daily PSTN/SIP Webhook

Next.js API routes for handling Daily PSTN/SIP Pipecat requests.

## Features

- API endpoint for handling Daily PSTN/SIP Pipecat requests
- HMAC signature validation
- Structured logging with Pino
- Support for dial-in and dial-out settings
- Voicemail detection and call transfer functionality
- Test request handling

## Setup

1. Clone the repository

2. Navigate to the `nextjs-webhook-server` directory:

   ```bash
   cd nextjs-webhook-server
   ```

3. Install dependencies:

   ```bash
   npm install
   ```

4. Create `.env.local` file with your credentials:

   ```bash
   cp env.local.example .env.local
   ```

5. Update your `.env` with your secrets:

   ```bash
   PIPECAT_CLOUD_API_KEY=pk_*
   AGENT_NAME=my-first-agent
   PINLESS_HMAC_SECRET=your_hmac_secret
   LOG_LEVEL=info
   ```

### Running the server

Run the development server:

```bash
npm run dev
```

The server will run on `http://localhost:7860` and you can expose it via ngrok for testing:

```bash
`ngrok http 7860`
```

> Tip: Use a subdomain for a consistent URL (e.g. `ngrok http -subdomain=mydomain http://localhost:7860`)

## API Endpoints

### GET /api

Returns a simple "Hello, World!" message with a cute cat emoji to verify the server is running.

### POST /api/dial

Handles dial-in and dial-out requests for Pipecat Cloud.

#### Test Requests

The endpoint handles test requests when a webhook is configured. Send a request with `"Test": "test"` to verify your setup:

```json
{
  "Test": "test"
}
```

#### Production Request Format

```json
{
  // for dial-in from webhook
  "To": "+14152251493",
  "From": "+14158483432",
  "callId": "string-contains-uuid",
  "callDomain": "string-contains-uuid",
  // for making a dial out to a phone or SIP
  "dialout_settings": [
    { "phoneNumber": "+14158483432", "callerId": "purchased_phone_uuid" },
    { "sipUri": "sip:username@sip.hostname.com" }
  ]
}
```

## Deployment

The application is configured for Vercel deployment:

1. Push your code to a Git repository
2. Import your project in Vercel dashboard
3. Configure environment variables:
   - `PIPECAT_CLOUD_API_KEY`
   - `AGENT_NAME`
   - `PINLESS_HMAC_SECRET`
   - `LOG_LEVEL` (optional, defaults to 'info')
4. Deploy!

## Security

- HMAC signature validation for request authentication
- Environment variables for sensitive credentials
- Method validation (POST only for /dial)



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/nextjs-webhook-server/env.local.example
================================================
AGENT_NAME=my-first-agent
PIPECAT_CLOUD_API_KEY=your_daily_api_key
PINLESS_HMAC_SECRET=your_hmac_secret
LOG_LEVEL="info"


================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/nextjs-webhook-server/package.json
================================================
{
  "name": "my-daily-app",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev -p 7860",
    "build": "next build",
    "start": "next start -p 7860",
    "lint": "next lint"
  },
  "dependencies": {
    "axios": "^1.11.0",
    "next": "^14.0.0",
    "pino": "^8.15.0",
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "eslint": "^8.46.0",
    "eslint-config-next": "^14.0.0"
  }
}



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/nextjs-webhook-server/vercel.js
================================================
module.exports = {
  version: 2,
  buildCommand: "next build",
  outputDirectory: ".next",
  cleanUrls: true
};


================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/nextjs-webhook-server/pages/api/dial.js
================================================
import { logger } from '../../lib/utils'; 
import axios from 'axios';
import crypto from 'crypto';

const validateSignature = (body, signature, timestamp, secret) => {
  // Skip if any required fields are missing
  if (!signature || !timestamp || !secret) {
    logger.warn('Missing required fields for HMAC validation');
    return true;
  }

  try {
    const decodedSecret = Buffer.from(secret, 'base64');
    const hmac = crypto.createHmac('sha256', decodedSecret);
    const signatureData = `${timestamp}.${body}`;
    const computedSignature = hmac.update(signatureData).digest('base64');
    
    logger.debug('Signature validation:', {
      timestamp,
      signatureData: signatureData.substring(0, 50) + '...',
      computedSignature,
      receivedSignature: signature
    });
    
    return computedSignature === signature;
  } catch (error) {
    logger.error('Error validating signature:', error);
    return true; // Allow request to proceed on error
  }
};

export default async function handler(req, res) {
  // Only allow POST requests
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    logger.info('Incoming request to /api/dial:');
    logger.info(`Headers: ${JSON.stringify(req.headers)}`);
    
    const rawBody = JSON.stringify(req.body);
    logger.info(`Raw body: ${rawBody}`);

    const signature = req.headers['x-pinless-signature'];
    const timestamp = req.headers['x-pinless-timestamp'];
    
    if (signature && timestamp) {
      logger.info('Validating HMAC signature');
      if (!validateSignature(rawBody, signature, timestamp, process.env.PINLESS_HMAC_SECRET)) {
        logger.error('Invalid HMAC signature', { signature, timestamp });
        return res.status(401).json({ 
          error: 'Invalid signature',
          message: 'Invalid HMAC signature'
        });
      }
    } else {
      logger.info('Skipping HMAC validation - no signature headers present');
    }

    // Extract request data
    const {
      Test: test,
      To,
      From,
      callId,
      callDomain,
      sipHeaders,
      dialout_settings,
      voicemail_detection,
      call_transfer
    } = req.body;

    // Handle test requests when a webhook is configured
    if (test === 'test') {
      logger.debug('Test request received');
      return res.status(200).json({ status: 'success', message: 'Test request received' });
    }

    // Process dialin settings
    let dialin_settings = null;
    const requiredFields = ['To', 'From', 'callId', 'callDomain'];
    
    if (requiredFields.every(field => req.body[field] !== undefined && req.body[field] !== null)) {
      dialin_settings = {
        // snake_case because pipecat expects this format
        From,
        To,
        call_id: callId, 
        call_domain: callDomain,
      };
      logger.debug(`Populated dialin_settings from request: ${JSON.stringify(dialin_settings)}`);
    }

    // Set up Daily room properties
    const daily_room_properties = {
      enable_dialout: dialout_settings !== undefined && dialout_settings !== null,
      exp: Math.floor(Date.now() / 1000) + (5 * 60), // 5 minutes from now
    };

    // Configure SIP if dialin settings are provided
    if (dialin_settings !== null) {
      const sip_config = {
        display_name: From,
        sip_mode: 'dial-in',
        num_endpoints: (call_transfer !== undefined && call_transfer !== null) ? 2 : 1,
        codecs: {"audio": ["OPUS"]},
      };
      daily_room_properties.sip = sip_config;
    }

    // Prepare payload for {service}/start API call
    const payload = {
      createDailyRoom: true,
      dailyRoomProperties: daily_room_properties,
      body: {
        dialin_settings,
        dialout_settings,
        voicemail_detection,
        call_transfer,
        sip_headers: sipHeaders,
      },
    };

    logger.debug(`Daily room properties: ${JSON.stringify(daily_room_properties)}`);

    // Get Daily API key and agent name from environment variables
    const pccApiKey = process.env.PIPECAT_CLOUD_API_KEY;
    const agentName = process.env.AGENT_NAME || 'my-first-agent';

    if (!pccApiKey) {
      throw new Error('PIPECAT_CLOUD_API_KEY environment variable is not set');
    }

    // Set up headers for Daily API call
    const headers = {
      'Authorization': `Bearer ${pccApiKey}`,
      'Content-Type': 'application/json',
    };

    const url = `https://api.pipecat.daily.co/v1/public/${agentName}/start`;
    logger.debug(`Making API call to Daily: ${url} ${JSON.stringify(headers)} ${JSON.stringify(payload)}`);
    
    try {
      const response = await axios.post(url, payload, { headers });
      logger.debug(`Response: ${JSON.stringify(response.data)}`);
      
      return res.status(200).json({
        status: 'success',
        data: response.data,
        room_properties: daily_room_properties,
      });
    } catch (error) {
      if (error.response) {
        // Pass through status code and error details from the Daily API
        const statusCode = error.response.status;
        const errorDetail = error.response.data || error.message;
        logger.error(`HTTP error: ${JSON.stringify(errorDetail)}`);
        return res.status(statusCode).json(errorDetail);
      } else {
        logger.error(`Request error: ${error.message}`);
        return res.status(500).json({ error: error.message });
      }
    }    
  } catch (error) {
    logger.error(`Unexpected error: ${error.message}`);
    return res.status(500).json({ error: 'Internal server error', message: error.message });
  }
}

// Configure body parser to preserve raw body text
export const config = {
  api: {
    bodyParser: {
      sizeLimit: '1mb',
    },
  },
};



================================================
FILE: deployment/pipecat-cloud-daily-pstn-server/nextjs-webhook-server/pages/api/index.js
================================================
import { logger } from '../../lib/utils';

export default function handler(req, res) {
  logger.info('Received request to /api');
  res.status(200).json({ message: 'Hello, World! from ᓚᘏᗢ' });
}



================================================
FILE: exotel-chatbot/README.md
================================================
# Exotel Voice Bot Examples

This repository contains examples of voice bots that integrate with Exotel's Voice API using Pipecat. The examples demonstrate both inbound and outbound calling scenarios using Exotel's WebSocket streaming for real-time audio processing.

## Examples

### 🔽 [Inbound Calling](./inbound/)

Demonstrates how to handle incoming phone calls where users call your Exotel number and interact with a voice bot.

### 🔼 [Outbound Calling](./outbound/)

Shows how to initiate outbound phone calls programmatically where your bot calls users.

## Architecture

Both examples use the same core architecture:

```
Phone Call ↔ Exotel ↔ WebSocket Stream ↔ Pipecat ↔ AI Services
```

**Components:**

- **Exotel**: Handles phone call routing and audio transport
- **WebSocket Stream**: Real-time bidirectional audio streaming
- **Pipecat**: Audio processing pipeline and AI service orchestration
- **AI Services**: OpenAI (LLM), Deepgram (STT), Cartesia (TTS)

## Getting Help

- **Detailed Setup**: See individual README files in `inbound/` and `outbound/` directories
- **Pipecat Documentation**: [docs.pipecat.ai](https://docs.pipecat.ai)
- **Exotel Documentation**: [https://support.exotel.com/support/solutions/articles/3000108630-working-with-the-stream-and-voicebot-applet](https://support.exotel.com/support/solutions/articles/3000108630-working-with-the-stream-and-voicebot-applet)



================================================
FILE: exotel-chatbot/inbound/README.md
================================================
# Exotel Chatbot: Inbound

This project is a Pipecat-based chatbot that integrates with Exotel to handle WebSocket connections and provide real-time communication. The project includes FastAPI endpoints for handling WebSocket voice streaming using Exotel's Voicebot Applet.

## Table of Contents

- [How It Works](#how-it-works)
- [Prerequisites](#prerequisites)
- [Setup](#setup)
- [Environment Configuration](#environment-configuration)
- [Local Development](#local-development)
- [Production Deployment](#production-deployment)
- [Accessing Call Information](#accessing-call-information)

## How It Works

When someone calls your Exotel number:

1. **Exotel routes the call**: Through your configured App Bazaar application
2. **Voicebot Applet activates**: Connects to your WebSocket endpoint
3. **WebSocket connection**: Audio streams between caller and your bot
4. **Call information**: Phone numbers and custom parameters are passed via WebSocket messages

The bot automatically receives the caller's and called phone numbers for personalized responses.

## Prerequisites

### Exotel

- An Exotel account with:
  - Voice streaming enabled (contact support if not available)
  - A purchased phone number that supports voice calls

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

   ```sh
   cd inbound
   uv sync
   ```

2. Create an .env file and add API keys:

   ```sh
   cp env.example .env
   ```

## Local Development

### Configure Exotel App Bazaar Application

1. Start ngrok:
   In a new terminal, start ngrok to tunnel the local server:

   ```sh
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

2. Purchase a number (if you haven't already):

   - Log in to the Exotel dashboard: https://my.exotel.com/
   - Navigate to ExoPhones and purchase a number
   - Note: You may need to complete KYC verification for your account

3. Enable Voice Streaming (if not already enabled):

   Voice streaming may not be enabled by default on all accounts:

   - Contact Exotel support at `hello@exotel.com`
   - Request: "Enable Voicebot Applet for voice streaming for account [Your Account SID]"
   - Include your use case: "AI voice bot integration"

4. Create Custom App in App Bazaar:

   - Navigate to App Bazaar in your Exotel dashboard
   - Click "Create Custom App" or edit an existing app
   - Build your call flow:

     **Add Voicebot Applet**

     - Drag the "Voicebot" applet to your call flow
     - Configure the Voicebot Applet:
       - **URL**: `wss://your-ngrok-url.ngrok.io/ws`
       - **Record**: Enable if you want call recordings

     **Optional: Add Hangup Applet**

     - Drag a "Hangup" applet at the end to properly terminate calls

   - Your final flow should look like:
     ```
     Call Start → [Voicebot Applet] → [Hangup Applet]
     ```

5. Link Number to App:

   - Navigate to "ExoPhones" in your dashboard
   - Find your purchased number
   - Click the edit/pencil icon
   - Under "App", select the custom app you just created
   - Save the configuration

### Run your Bot

The bot.py file uses the Pipecat development runner, which runs a FastAPI server in order to receive connections.

1. To get started, we'll run our bot.py file:

```bash
uv run bot.py --transport exotel --proxy your_ngrok_url
```

> Replace `your_ngrok_url` with your ngrok URL (e.g. your-subdomain.ngrok.io)

### Call your Bot

Place a call to the number associated with your bot. The bot will answer and start the conversation.

## Production Deployment

### 1. Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to Pipecat Cloud.

### 2. Update Exotel App Bazaar Configuration

Update your Voicebot Applet configuration to use your production server:

- Change the WebSocket URL from your ngrok URL to your production server URL — this should be the Pipecat Cloud base URL:

  ```bash
  wss://api.pipecat.daily.co/ws/exotel?serviceHost=AGENT_NAME.ORGANIZATION_NAME
  ```

  Replace:

  - `AGENT_NAME` with your deployed agent's name
  - `ORGANIZATION_NAME` with your organization ID

- Update custom parameters as needed for your production environment
- If the bot is deployed to a region other than us-west (default), update the websocket url with region. For example, if deployed in `eu-central`, the url becomes `"wss://eu-central.api.pipecat.daily.co/ws/exotel"`


### Call your Bot

Place a call to the number associated with your bot. The bot will answer and start the conversation.

## Accessing Call Information in Your Bot

Your bot automatically receives call information through Exotel's WebSocket messages. The server extracts the `from` and `to` phone numbers and makes them available to your bot.

In your `bot.py`, you can access this information from the WebSocket connection. The Pipecat development runner extracts this data using the `parse_telephony_websocket` function. This allows your bot to provide personalized responses based on who's calling and which number they called.



================================================
FILE: exotel-chatbot/inbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.exotel import ExotelFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in an audio call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
    logger.info(f"Auto-detected transport: {transport_type}")

    serializer = ExotelFrameSerializer(
        stream_sid=call_data["stream_id"],
        call_sid=call_data["call_id"],
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: exotel-chatbot/inbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: exotel-chatbot/inbound/env.example
================================================
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=


================================================
FILE: exotel-chatbot/inbound/pcc-deploy.toml
================================================
agent_name = "exotel-chatbot-dial-in"
image = "your_username/exotel-chatbot-dial-in:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "exotel-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: exotel-chatbot/inbound/pyproject.toml
================================================
[project]
name = "exotel-chatbot-dial-in"
version = "0.1.0"
description = "Exotel dial-in example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,openai,silero,deepgram,runner]>=0.0.85",
  "pipecatcloud>=0.2.4"
]



================================================
FILE: exotel-chatbot/outbound/README.md
================================================
# Exotel Chatbot: Outbound

This project is a FastAPI-based chatbot that integrates with Exotel to make outbound calls with personalized call information. The project uses Exotel's Connect API to initiate calls and App Bazaar configuration to handle WebSocket connections.

## How It Works

When you want to make an outbound call:

1. **Send POST request**: `POST /start` with a phone number to call
2. **Server calls Exotel Connect API**: Uses "Connect Two Numbers" API
3. **Exotel calls your bot number first**: Your bot number "answers" and connects to WebSocket
4. **Exotel calls the customer second**: Customer answers and gets connected to your bot
5. **Audio flows**: Customer ↔ Exotel ↔ WebSocket ↔ Your Bot

## Architecture

```
curl request → /start endpoint → Exotel Connect API → Bot number called →
App Bazaar triggers → WebSocket connects → Customer called → Audio bridged
```

## Prerequisites

### Exotel

- An Exotel account with:
  - API Key and API Token
  - Account SID
  - A purchased phone number that supports voice calls (this will be your bot number)
  - Voice streaming enabled (contact support if not available)

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

```bash
cd outbound
uv sync
```

2. Get your Exotel credentials:

- **API Key & Token**: Found in your [Exotel Console](https://my.exotel.com/) → API Settings
- **Account SID**: Available in your API settings page
- **Phone Number**: [Purchase a phone number](https://my.exotel.com/) that supports voice calls (this becomes your bot number)

3. Set up environment variables:

```bash
cp env.example .env
# Edit .env with your API keys
```

## Local Development

### Configure Your Bot Number in App Bazaar

**Important**: Your bot number (EXOTEL_PHONE_NUMBER) must be configured to connect to WebSocket when called.

1. Start ngrok:
   In a new terminal, start ngrok to tunnel the local server:

   ```sh
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

2. Configure your bot number in App Bazaar:

   - Navigate to **ExoPhones** in your Exotel dashboard
   - Find your bot number (the one in EXOTEL_PHONE_NUMBER)
   - Click edit and create/assign an App Bazaar flow:

     **Create App Bazaar Flow:**

     - Navigate to App Bazaar → Create Custom App
     - Build your call flow:

       **Add Voicebot Applet**

       - Drag the "Voicebot" applet to your call flow
       - Configure the Voicebot Applet:
         - **URL**: `wss://your-ngrok-url.ngrok.io/ws`
         - **Record**: Enable if you want call recordings

       **Optional: Add Hangup Applet**

       - Drag a "Hangup" applet at the end to properly terminate calls

     - Your final flow should look like:

       ```
       Call Start → [Voicebot Applet] → [Hangup Applet]
       ```

     - Save your App Bazaar configuration
     - **Assign this flow to your bot number** in ExoPhones settings

### Run the Local Server

```bash
uv run server.py
```

The server will start on port 7860.

## Making an Outbound Call

With the server running and your bot number configured in App Bazaar, you can initiate an outbound call:

```bash
curl -X POST https://your-ngrok-url.ngrok.io/start \
  -H "Content-Type: application/json" \
  -d '{
    "dialout_settings": {
      "phone_number": "+1234567890"
    }
  }'
```

**What happens:**

1. Your server calls Exotel's Connect API
2. Exotel calls your bot number first (triggers WebSocket connection)
3. Exotel calls the customer number second
4. Both calls are bridged together
5. Customer talks directly to your bot

**Expected response:**

```json
{
  "call_sid": "5570510625ba6bab3d653ab0c479199a",
  "status": "call_initiated",
  "phone_number": "+1234567890"
}
```

Replace:

- `your-ngrok-url.ngrok.io` with your actual ngrok URL
- `+1234567890` with the phone number you want to call

## Production Deployment

### 1. Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to Pipecat Cloud.

### 2. Deploy the Server

The `server.py` handles outbound call initiation and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (initiates Exotel calls)

### 3. Update Your Bot Number's App Bazaar Configuration

Update your bot number's Voicebot Applet configuration for production:

- Navigate to **ExoPhones** → find your bot number → edit the assigned App Bazaar flow
- Update the Voicebot Applet URL to use Pipecat Cloud:

  ```bash
  wss://api.pipecat.daily.co/ws/exotel?serviceHost=AGENT_NAME.ORGANIZATION_NAME
  ```

  Replace:

  - `AGENT_NAME` with your deployed agent's name
  - `ORGANIZATION_NAME` with your organization ID

> Alternatively, you can test your Pipecat Cloud deployment by keeping your bot number pointed to your local server via ngrok.
> If the bot is deployed to a region other than us-west (default), update the websocket url with region. For example, if deployed in `eu-central`, the url becomes `"wss://eu-central.api.pipecat.daily.co/ws/exotel"`

### Call your Bot

As you did before, initiate a call via `curl` command to trigger your bot to dial a number.

## Accessing Call Information in Your Bot

Your bot automatically receives call information through Exotel's WebSocket messages. The server extracts the `from` and `to` phone numbers and makes them available to your bot.

In your `bot.py`, you can access this information from the WebSocket connection. The Pipecat development runner extracts this data using the `parse_telephony_websocket` function. This allows your bot to provide personalized responses based on who's calling and which number they called.

## Key Differences from Other Providers

- **Connect Two Numbers API**: Exotel calls your bot number first, then the customer
- **Bot Number Configuration**: Your bot number must be pre-configured in App Bazaar
- **No Dynamic XML**: Unlike Plivo/Twilio, no dynamic XML response endpoints needed
- **Call Bridging**: Exotel handles bridging the bot and customer calls automatically
- **Automatic Call Information**: Phone numbers provided automatically in WebSocket messages



================================================
FILE: exotel-chatbot/outbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.exotel import ExotelFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly assistant. "
                "Your responses will be read aloud, so keep them concise and conversational. "
                "Avoid special characters or formatting. "
                "Begin by saying: 'Hello! This is an automated call from our Exotel chatbot demo.' "
            ),
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Starting outbound call conversation")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
    logger.info(f"Auto-detected transport: {transport_type}")

    serializer = ExotelFrameSerializer(
        stream_sid=call_data["stream_id"],
        call_sid=call_data["call_id"],
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)



================================================
FILE: exotel-chatbot/outbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: exotel-chatbot/outbound/env.example
================================================
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=

# Exotel API credentials (required)
EXOTEL_API_KEY=
EXOTEL_API_TOKEN=
EXOTEL_SID=

# Your Exotel phone number for outbound calls
EXOTEL_PHONE_NUMBER=

# Note: Your bot number should be configured in App Bazaar to connect to WebSocket


================================================
FILE: exotel-chatbot/outbound/pcc-deploy.toml
================================================
agent_name = "exotel-chatbot-dial-out"
image = "your_username/exotel-chatbot-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "exotel-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: exotel-chatbot/outbound/pyproject.toml
================================================
[project]
name = "exotel-chatbot-dial-out"
version = "0.1.0"
description = "Exotel dial-out example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,openai,silero,deepgram,runner]>=0.0.85",
  "pipecatcloud>=0.2.4"
]



================================================
FILE: exotel-chatbot/outbound/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""server.py

Webhook server to handle outbound call requests, initiate calls via Exotel API,
and handle subsequent WebSocket connections for Media Streams.
"""

import os
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse

load_dotenv(override=True)


# ----------------- HELPERS ----------------- #


async def make_exotel_call(session: aiohttp.ClientSession, to_number: str, from_number: str):
    """Make an outbound call using Exotel's Connect API."""
    api_key = os.getenv("EXOTEL_API_KEY")
    api_token = os.getenv("EXOTEL_API_TOKEN")
    sid = os.getenv("EXOTEL_SID")

    if not all([api_key, api_token, sid]):
        raise ValueError("Missing Exotel credentials: EXOTEL_API_KEY, EXOTEL_API_TOKEN, EXOTEL_SID")

    # Exotel Connect API endpoint
    url = f"https://api.exotel.com/v1/Accounts/{sid}/Calls/connect"

    # Use form data for Exotel Connect Two Numbers API
    data = {
        "From": from_number,  # Bot number (called first, connects to WebSocket via App Bazaar)
        "To": to_number,  # Customer number (called second, after bot "answers")
        "CallerId": from_number,  # Your ExoPhone number
        "CallType": "trans",  # Transactional call
    }

    # Use HTTP Basic Auth
    auth = aiohttp.BasicAuth(api_key, api_token)

    async with session.post(url, data=data, auth=auth) as response:
        if response.status != 200:
            error_text = await response.text()
            raise Exception(f"Exotel API error ({response.status}): {error_text}")

        # Exotel returns XML by default, extract key information
        result_text = await response.text()

        # Extract Sid from XML response for tracking
        call_sid = "unknown"
        if "<Sid>" in result_text:
            start = result_text.find("<Sid>") + 5
            end = result_text.find("</Sid>")
            if end > start:
                call_sid = result_text[start:end]

        return {"status": "call_initiated", "call_sid": call_sid}


# ----------------- API ----------------- #


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create aiohttp session for Exotel API calls
    app.state.session = aiohttp.ClientSession()
    yield
    # Close session when shutting down
    await app.state.session.close()


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for testing
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/start")
async def initiate_outbound_call(request: Request) -> JSONResponse:
    """Handle outbound call request and initiate call via Exotel."""
    print("Received outbound call request")

    try:
        data = await request.json()

        # Validate request data
        if not data.get("dialout_settings"):
            raise HTTPException(
                status_code=400, detail="Missing 'dialout_settings' in the request body"
            )

        if not data["dialout_settings"].get("phone_number"):
            raise HTTPException(
                status_code=400, detail="Missing 'phone_number' in dialout_settings"
            )

        # Extract the phone number to dial
        phone_number = str(data["dialout_settings"]["phone_number"])
        print(f"Processing outbound call to {phone_number}")

        # Initiate outbound call via Exotel Connect API
        try:
            call_result = await make_exotel_call(
                session=request.app.state.session,
                to_number=phone_number,
                from_number=os.getenv("EXOTEL_PHONE_NUMBER"),
            )

            # Extract call SID from Exotel response
            call_sid = call_result.get("call_sid", "unknown")

        except Exception as e:
            print(f"Error initiating Exotel call: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to initiate call: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

    return JSONResponse(
        {
            "call_sid": call_sid,
            "status": "call_initiated",
            "phone_number": phone_number,
        }
    )


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Handle WebSocket connection from Exotel Media Streams."""
    await websocket.accept()
    print("WebSocket connection accepted for outbound call")

    try:
        # Import the bot function from the bot module
        from bot import bot
        from pipecat.runner.types import WebSocketRunnerArguments

        # Create runner arguments and run the bot
        runner_args = WebSocketRunnerArguments(websocket=websocket)
        runner_args.handle_sigint = False

        await bot(runner_args)

    except Exception as e:
        print(f"Error in WebSocket endpoint: {e}")
        await websocket.close()


# ----------------- Main ----------------- #


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)



================================================
FILE: freeze-test/README.md
================================================
# Freeze Test Client

The purpose of this example is to create an environment for testing the bot and try to create freezing conditions.

### Approach 1: Server-Side Testing with `SimulateFreezeInput`

- Utilize only the bot `freeze_test_bot.py` with the `SimulateFreezeInput` processor. This input continuously injects frames, simulating user speech interruptions at random intervals.
- This approach excludes the use of input transport and speech-to-text (STT) functionalities.

### Approach 2: Server-Side with TypeScript Client

- Combine server-side operations with a TypeScript client.
- The client initially records a segment of audio, e.g., 5–10 seconds long. It can be anything.
- After that, it replays this recorded audio to the server at random intervals, mimicking user input interruptions.
- This helps testing interruptions in the pipeline as if real users were interacting with the bot.

## Setup

Follow these steps to set up and run the Freeze Test Client:

1. **Run the Bot Server**  
   - Set up and activate your virtual environment:
       ```bash
       python3 -m venv venv
       source venv/bin/activate  # On Windows: venv\Scripts\activate
       ```

   - Install dependencies:
      ```bash
      pip install -r requirements.txt
      ```

   - Create your `.env` file and set your env vars:
      ```bash
      cp env.example .env
      ```
   
   - Run the server:
      ```bash
      python freeze_test_bot.py
      ```

2. **Navigate to the Client Directory**
   ```bash
   cd client
   ```

3. **Install Dependencies**
   ```bash
   npm install
   ```

4. **Run the Client Application**
   ```bash
   npm run dev
   ```

5. **Access the Client in Your Browser**  
   Visit [http://localhost:5173](http://localhost:5173) to interact with the Freeze Test Client.



================================================
FILE: freeze-test/env.example
================================================
SENTRY_DSN=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=
OPENAI_API_KEY=


================================================
FILE: freeze-test/freeze_test_bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import asyncio
import os
import random
from contextlib import asynccontextmanager
from typing import Any, Dict

import sentry_sdk
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, Request, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger
from pipecat.audio.mixers.soundfile_mixer import SoundfileMixer
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import (
    CancelFrame,
    EndFrame,
    Frame,
    InterimTranscriptionFrame,
    InterruptionFrame,
    LLMContextFrame,
    LLMFullResponseEndFrame,
    LLMMessagesAppendFrame,
    StartFrame,
    StopFrame,
    TranscriptionFrame,
    TTSSpeakFrame,
    UserStartedSpeakingFrame,
    UserStoppedSpeakingFrame,
)
from pipecat.observers.loggers.debug_log_observer import DebugLogObserver
from pipecat.pipeline.parallel_pipeline import ParallelPipeline
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIProcessor
from pipecat.processors.metrics.sentry import SentryMetrics
from pipecat.processors.user_idle_processor import UserIdleProcessor
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)
from pipecat.utils.time import time_now_iso8601

load_dotenv(override=True)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handles FastAPI startup and shutdown."""
    yield  # Run app


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class SimulateFreezeInput(FrameProcessor):
    def __init__(
        self,
        **kwargs,
    ):
        super().__init__(**kwargs)
        # Whether we have seen a StartFrame already.
        self._initialized = False
        self._send_frames_task = None

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)
        if isinstance(frame, StartFrame):
            # Push StartFrame before start(), because we want StartFrame to be
            # processed by every processor before any other frame is processed.
            await self.push_frame(frame, direction)
            await self._start(frame)
        elif isinstance(frame, CancelFrame):
            logger.info("SimulateFreezeInput: Received cancel frame")
            await self._stop()
            await self.push_frame(frame, direction)
        elif isinstance(frame, EndFrame):
            logger.info("SimulateFreezeInput: Received end frame")
            await self.push_frame(frame, direction)
            await self._stop()
        elif isinstance(frame, StopFrame):
            logger.info("SimulateFreezeInput: Received stop frame")
            await self.push_frame(frame, direction)
            await self._stop()
        else:
            await self.push_frame(frame, direction)

    async def _start(self, frame: StartFrame):
        if self._initialized:
            return
        logger.info(f"Starting SimulateFreezeInput")
        self._initialized = True
        if not self._send_frames_task:
            self._send_frames_task = self.create_task(self._send_frames())

    async def _stop(self):
        logger.info(f"Stopping SimulateFreezeInput")
        self._initialized = False
        if self._send_frames_task:
            await self.cancel_task(self._send_frames_task)
            self._send_frames_task = None

    async def _send_user_text(self, text: str):
        # Emulation as if the user has spoken and the stt transcribed
        await self.push_interruption_task_frame_and_wait()
        await self.push_frame(UserStartedSpeakingFrame())
        await self.push_frame(
            TranscriptionFrame(
                text,
                "",
                time_now_iso8601(),
            )
        )
        # Need to wait before sending the UserStoppedSpeakingFrame,
        # otherwise TranscriptionFrame will be processed
        # later than the UserStoppedSpeakingFrame
        await asyncio.sleep(0.1)
        await self.push_frame(UserStoppedSpeakingFrame())

    async def _send_frames(self):
        try:
            i = 0
            while True:
                logger.debug("SimulateFreezeInput _send_frames")
                await self._send_user_text("Tell me a brief history of Brazil!")
                await asyncio.sleep(3)
                await self._send_user_text("and who has discovered it")
                i += 1
                if i >= 20:
                    break
                # sleeping 1s before interrupting
                wait_time = random.uniform(1, 10)
                await asyncio.sleep(wait_time)
        except Exception as e:
            logger.error(f"{self} exception receiving data: {e.__class__.__name__} ({e})")


OFFICE_SOUND_FILE = os.path.join(os.path.dirname(__file__), "office-ambience-24000-mono.mp3")


async def run_example(websocket_client):
    logger.info(f"Starting bot")

    # Create a transport using the WebRTC connection
    transport = FastAPIWebsocketTransport(
        websocket=websocket_client,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=ProtobufFrameSerializer(),
            audio_out_mixer=SoundfileMixer(
                sound_files={"office": OFFICE_SOUND_FILE},
                default_sound="office",
                volume=2.0,
            ),
        ),
    )

    sentry_sdk.init(
        dsn=os.getenv("SENTRY_DSN"),
        traces_sample_rate=1.0,
    )

    freeze = SimulateFreezeInput()

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    async def handle_user_idle(user_idle: UserIdleProcessor, retry_count: int) -> bool:
        if retry_count == 1:
            # First attempt: Add a gentle prompt to the conversation
            message = {
                "role": "system",
                "content": "The user has been quiet. Politely and briefly ask if they're still there.",
            }
            await user_idle.push_frame(LLMMessagesAppendFrame([message], run_llm=True))
            return True
        elif retry_count == 2:
            # Second attempt: More direct prompt
            message = {
                "role": "system",
                "content": "The user is still inactive. Ask if they'd like to continue our conversation.",
            }
            await user_idle.push_frame(LLMMessagesAppendFrame([message], run_llm=True))
            return True
        else:
            # Third attempt: End the conversation
            await user_idle.push_frame(
                TTSSpeakFrame("It seems like you're busy right now. Have a nice day!")
            )
            await task.queue_frame(EndFrame())
            return False

    user_idle = UserIdleProcessor(callback=handle_user_idle, timeout=10.0)

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
        metrics=SentryMetrics(),
    )

    llm = OpenAILLMService(
        api_key=os.getenv("OPENAI_API_KEY"),
        metrics=SentryMetrics(),
    )

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            ParallelPipeline(
                [
                    freeze,
                ],
                [
                    transport.input(),
                    stt,
                ],
            ),
            user_idle,
            rtvi,
            context_aggregator.user(),  # User responses
            llm,  # LLM
            tts,  # TTS
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            allow_interruptions=True,
            enable_metrics=True,
            enable_usage_metrics=True,
            report_only_initial_ttfb=True,
            audio_in_sample_rate=8000,
            audio_out_sample_rate=24000,
        ),
        idle_timeout_secs=120,
        observers=[
            DebugLogObserver(
                frame_types={
                    InterimTranscriptionFrame: None,
                    TranscriptionFrame: None,
                    # TTSTextFrame: None,
                    # LLMTextFrame: None,
                    LLMContextFrame: None,
                    LLMFullResponseEndFrame: None,
                    UserStartedSpeakingFrame: None,
                    UserStoppedSpeakingFrame: None,
                    InterruptionFrame: None,
                    CancelFrame: None,
                },
                exclude_fields={
                    "result",
                    "metadata",
                    "audio",
                    "image",
                    "images",
                },
            ),
        ],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.info(f"Client ready")
        await rtvi.set_bot_ready()
        # Kick off the conversation.
        # messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        # await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("WebSocket connection accepted")
    try:
        await run_example(websocket)
    except Exception as e:
        print(f"Exception in run_bot: {e}")


@app.post("/connect")
async def bot_connect(request: Request) -> Dict[Any, Any]:
    server_mode = os.getenv("WEBSOCKET_SERVER", "fast_api")
    if server_mode == "websocket_server":
        ws_url = "ws://localhost:8765"
    else:
        ws_url = "ws://localhost:7860/ws"
    return {"ws_url": ws_url}


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Pipecat Bot Runner")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    args = parser.parse_args()

    uvicorn.run(app, host=args.host, port=args.port)



================================================
FILE: freeze-test/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
pipecat-ai[silero,websocket,openai, deepgram, cartesia, sentry]



================================================
FILE: freeze-test/client/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
<div class="container">
  <div class="status-bar">
    <div class="status">
      Transport: <span id="connection-status">Disconnected</span>
    </div>
    <div class="controls">
      <button id="connect-btn">Connect</button>
      <button id="disconnect-btn" disabled>Disconnect</button>
    </div>
  </div>
  <div class="status-bar">
    <div class="status">
      Playing audio: <span id="play-audio-status"></span>
    </div>
    <div class="controls">
      <button id="play-btn">Start</button>
      <button id="stop-btn" disabled>Stop</button>
    </div>
  </div>

  <audio id="bot-audio" autoplay></audio>

  <div class="debug-panel">
    <h3>Debug Info</h3>
    <div id="debug-log"></div>
  </div>
</div>

<script type="module" src="/src/app.ts"></script>
<link rel="stylesheet" href="/src/style.css">
</body>

</html>



================================================
FILE: freeze-test/client/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.15.30",
    "@types/protobufjs": "^6.0.0",
    "@vitejs/plugin-react-swc": "^3.10.1",
    "typescript": "^5.8.3",
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/websocket-transport": "^1.2.0",
    "protobufjs": "^7.4.0"
  }
}



================================================
FILE: freeze-test/client/tsconfig.json
================================================
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */

    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */

    /* Language and Environment */
    "target": "es2016",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for legacy experimental decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */

    /* Modules */
    "module": "commonjs",                                /* Specify what module code is generated. */
    // "rootDir": "./",                                  /* Specify the root folder within your source files. */
    // "moduleResolution": "node10",                     /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "allowImportingTsExtensions": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */
    // "rewriteRelativeImportExtensions": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */
    // "resolvePackageJsonExports": true,                /* Use the package.json 'exports' field when resolving package imports. */
    // "resolvePackageJsonImports": true,                /* Use the package.json 'imports' field when resolving imports. */
    // "customConditions": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */
    // "noUncheckedSideEffectImports": true,             /* Check side effect imports. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "allowArbitraryExtensions": true,                 /* Enable importing files with any extension, provided a declaration file is present. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */

    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */

    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    // "outDir": "./",                                   /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */

    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "verbatimModuleSyntax": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */
    // "isolatedDeclarations": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true,            /* Ensure that casing is correct in imports. */

    /* Type Checking */
    "strict": true,                                      /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "strictBuiltinIteratorReturn": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */

    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true                                 /* Skip type checking all .d.ts files. */
  }
}



================================================
FILE: freeze-test/client/vite.config.js
================================================
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

export default defineConfig({
    plugins: [react()],
    server: {
        proxy: {
            // Proxy /api requests to the backend server
            '/connect': {
                target: 'http://0.0.0.0:7860', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: freeze-test/client/src/app.ts
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

/**
 * RTVI Client Implementation
 *
 * This client connects to an RTVI-compatible bot server using WebSocket.
 *
 * Requirements:
 * - A running RTVI bot server (defaults to http://localhost:7860)
 */

import {
  PipecatClient,
  PipecatClientOptions,
  RTVIEvent,
} from '@pipecat-ai/client-js';
import {
  ProtobufFrameSerializer,
  WebSocketTransport,
} from '@pipecat-ai/websocket-transport';

class RecordingSerializer extends ProtobufFrameSerializer {
  private lastTimestamp: number | null = null;
  private recordingAudioToSend: boolean = false;
  private _recordedAudio: { data: ArrayBuffer; delay: number }[] = [];

  public startRecording() {
    this.recordingAudioToSend = true;
    this._recordedAudio = [];
    this.lastTimestamp = null;
  }

  public stopRecording() {
    this.recordingAudioToSend = false;
  }

  // @ts-ignore
  serializeAudio(
    data: ArrayBuffer,
    sampleRate: number,
    numChannels: number
  ): Uint8Array | null {
    if (this.recordingAudioToSend) {
      const now = Date.now();
      // Compute delay since last packet
      const delay = this.lastTimestamp ? now - this.lastTimestamp : 0;
      this.lastTimestamp = now;
      // Save audio chunk and delay
      this._recordedAudio.push({ data, delay });
      return null;
    } else {
      return super.serializeAudio(data, sampleRate, numChannels);
    }
  }

  public get recordedAudio() {
    return this._recordedAudio;
  }
}

class WebsocketClientApp {
  private ENABLE_RECORDING_MODE = false;
  private RECORDING_TIME_MS = 10000;

  private pcClient: PipecatClient | null = null;
  private connectBtn: HTMLButtonElement | null = null;
  private disconnectBtn: HTMLButtonElement | null = null;
  private statusSpan: HTMLElement | null = null;
  private debugLog: HTMLElement | null = null;
  private botAudio: HTMLAudioElement;

  private declare websocketTransport: WebSocketTransport;
  private sendRecordedAudio: boolean = false;
  private declare recordingSerializer: RecordingSerializer;

  private playBtn: HTMLButtonElement | null = null;
  private stopBtn: HTMLButtonElement | null = null;

  constructor() {
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    //this.botAudio.playsInline = true;
    document.body.appendChild(this.botAudio);

    this.setupDOMElements();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      'connect-btn'
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      'disconnect-btn'
    ) as HTMLButtonElement;
    this.statusSpan = document.getElementById('connection-status');
    this.debugLog = document.getElementById('debug-log');
    this.playBtn = document.getElementById('play-btn') as HTMLButtonElement;
    this.stopBtn = document.getElementById('stop-btn') as HTMLButtonElement;
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  private setupEventListeners(): void {
    this.connectBtn?.addEventListener('click', () => this.connect());
    this.disconnectBtn?.addEventListener('click', () => this.disconnect());
    this.playBtn?.addEventListener('click', () =>
      this.startSendingRecordedAudio()
    );
    this.stopBtn?.addEventListener('click', () =>
      this.stopSendingRecordedAudio()
    );
  }

  /**
   * Add a timestamped message to the debug log
   */
  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3';
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50';
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.pcClient) return;
    const tracks = this.pcClient.tracks();
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.pcClient) return;

    // Listen for new tracks starting
    this.pcClient.on(RTVIEvent.TrackStarted, (track, participant) => {
      // Only handle non-local (bot) tracks
      if (!participant?.local && track.kind === 'audio') {
        this.setupAudioTrack(track);
      }
    });

    // Listen for tracks stopping
    this.pcClient.on(RTVIEvent.TrackStopped, (track, participant) => {
      this.log(
        `Track stopped: ${track.kind} from ${participant?.name || 'unknown'}`
      );
    });
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  private setupAudioTrack(track: MediaStreamTrack): void {
    this.log('Setting up audio track');
    if (
      this.botAudio.srcObject &&
      'getAudioTracks' in this.botAudio.srcObject
    ) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the RTVI client, initializes devices, and establishes the connection
   */
  public async connect(): Promise<void> {
    try {
      const startTime = Date.now();

      this.recordingSerializer = new RecordingSerializer();
      const ws_opts = {
        serializer: this.ENABLE_RECORDING_MODE
          ? this.recordingSerializer
          : new ProtobufFrameSerializer(),
        recorderSampleRate: 8000,
        playerSampleRate: 24000,
      };

      const RTVIConfig: PipecatClientOptions = {
        transport: new WebSocketTransport(ws_opts),
        enableMic: true,
        enableCam: false,
        callbacks: {
          onConnected: () => {
            this.updateStatus('Connected');
            if (this.connectBtn) this.connectBtn.disabled = true;
            if (this.disconnectBtn) this.disconnectBtn.disabled = false;
          },
          onDisconnected: () => {
            this.updateStatus('Disconnected');
            if (this.connectBtn) this.connectBtn.disabled = false;
            if (this.disconnectBtn) this.disconnectBtn.disabled = true;
            this.log('Client disconnected');
          },
          onBotReady: (data) => {
            this.log(`Bot ready: ${JSON.stringify(data)}`);
            this.setupMediaTracks();
          },
          onUserTranscript: (data) => {
            if (data.final) {
              this.log(`User: ${data.text}`);
            }
          },
          onBotTranscript: (data) => this.log(`Bot: ${data.text}`),
          onMessageError: (error) => console.error('Message error:', error),
          onError: (error) => console.error('Error:', error),
        },
      };
      this.pcClient = new PipecatClient(RTVIConfig);
      this.websocketTransport = this.pcClient.transport as WebSocketTransport;
      this.setupTrackListeners();

      this.log('Initializing devices...');
      await this.pcClient.initDevices();

      this.log('Connecting to bot...');
      await this.pcClient.startBotAndConnect({
        endpoint: 'http://localhost:7860/connect',
      });

      const timeTaken = Date.now() - startTime;
      this.log(`Connection complete, timeTaken: ${timeTaken}`);

      if (this.ENABLE_RECORDING_MODE) {
        this.log(
          `Starting to recording the next ${
            this.RECORDING_TIME_MS / 1000
          }s of audio`
        );
        this.recordingSerializer.startRecording();
        await this.sleep(this.RECORDING_TIME_MS);
        this.recordingSerializer.stopRecording();
        this.log('Recording stopped');
        this.pcClient.enableMic(false);
        this.startSendingRecordedAudio();
      }
    } catch (error) {
      this.log(`Error connecting: ${(error as Error).message}`);
      this.updateStatus('Error');
      // Clean up if there's an error
      if (this.pcClient) {
        try {
          await this.pcClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  public async disconnect(): Promise<void> {
    if (this.pcClient) {
      try {
        this.stopSendingRecordedAudio();
        await this.pcClient.disconnect();
        this.pcClient = null;
        if (
          this.botAudio.srcObject &&
          'getAudioTracks' in this.botAudio.srcObject
        ) {
          this.botAudio.srcObject
            .getAudioTracks()
            .forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }
      } catch (error) {
        this.log(`Error disconnecting: ${(error as Error).message}`);
      }
    }
  }

  private startSendingRecordedAudio() {
    this.sendRecordedAudio = true;
    if (this.playBtn) this.playBtn.disabled = true;
    if (this.stopBtn) this.stopBtn.disabled = false;
    void this.replayAudio();
  }

  private stopSendingRecordedAudio() {
    if (this.stopBtn) this.stopBtn.disabled = true;
    if (this.playBtn) this.playBtn.disabled = false;
    this.sendRecordedAudio = false;
  }

  private async replayAudio() {
    if (this.sendRecordedAudio) {
      this.log('Sending recorded audio');
      for (const chunk of this.recordingSerializer.recordedAudio) {
        await this.sleep(chunk.delay);
        this.websocketTransport.handleUserAudioStream(chunk.data);
      }
      const randomDelay = 1000 + Math.random() * (10000 - 500);
      await this.sleep(randomDelay);

      void this.replayAudio();
    }
  }

  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}

declare global {
  interface Window {
    WebsocketClientApp: typeof WebsocketClientApp;
  }
}

window.addEventListener('DOMContentLoaded', () => {
  window.WebsocketClientApp = WebsocketClientApp;
  new WebsocketClientApp();
});



================================================
FILE: freeze-test/client/src/style.css
================================================
body {
    margin: 0;
    padding: 20px;
    font-family: Arial, sans-serif;
    background-color: #f0f0f0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
}

.status-bar {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px;
    background-color: #fff;
    border-radius: 8px;
    margin-bottom: 20px;
}

.controls button {
    padding: 8px 16px;
    margin-left: 10px;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}

#connect-btn {
    background-color: #4caf50;
    color: white;
}

#disconnect-btn {
    background-color: #f44336;
    color: white;
}

button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.main-content {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.bot-container {
    display: flex;
    flex-direction: column;
    align-items: center;
}

#bot-video-container {
    width: 640px;
    height: 360px;
    background-color: #e0e0e0;
    border-radius: 8px;
    margin: 20px auto;
    overflow: hidden;
    display: flex;
    align-items: center;
    justify-content: center;
}

#bot-video-container video {
    width: 100%;
    height: 100%;
    object-fit: cover;
}

.debug-panel {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
}

.debug-panel h3 {
    margin: 0 0 10px 0;
    font-size: 16px;
    font-weight: bold;
}

#debug-log {
    height: 500px;
    overflow-y: auto;
    background-color: #f8f8f8;
    padding: 10px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 12px;
    line-height: 1.4;
}



================================================
FILE: instant-voice/README.md
================================================
# Instant Voice

This demo showcases how to enable instant voice communication as soon as a user connects.
By leveraging optimizations on both the server and client sides, users can start speaking immediately after pressing the connect button.

## How It Works

### Server-Side Improvements:
- A **pool of Daily rooms** is managed to ensure quick connections.
- When a user connects, an existing room from the pool is assigned.
- A new room is created asynchronously to maintain the predefined pool size.

### Client-Side Improvements:
- Using the **DailyTransport** property `bufferLocalAudioUntilBotReady` set to enabled, users can start speaking immediately
  upon receiving the `AUDIO_BUFFERING_STARTED` event (typically within ~1s).
- This allows users to speak even before the bot is fully ready or the WebRTC connection is fully established.

## Quick Start

### 1. Start the Bot Server

1. Navigate to the server directory:
   ```bash
   cd server
   ```
2. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Copy the `.env.example` file to `.env` and configure it:
    - Add your API keys.
5. Start the server:
   ```bash
   python src/server.py
   ```

### 2. Connect Using the Client App

For client-side setup, refer to the [JavaScript Guide](client/javascript/README.md).

## Important Notes
- The bot server **must** be running before using the client implementation.
- Ensure your environment variables are correctly set up.

## Requirements
- **Python 3.10+**
- **Node.js 16+** (for JavaScript/React client)
- **Daily API key**
- **Google API key**
- **Modern web browser with WebRTC support**



================================================
FILE: instant-voice/client/javascript/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the bot server. See the [server README](../../README).

2. Navigate to the `client/javascript` directory:

```bash
cd client/javascript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: instant-voice/client/javascript/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
<div class="container">
  <div class="status-bar">
    <div class="status">
      Buffering audio: <span id="buffering-status">No</span>
    </div>
    <div class="status">
      Transport: <span id="connection-status">Disconnected</span>
    </div>
    <div class="controls">
      <button id="connect-btn">Connect</button>
      <button id="disconnect-btn" disabled>Disconnect</button>
    </div>
  </div>

  <audio id="bot-audio" autoplay></audio>

  <div class="debug-panel">
    <h3>Debug Info</h3>
    <div id="debug-log"></div>
  </div>
</div>

<script type="module" src="/src/app.ts"></script>
<link rel="stylesheet" href="/src/style.css">
</body>

</html>



================================================
FILE: instant-voice/client/javascript/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.13.1",
    "@vitejs/plugin-react-swc": "^3.7.2",
    "typescript": "^5.7.3",
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/daily-transport": "^1.2.0"
  }
}



================================================
FILE: instant-voice/client/javascript/tsconfig.json
================================================
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */

    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */

    /* Language and Environment */
    "target": "es2016",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for legacy experimental decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */

    /* Modules */
    "module": "commonjs",                                /* Specify what module code is generated. */
    // "rootDir": "./",                                  /* Specify the root folder within your source files. */
    // "moduleResolution": "node10",                     /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "allowImportingTsExtensions": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */
    // "rewriteRelativeImportExtensions": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */
    // "resolvePackageJsonExports": true,                /* Use the package.json 'exports' field when resolving package imports. */
    // "resolvePackageJsonImports": true,                /* Use the package.json 'imports' field when resolving imports. */
    // "customConditions": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */
    // "noUncheckedSideEffectImports": true,             /* Check side effect imports. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "allowArbitraryExtensions": true,                 /* Enable importing files with any extension, provided a declaration file is present. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */

    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */

    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    // "outDir": "./",                                   /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */

    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "verbatimModuleSyntax": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */
    // "isolatedDeclarations": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true,            /* Ensure that casing is correct in imports. */

    /* Type Checking */
    "strict": true,                                      /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "strictBuiltinIteratorReturn": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */

    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true                                 /* Skip type checking all .d.ts files. */
  }
}



================================================
FILE: instant-voice/client/javascript/vite.config.js
================================================
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

export default defineConfig({
    plugins: [react()],
    server: {
        proxy: {
            // Proxy /api requests to the backend server
            '/connect': {
                target: 'http://0.0.0.0:7860', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: instant-voice/client/javascript/src/app.ts
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

/**
 * Pipecat Client Implementation
 *
 * This client connects to an RTVI-compatible bot server using WebRTC (via Daily).
 * It handles audio/video streaming and manages the connection lifecycle.
 *
 * Requirements:
 * - A running RTVI bot server (defaults to http://localhost:7860)
 * - The server must implement the /connect endpoint that returns Daily.co room credentials
 * - Browser with WebRTC support
 */

import {
  Participant,
  PipecatClient,
  PipecatClientOptions,
  RTVIEvent,
} from '@pipecat-ai/client-js';
import {
  DailyEventCallbacks,
  DailyTransport,
} from '@pipecat-ai/daily-transport';
import SoundUtils from './util/soundUtils';

/**
 * InstantVoiceClient handles the connection and media management for a real-time
 * voice and video interaction with an AI bot.
 */
class InstantVoiceClient {
  private declare pcClient: PipecatClient;
  private connectBtn: HTMLButtonElement | null = null;
  private disconnectBtn: HTMLButtonElement | null = null;
  private statusSpan: HTMLElement | null = null;
  private bufferingAudioSpan: HTMLElement | null = null;
  private debugLog: HTMLElement | null = null;
  private botAudio: HTMLAudioElement;
  private declare startTime: number;

  constructor() {
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    document.body.appendChild(this.botAudio);
    this.setupDOMElements();
    this.setupEventListeners();
    this.initializePipecatClient();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      'connect-btn'
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      'disconnect-btn'
    ) as HTMLButtonElement;
    this.statusSpan = document.getElementById('connection-status');
    this.bufferingAudioSpan = document.getElementById('buffering-status');
    this.debugLog = document.getElementById('debug-log');
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  private setupEventListeners(): void {
    this.connectBtn?.addEventListener('click', () => this.connect());
    this.disconnectBtn?.addEventListener('click', () => this.disconnect());
  }

  private initializePipecatClient(): void {
    const PipecatConfig: PipecatClientOptions = {
      transport: new DailyTransport({
        bufferLocalAudioUntilBotReady: true,
      }),
      enableMic: true,
      enableCam: false,
      callbacks: {
        onConnected: () => {
          this.updateStatus('Connected');
          if (this.connectBtn) this.connectBtn.disabled = true;
          if (this.disconnectBtn) this.disconnectBtn.disabled = false;
        },
        onDisconnected: () => {
          this.updateStatus('Disconnected');
          this.updateBufferingStatus('No');
          if (this.connectBtn) this.connectBtn.disabled = false;
          if (this.disconnectBtn) this.disconnectBtn.disabled = true;
          this.log('Client disconnected');
        },
        onBotConnected: (participant: Participant) => {
          this.log(`onBotConnected, timeTaken: ${Date.now() - this.startTime}`);
        },
        onBotReady: (data) => {
          this.log(`onBotReady, timeTaken: ${Date.now() - this.startTime}`);
          this.log(`Bot ready: ${JSON.stringify(data)}`);
          this.setupMediaTracks();
        },
        onUserTranscript: (data) => {
          if (data.final) {
            this.log(`User: ${data.text}`);
          }
        },
        onBotTranscript: (data) => this.log(`Bot: ${data.text}`),
        onMessageError: (error) => console.error('Message error:', error),
        onError: (error) => console.error('Error:', error),
        onAudioBufferingStarted: () => {
          SoundUtils.beep();
          this.updateBufferingStatus('Yes');
          this.log(
            `onMicCaptureStarted, timeTaken: ${Date.now() - this.startTime}`
          );
        },
        onAudioBufferingStopped: () => {
          this.updateBufferingStatus('No');
          this.log(
            `onMicCaptureStopped, timeTaken: ${Date.now() - this.startTime}`
          );
        },
      } as DailyEventCallbacks,
    };

    this.pcClient = new PipecatClient(PipecatConfig);
    this.setupTrackListeners();
  }

  /**
   * Add a timestamped message to the debug log
   */
  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3';
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50';
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  /**
   * Update the connection status display
   */
  private updateBufferingStatus(status: string): void {
    if (this.bufferingAudioSpan) {
      this.bufferingAudioSpan.textContent = status;
    }
    this.log(`BufferingStatus: ${status}`);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.pcClient) return;
    const tracks = this.pcClient.tracks();
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.pcClient) return;

    // Listen for new tracks starting
    this.pcClient.on(RTVIEvent.TrackStarted, (track, participant) => {
      // Only handle non-local (bot) tracks
      if (!participant?.local && track.kind === 'audio') {
        this.setupAudioTrack(track);
      }
    });

    // Listen for tracks stopping
    this.pcClient.on(RTVIEvent.TrackStopped, (track, participant) => {
      this.log(
        `Track stopped: ${track.kind} from ${participant?.name || 'unknown'}`
      );
    });
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  private setupAudioTrack(track: MediaStreamTrack): void {
    this.log('Setting up audio track');
    if (
      this.botAudio.srcObject &&
      'getAudioTracks' in this.botAudio.srcObject
    ) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the Pipecat client, initializes devices, and establishes the connection
   */
  public async connect(): Promise<void> {
    try {
      this.startTime = Date.now();
      this.log('Connecting to bot...');
      await this.pcClient.startBotAndConnect({
        // The baseURL and endpoint of your bot server that the client will connect to
        endpoint: 'http://localhost:7860/connect',
      });
    } catch (error) {
      this.log(`Error connecting: ${(error as Error).message}`);
      this.updateStatus('Error');
      this.updateBufferingStatus('No');

      // Clean up if there's an error
      if (this.pcClient) {
        try {
          await this.pcClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  public async disconnect(): Promise<void> {
    try {
      await this.pcClient.disconnect();
      if (
        this.botAudio.srcObject &&
        'getAudioTracks' in this.botAudio.srcObject
      ) {
        this.botAudio.srcObject
          .getAudioTracks()
          .forEach((track) => track.stop());
        this.botAudio.srcObject = null;
      }
    } catch (error) {
      this.log(`Error disconnecting: ${(error as Error).message}`);
    }
  }
}

declare global {
  interface Window {
    InstantVoiceClient: typeof InstantVoiceClient;
  }
}

window.addEventListener('DOMContentLoaded', () => {
  window.InstantVoiceClient = InstantVoiceClient;
  new InstantVoiceClient();
});



================================================
FILE: instant-voice/client/javascript/src/style.css
================================================
body {
    margin: 0;
    padding: 20px;
    font-family: Arial, sans-serif;
    background-color: #f0f0f0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
}

.status-bar {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px;
    background-color: #fff;
    border-radius: 8px;
    margin-bottom: 20px;
}

.controls button {
    padding: 8px 16px;
    margin-left: 10px;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}

#connect-btn {
    background-color: #4caf50;
    color: white;
}

#disconnect-btn {
    background-color: #f44336;
    color: white;
}

button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.main-content {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.bot-container {
    display: flex;
    flex-direction: column;
    align-items: center;
}

#bot-video-container {
    width: 640px;
    height: 360px;
    background-color: #e0e0e0;
    border-radius: 8px;
    margin: 20px auto;
    overflow: hidden;
    display: flex;
    align-items: center;
    justify-content: center;
}

#bot-video-container video {
    width: 100%;
    height: 100%;
    object-fit: cover;
}

.debug-panel {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
}

.debug-panel h3 {
    margin: 0 0 10px 0;
    font-size: 16px;
    font-weight: bold;
}

#debug-log {
    height: 500px;
    overflow-y: auto;
    background-color: #f8f8f8;
    padding: 10px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 12px;
    line-height: 1.4;
}



================================================
FILE: instant-voice/client/javascript/src/util/soundUtils.ts
================================================
class SoundUtils {
    static beep() {
        const snd = new Audio("data:audio/wav;base64,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");
        void snd.play();
    }
}

export default SoundUtils;



================================================
FILE: instant-voice/server/env.example
================================================
DAILY_API_KEY=
GOOGLE_API_KEY=
DAILY_SAMPLE_ROOM_URL=https://yourdomain.daily.co/yourroom # (for joining the bot to the same room repeatedly for local dev)


================================================
FILE: instant-voice/server/pyproject.toml
================================================
[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = [
    "I", # Import rules
]

[tool.ruff.lint.pydocstyle]
convention = "google"


================================================
FILE: instant-voice/server/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
pipecat-ai[openai,silero,websocket,google,daily]>=0.0.82



================================================
FILE: instant-voice/server/src/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import asyncio
import os
from contextlib import asynccontextmanager
from typing import Any, Dict, List

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pipecat.transports.daily.utils import DailyRESTHelper, DailyRoomParams

# Load environment variables
load_dotenv(override=True)

NUMBER_OF_ROOMS = 1


class RoomPool:
    """Manages a pool of pre-created rooms for quick allocation."""

    def __init__(self, daily_rest_helper: DailyRESTHelper):
        self.daily_rest_helper = daily_rest_helper
        self.pool: List[Dict[str, str]] = []
        self.lock = asyncio.Lock()

    async def fill_pool(self, count: int):
        """Fills the pool with `count` new rooms."""
        for _ in range(count):
            await self.add_room()

    async def add_room(self):
        """Creates a new room and adds it to the pool."""
        try:
            room = await self.daily_rest_helper.create_room(DailyRoomParams())
            if not room.url:
                raise HTTPException(status_code=500, detail="Failed to create room")

            user_token = await self.daily_rest_helper.get_token(room.url)
            if not user_token:
                raise HTTPException(status_code=500, detail="Failed to get user token")

            bot_token = await self.daily_rest_helper.get_token(room.url)
            if not bot_token:
                raise HTTPException(status_code=500, detail="Failed to get bot token")

            async with self.lock:
                self.pool.append(
                    {"room_url": room.url, "user_token": user_token, "bot_token": bot_token}
                )

        except Exception as e:
            print(f"Error adding room to pool: {e}")

    async def get_room(self) -> Dict[str, str]:
        """Retrieves a room from the pool and requests a new one to maintain the size."""
        async with self.lock:
            if not self.pool:
                raise HTTPException(status_code=503, detail="No available rooms")

            room = self.pool.pop(0)  # Get first available room

        # Start a background task to replenish the pool
        asyncio.create_task(self.add_room())

        return room

    async def delete_room(self, room_url: str):
        """Deletes a room when it is not needed anymore"""
        await self.daily_rest_helper.delete_room_by_url(room_url)

    async def cleanup(self):
        for rooms in self.pool:
            room_url = rooms["room_url"]
            await self.delete_room(room_url)


class BotManager:
    """Manages bot subprocesses asynchronously."""

    def __init__(self):
        self.bot_procs: Dict[int, asyncio.subprocess.Process] = {}
        self.room_mappings: Dict[int, str] = {}  # Maps process ID to room URL

    async def start_bot(self, room_url: str, token: str) -> int:
        bot_file = "single_bot"
        command = f"python3 -m {bot_file} -u {room_url} -t {token}"

        try:
            proc = await asyncio.create_subprocess_shell(
                command,
                cwd=os.path.dirname(os.path.abspath(__file__)),
            )
            if proc.pid is None:
                raise HTTPException(status_code=500, detail="Failed to get subprocess PID")

            self.bot_procs[proc.pid] = proc
            self.room_mappings[proc.pid] = room_url
            # Monitor the process and delete the room when it exits
            asyncio.create_task(self._monitor_process(proc.pid))

            return proc.pid
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    async def _monitor_process(self, pid: int):
        """Monitors a bot process and deletes the associated room when it exits."""
        proc = self.bot_procs.get(pid)
        if proc:
            await proc.wait()  # Wait for the process to exit
            room_url = self.room_mappings.pop(pid, None)

            if room_url:
                await room_pool.delete_room(room_url)
                print(f"Deleted room: {room_url}")

            del self.bot_procs[pid]

    async def cleanup(self):
        """Terminates all running bot processes and deletes associated rooms."""
        for pid, proc in list(self.bot_procs.items()):
            try:
                proc.terminate()
                await asyncio.wait_for(proc.wait(), timeout=5)

                room_url = self.room_mappings.pop(pid, None)
                if room_url:
                    await room_pool.delete_room(room_url)  # Delete room when process terminates
                    print(f"Deleted room: {room_url}")

            except asyncio.TimeoutError:
                print(f"Process {pid} did not terminate in time.")
            except Exception as e:
                print(f"Error terminating process {pid}: {e}")

        # Clear remaining mappings
        self.bot_procs.clear()
        self.room_mappings.clear()


# Global instances
bot_manager = BotManager()
room_pool: RoomPool  # Will be initialized in lifespan


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handles FastAPI startup and shutdown."""
    global room_pool
    aiohttp_session = aiohttp.ClientSession()
    daily_rest_helper = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )

    room_pool = RoomPool(daily_rest_helper)
    await room_pool.fill_pool(NUMBER_OF_ROOMS)  # Fill pool on startup

    yield  # Run app

    await bot_manager.cleanup()
    await room_pool.cleanup()
    await aiohttp_session.close()


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/connect")
async def bot_connect(request: Request) -> Dict[Any, Any]:
    try:
        room = await room_pool.get_room()
        await bot_manager.start_bot(room["room_url"], room["bot_token"])
    except HTTPException as e:
        return {"error": str(e)}

    return {
        "room_url": room["room_url"],
        "token": room["user_token"],
    }


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)



================================================
FILE: instant-voice/server/src/single_bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import argparse
import asyncio
import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


def extract_arguments():
    parser = argparse.ArgumentParser(description="Instant Voice Example")
    parser.add_argument(
        "-u", "--url", type=str, required=True, help="URL of the Daily room to join"
    )
    parser.add_argument(
        "-t", "--token", type=str, required=False, help="Token of the Daily room to join"
    )
    args, unknown = parser.parse_known_args()
    url = args.url or os.getenv("DAILY_SAMPLE_ROOM_URL")
    token = args.token
    return url, token


async def main():
    room_url, token = extract_arguments()
    print(f"room_url: {room_url}")

    daily_transport = DailyTransport(
        room_url,
        token,
        "Instant voice Chatbot",
        DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_user_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
    )

    context = LLMContext()
    context_aggregator = LLMContextAggregatorPair(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]), transport=daily_transport)

    pipeline = Pipeline(
        [
            daily_transport.input(),
            context_aggregator.user(),
            rtvi,
            llm,  # LLM
            daily_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @daily_transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        logger.debug("First participant joined: {}", participant["id"])

    @daily_transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        logger.debug(f"Participant left: {participant}")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: ivr-navigation/README.md
================================================
# IVR Navigation Bot

This project demonstrates how to create a voice bot that can automatically navigate Interactive Voice Response (IVR) phone systems using AI-powered decision making.

## How It Works

1. The server receives a request with the phone number to dial out to
2. The server creates a Daily room with SIP capabilities and starts the bot
3. The bot dials the specified number and connects to an IVR system
4. The IVR Navigator automatically detects menu options and navigates toward the specified goal
5. The bot uses DTMF tones and natural language responses to traverse the phone menu
6. Once the goal is reached, the bot ends the call

## Expected Navigation Path

When calling the test number (+1-412-314-6113), the bot will navigate through Daily Pharmacy's IVR system:

1. **Main Menu**: "Press 1 for prescription services, Press 2 for pharmacy hours..."

   - Bot selects option 1 (prescription services)

2. **Date of Birth Verification**: "Please enter your date of birth..."

   - Bot enters: 01011970 (configured for Mark Backman)

3. **Prescription Number**: "Please enter your 7 digit prescription number..."

   - Bot enters: 1234567 (configured prescription)

4. **Prescription Found**: "I found your prescription for Ibuprofen 800mg..."
   - Bot receives status information and completes the call

The IVR Navigator handles this navigation automatically using the goal and patient information configured in the bot.

## Prerequisites

### Daily

- A Daily account with an API key (or Daily API key from Pipecat Cloud account)
- A phone number purchased through Daily with dial-out enabled

For detailed setup instructions on purchasing phone numbers and enabling dial-out, see the [Daily PSTN dial-out example](https://github.com/pipecat-ai/pipecat-examples/tree/main/phone-chatbot/daily-pstn-dial-out).

### AI Services

- Deepgram API key for speech-to-text
- OpenAI API key for the LLM inference
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager (recommended) or pip
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Create a virtual environment and install dependencies

   ```bash
   uv sync
   ```

2. Set up environment variables

   Copy the example file and fill in your API keys:

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server for handling dial-out requests and starting the bot
- Default configuration for development and testing

### Production (`ENV=production`)

- Bot is deployed to Pipecat Cloud; requires `PIPECAT_API_KEY` and `PIPECAT_AGENT_NAME`
- Set these when deploying to production environments
- Your FastAPI server runs either locally or deployed to your infrastructure

## Run the Bot Locally

1. Start the server:

   ```bash
   uv run server.py
   ```

2. Test the IVR navigation functionality

   With server.py running, send the following curl command from your terminal:

   ```bash
   curl -X POST "http://localhost:7860/start" \
     -H "Content-Type: application/json" \
     -d '{
       "dialout_settings": {
         "phone_number": "+14123146113"
       }
     }'
   ```

   The server will create a room, the bot will join and dial the test IVR system. The bot will automatically navigate through the Daily Pharmacy IVR menu to obtain prescription status information for the user.

3. **Observe the call (optional)**

   You can join the Daily room to listen in on the IVR navigation. When the bot starts, you'll see a message in the console like:

   ```
   Joining https://YOUR-ACCOUNT.daily.co/AUTO-GENERATED-ROOM
   ```

   Open this URL in your browser to observe the bot navigating through the IVR system in real-time.

## Production Deployment

You can deploy your bot to Pipecat Cloud and server to your infrastructure to run this bot in a production environment.

#### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

You'll only deploy your `bot.py` file.

#### Deploy the Server

The `server.py` handles dial-out requests and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (receives requests and starts the bot)

#### Environment Variables for Production

Add these to your production environment:

```bash
ENV=production
PIPECAT_API_KEY=your_pipecat_cloud_api_key
PIPECAT_AGENT_NAME=your-agent-name
```

The server automatically detects the environment and routes bot starting requests accordingly.

## Learn More About IVR Navigation

For comprehensive information about IVR navigation capabilities, configuration options, and advanced usage patterns, see the [IVR Navigation Guide](https://docs.pipecat.ai/guides/fundamentals/ivr).

## Troubleshooting

### IVR navigation gets stuck

- Check that the bot's goal and patient information match the IVR system's expected inputs
- Review the bot logs for navigation decision details

### Call connects but no bot is heard

- Ensure your Daily API key is correct and has SIP capabilities
- Verify that the Cartesia API key and voice ID are correct
- Check that dial-out is enabled on your Daily domain

### Bot starts but disconnects immediately

- Check the Daily logs for any error messages
- Ensure your server has stable internet connectivity



================================================
FILE: ivr-navigation/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""simple_dialout.py.

Daily PSTN Dial-out Bot.
"""

import os
from typing import Any

from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.extensions.ivr.ivr_navigator import IVRNavigator
from pipecat.frames.frames import EndTaskFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv()


async def handle_end_call(params: FunctionCallParams):
    await params.llm.push_frame(EndTaskFrame(), FrameDirection.UPSTREAM)


async def run_bot(transport: BaseTransport, handle_sigint: bool) -> None:
    """Run the voice bot with the given parameters."""

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY", ""),
        voice_id="b7d50908-b17c-442d-ad8d-810c63997ed9",  # Use Helpful Woman voice by default
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    llm.register_function("end_call", handle_end_call)

    end_call_function = FunctionSchema(
        name="end_call",
        description="End the call",
        properties={
            "reason": {
                "type": "string",
                "description": "The reason for ending the call",
            },
        },
        required=["reason"],
    )

    tools = ToolsSchema(standard_tools=[end_call_function])

    ivr_navigator = IVRNavigator(
        llm=llm,
        ivr_prompt="""You are calling Daily Pharmacy on behalf of Mark Backman. Your goal is to obtain status of his prescription and whether it's ready for pickup. Once you have received that information, call the end_call function with the reason 'Call completed' to end the call.

Relevant information:
- Date of birth: 01/01/1970
- Prescription number: 1234567""",
    )

    context = LLMContext(tools=tools)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            ivr_navigator,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    # ------------ RETRY LOGIC VARIABLES ------------
    max_retries = 5
    retry_count = 0
    dialout_successful = False

    async def attempt_dialout(dialout_params):
        """Attempt to start dialout with retry logic."""
        nonlocal retry_count, dialout_successful

        if retry_count < max_retries and not dialout_successful:
            retry_count += 1
            phone_number = dialout_params.get("phoneNumber", "unknown")
            logger.info(
                f"Attempting dialout (attempt {retry_count}/{max_retries}) to: {phone_number}"
            )
            await transport.start_dialout(dialout_params)
        else:
            logger.error(f"Maximum retry attempts ({max_retries}) reached. Giving up on dialout.")

    @transport.event_handler("on_joined")
    async def on_joined(transport, data):
        # Extract dialout settings from transport's body data
        body_data = getattr(transport, "_body_data", {})
        dialout_settings = body_data.get("dialout_settings", {})

        if not dialout_settings.get("phone_number"):
            logger.error("Dial-out phone number not found in the dial-out settings")
            return

        phone_number = dialout_settings["phone_number"]
        caller_id = dialout_settings.get("caller_id")

        # Build dialout parameters conditionally
        dialout_params = {"phoneNumber": phone_number}
        if caller_id:
            dialout_params["callerId"] = caller_id
            logger.debug(f"Including caller ID in dialout: {caller_id}")

        logger.debug(f"Dialout parameters: {dialout_params}")
        logger.debug(f"Dialout settings detected; starting dialout to number: {phone_number}")
        await attempt_dialout(dialout_params)

    @transport.event_handler("on_dialout_connected")
    async def on_dialout_connected(transport, data):
        logger.debug(f"Dial-out connected: {data}")

    @transport.event_handler("on_dialout_answered")
    async def on_dialout_answered(transport, data):
        nonlocal dialout_successful
        logger.debug(f"Dial-out answered: {data}")
        dialout_successful = True  # Mark as successful to stop retries
        # The bot will wait to hear the user before the bot speaks

    @transport.event_handler("on_dialout_error")
    async def on_dialout_error(transport, data: Any):
        logger.error(f"Dial-out error (attempt {retry_count}/{max_retries}): {data}")

        if retry_count < max_retries:
            # Get dialout params again for retry
            body_data = getattr(transport, "_body_data", {})
            dialout_settings = body_data.get("dialout_settings", {})
            phone_number = dialout_settings.get("phone_number")
            caller_id = dialout_settings.get("caller_id")

            dialout_params = {"phoneNumber": phone_number}
            if caller_id:
                dialout_params["callerId"] = caller_id

            logger.info(f"Retrying dialout")
            await attempt_dialout(dialout_params)
        else:
            logger.error(f"All {max_retries} dialout attempts failed. Stopping bot.")
            await task.cancel()

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        logger.debug(f"First participant joined: {participant['id']}")

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        logger.debug(f"Participant left: {participant}, reason: {reason}")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    # Body is always a dict (compatible with both local and Pipecat Cloud)
    body_data = runner_args.body
    room_url = body_data.get("room_url")
    token = body_data.get("token")
    dialout_settings = body_data.get("dialout_settings", {})

    if not dialout_settings.get("phone_number"):
        logger.error("Phone number is required in dialout_settings.")
        return None

    transport_params = DailyParams(
        api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        api_key=os.getenv("DAILY_API_KEY", ""),
        audio_in_enabled=True,
        audio_out_enabled=True,
        video_out_enabled=False,
        vad_analyzer=SileroVADAnalyzer(),
        audio_in_user_tracks=False,  # Set False for multi-user call to mix tracks
    )

    transport = DailyTransport(
        room_url,
        token,
        "Simple Dial-out Bot",
        transport_params,
    )

    # Store body data in transport for access in event handlers
    transport._body_data = body_data

    await run_bot(transport, runner_args.handle_sigint)



================================================
FILE: ivr-navigation/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: ivr-navigation/env.example
================================================
# Daily credentials
DAILY_API_KEY=your_daily_api_key
DAILY_API_URL=https://api.daily.co/v1

# Service keys
DEEPGRAM_API_KEY=your_deepgram_api_key
OPENAI_API_KEY=your_openai_api_key
CARTESIA_API_KEY=your_cartesia_api_key

# Environment mode: "local" for development, "production" for cloud deployment
ENV=local

# Local server URL (only needed if running on different port/host)
LOCAL_SERVER_URL=http://localhost:7860

# Pipecat Cloud (only needed for production)
PIPECAT_API_KEY=
PIPECAT_AGENT_NAME=daily-pstn-dial-out


================================================
FILE: ivr-navigation/pcc-deploy.toml
================================================
agent_name = "daily-pstn-dial-out"
image = "your_username/daily-pstn-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "daily-pstn-secrets"

[scaling]
	min_agents = 1



================================================
FILE: ivr-navigation/pyproject.toml
================================================
[project]
name = "daily-pstn-dial-out"
version = "0.1.0"
description = "Daily PSTN Dial-out example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,cartesia,openai,silero,runner]>=0.0.84",
    "pipecatcloud>=0.2.4",
]

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []


================================================
FILE: ivr-navigation/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Webhook server to handle Daily PSTN dial-out requests and start the voice bot.

This server provides endpoints for handling Daily PSTN dial-out requests and starting the bot.
The server automatically detects the environment (local vs production) and routes
bot starting requests accordingly:
- Local: Uses internal /start_bot endpoint
- Production: Calls Pipecat Cloud API

All call data (room_url, token, dialout_settings) flows through the body parameter
to ensure consistency between local and cloud deployments.
"""

import asyncio
import json
import os
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from loguru import logger
from pipecat.runner.daily import configure
from pipecat.runner.types import DailyRunnerArguments

from bot import bot as bot_function

load_dotenv()

# ----------------- API ----------------- #


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create aiohttp session to be used for Daily API calls
    app.state.session = aiohttp.ClientSession()
    yield
    # Close session when shutting down
    await app.state.session.close()


app = FastAPI(lifespan=lifespan)


@app.post("/start")
async def handle_dial_out_request(request: Request) -> JSONResponse:
    """Handle dial-out request.

    This endpoint:
    1. Receives dial-out request with phone number and optional caller ID
    2. Creates a Daily room with dial-out capabilities
    3. Starts the bot (locally or via Pipecat Cloud based on ENV)
    4. Returns room details for monitoring

    Returns:
        JSONResponse with room_url and token
    """
    logger.debug("Received dial-out request")

    # Get the dial-out properties from the request
    try:
        data = await request.json()

        # Handle webhook test requests
        if "test" in data:
            return JSONResponse({"test": True})

        if not data.get("dialout_settings"):
            raise HTTPException(
                status_code=400, detail="Missing 'dialout_settings' in the request body"
            )

        if not data["dialout_settings"].get("phone_number"):
            raise HTTPException(
                status_code=400, detail="Missing 'phone_number' in dialout_settings"
            )

        # Extract the phone number we want to dial out to
        phone_number = str(data["dialout_settings"]["phone_number"])
        logger.debug(f"Processing dial-out to {phone_number}")

        # Create a Daily room with dial-out capabilities
        try:
            # Use sip_caller_phone for room configuration (this sets up SIP capabilities)
            room_details = await configure(request.app.state.session, sip_caller_phone=phone_number)
        except Exception as e:
            logger.error(f"Error creating Daily room: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create Daily room: {str(e)}")

        # Extract necessary details
        room_url = room_details.room_url
        token = room_details.token
        logger.debug(f"Created Daily room: {room_url} with token: {token}")

        # Start the bot - either locally or via Pipecat Cloud
        try:
            # Check environment mode (local development vs production)
            environment = os.getenv("ENV", "local")  # "local" or "production"

            # Prepare body data with all necessary information
            # This data structure is consistent between local and cloud deployments
            body_data = {
                **data,  # Original request data (dialout_settings)
                "room_url": room_url,
                "token": token,
            }

            if environment == "production":
                # Production: Call Pipecat Cloud API to start the bot
                pipecat_api_key = os.getenv("PIPECAT_API_KEY")
                agent_name = os.getenv("PIPECAT_AGENT_NAME")

                if not pipecat_api_key:
                    raise HTTPException(
                        status_code=500, detail="PIPECAT_API_KEY required for production mode"
                    )

                logger.debug(f"Starting bot via Pipecat Cloud for dial-out to {phone_number}")
                async with request.app.state.session.post(
                    f"https://api.pipecat.daily.co/v1/public/{agent_name}/start",
                    headers={
                        "Authorization": f"Bearer {pipecat_api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "createDailyRoom": False,  # We already created the room
                        "body": body_data,
                    },
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise HTTPException(
                            status_code=500,
                            detail=f"Failed to start bot via Pipecat Cloud: {error_text}",
                        )
                    cloud_data = await response.json()
                    logger.debug(f"Bot started successfully via Pipecat Cloud")
            else:
                # Local development: Call internal /start_bot endpoint to start the bot
                local_server_url = os.getenv("LOCAL_SERVER_URL", "http://localhost:7860")

                logger.debug(
                    f"Starting bot via local /start_bot endpoint for dial-out to {phone_number}"
                )
                async with request.app.state.session.post(
                    f"{local_server_url}/start_bot",
                    headers={"Content-Type": "application/json"},
                    json={
                        "createDailyRoom": False,  # We already created the room
                        "body": body_data,
                    },
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise HTTPException(
                            status_code=500,
                            detail=f"Failed to start bot via local /start_bot endpoint: {error_text}",
                        )
                    local_data = await response.json()
                    logger.debug(f"Bot started successfully via local /start_bot endpoint")

        except Exception as e:
            logger.error(f"Error starting bot: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to start bot: {str(e)}")

    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

    # Return room details for monitoring
    return JSONResponse({"room_url": room_url, "token": token})


@app.post("/start_bot")
async def start_bot_endpoint(request: Request):
    """Start bot endpoint for local development.

    This endpoint mimics the Pipecat Cloud API pattern, receiving the same body data
    structure and starting the bot locally. Used only in local development mode.

    Args:
        request: FastAPI request containing body with room_url, token, dialout_settings

    Returns:
        dict: Success status and phone number
    """
    try:
        # Parse the request body
        request_data = await request.json()
        body = request_data.get("body", {})

        # Extract required data from body
        room_url = body.get("room_url")
        token = body.get("token")
        dialout_settings = body.get("dialout_settings", {})
        phone_number = dialout_settings.get("phone_number")

        if not all([room_url, token, phone_number]):
            raise HTTPException(
                status_code=400,
                detail="Missing required parameters in body: room_url, token, dialout_settings.phone_number",
            )

        # Create runner arguments with body data
        # Note: room_url and token are passed via body, not as direct arguments
        runner_args = DailyRunnerArguments(
            room_url=None,  # Data comes from body
            token=None,  # Data comes from body
            body=body,
        )
        runner_args.handle_sigint = False

        # Start the bot in the background
        asyncio.create_task(bot_function(runner_args))

        return {"status": "Bot started successfully", "phone_number": phone_number}

    except Exception as e:
        logger.error(f"Error in /start_bot endpoint: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start bot: {str(e)}")


@app.get("/health")
async def health_check():
    """Health check endpoint.

    Returns:
        dict: Status indicating server health
    """
    return {"status": "healthy"}


# ----------------- Main ----------------- #


if __name__ == "__main__":
    # Run the server
    port = int(os.getenv("PORT", "7860"))
    logger.info(f"Starting server on port {port}")
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)



================================================
FILE: local-input-select-stt/README.md
================================================
# Pipecat Audio Transcription Example 🚀🎙️

Welcome to the **Pipecat Audio Transcription Example**!

This project showcases how to integrate the awesome [pipecat](https://github.com/pipecat-ai/pipecat) library with a neat textual interface (powered by [Textual](https://github.com/Textualize/textual)) to select audio devices, perform real-time speech-to-text (STT) transcription using [Whisper](https://github.com/openai/whisper).

> **Note:** Although the script allows you to select both input and output audio devices, this example only utilizes the audio **input** for transcription.

---

## 🎉 Features

- **Interactive Audio Device Selection:**  
  Choose your preferred audio input device using a cool, textual UI.
- **State-of-the-Art Transcription:**  
  Leverage Whisper's large model (running on CUDA) for high-quality, real-time STT.
- **Live Transcription Logging:**  
  Watch your spoken words transform into text on your console instantly.
- **Easy Setup:**  
  Everything you need is in the [`requirements.txt`](./requirements.txt).

---

## 🎥 Demo

Get a quick glimpse of the app in action!  
_(Don't worry – I'll be adding a GIF demo here soon!)_

![Demo GIF](demo.gif)

---

## 🔧 Installation

Install Dependencies:

```bash
pip install -r requirements.txt
```

---

## 🚀 Usage

Run the main script:

```bash
python bot.py
```

When the app launches, you'll see a textual interface that lets you select your audio input device. Once selected, the app will begin capturing audio, transcribing it using Whisper.

---

## ⚙️ How It Works

1. **LocalAudioTransport:**  
   Captures audio from your chosen input device.
2. **WhisperSTTService:**  
   Processes the audio stream using Whisper's large model for speech-to-text conversion.
3. **TranscriptionLogger:**  
   Logs the transcribed text to the console as soon as it's processed.

---

## 📦 Dependencies

The project relies on:

- [pipecat](https://github.com/yourusername/pipecat) – For building the audio processing pipeline.
- [Textual](https://github.com/Textualize/textual) – For the interactive terminal UI.
- [Whisper](https://github.com/openai/whisper) – For state-of-the-art STT transcription.

---

## Example improvements:

I plan to improve this example with local LLM calls and audio output.



================================================
FILE: local-input-select-stt/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import sys
from typing import Tuple

from dotenv import load_dotenv
from loguru import logger
from pipecat.observers.loggers.transcription_log_observer import TranscriptionLogObserver
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineTask
from pipecat.services.whisper.stt import Model, WhisperSTTService
from pipecat.transports.local.audio import LocalAudioTransport, LocalAudioTransportParams
from select_audio_device import AudioDevice, run_device_selector

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


async def main(input_device: int, output_device: int):
    transport = LocalAudioTransport(
        LocalAudioTransportParams(
            audio_in_enabled=True,
            audio_out_enabled=False,
            input_device_index=input_device,
            output_device_index=output_device,
        )
    )

    stt = WhisperSTTService(device="cuda", model=Model.LARGE, no_speech_prob=0.3)

    pipeline = Pipeline([transport.input(), stt])

    task = PipelineTask(pipeline, observers=[TranscriptionLogObserver()])

    runner = PipelineRunner(handle_sigint=False if sys.platform == "win32" else True)

    await asyncio.gather(runner.run(task))


if __name__ == "__main__":
    res: Tuple[AudioDevice, AudioDevice, int] = asyncio.run(
        run_device_selector()  # runs the textual app that allows to select input device
    )

    asyncio.run(main(res[0].index, res[1].index))



================================================
FILE: local-input-select-stt/requirements.txt
================================================
pipecat-ai[whisper, openai]
textual==1.0.0
pydantic-settings==2.7.1
pyaudio==0.2.14



================================================
FILE: local-input-select-stt/select_audio_device.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

from typing import List, Optional, Tuple

import pyaudio
from pydantic import BaseModel, ConfigDict, Field
from pydantic_settings import BaseSettings
from textual.app import App, ComposeResult
from textual.containers import Container
from textual.widgets import Footer, Header, Label, ListItem, ListView, Select
from textual.widgets.option_list import Option

# ─── DATA MODELS ───────────────────────────────────────────────────────────────


class HostApi(BaseModel):
    index: int
    struct_version: int = Field(..., alias="structVersion")
    type: int
    name: str
    device_count: int = Field(..., alias="deviceCount")
    default_input_device: int = Field(..., alias="defaultInputDevice")
    default_output_device: int = Field(..., alias="defaultOutputDevice")


class AudioDevice(BaseModel):
    model_config = ConfigDict(populate_by_name=True)
    index: int
    struct_version: int = Field(..., alias="structVersion")
    name: str
    host_api: int = Field(..., alias="hostApi")
    max_input_channels: int = Field(..., alias="maxInputChannels")
    max_output_channels: int = Field(..., alias="maxOutputChannels")
    default_low_input_latency: float = Field(..., alias="defaultLowInputLatency")
    default_low_output_latency: float = Field(..., alias="defaultLowOutputLatency")
    default_high_input_latency: float = Field(..., alias="defaultHighInputLatency")
    default_high_output_latency: float = Field(..., alias="defaultHighOutputLatency")
    default_sample_rate: float = Field(..., alias="defaultSampleRate")


# ─── SETTINGS MODEL ───────────────────────────────────────────────────────────


class AudioSettings(BaseSettings):  # to save settings to a file
    host_api: Optional[int] = None
    input_device: Optional[AudioDevice] = None
    output_device: Optional[AudioDevice] = None

    class Config:
        env_file = "settings.env"  # or adjust as needed

    def save_to_json(self, filepath: str) -> None:
        with open(filepath, "w") as f:
            f.write(self.model_dump_json(indent=2))


# ─── TEXTUAL APP ──────────────────────────────────────────────────────────────


class AudioDeviceSelectorApp(App):
    CSS = """
    Screen {
        align: center middle;
    }
    #container {
        width: 80%;
        border: round green;
        padding: 1 2;
    }
    """

    def __init__(
        self,
        default_host_api: Optional[int] = None,
        default_input_device: Optional[AudioDevice] = None,
        default_output_device: Optional[AudioDevice] = None,
        **kwargs,
    ) -> None:
        super().__init__(**kwargs)
        # Save defaults passed from settings.
        self.default_host_api: Optional[int] = default_host_api
        self.default_input_device: Optional[AudioDevice] = default_input_device
        self.default_output_device: Optional[AudioDevice] = default_output_device

        self.pyaudio_instance = pyaudio.PyAudio()

        # Static datastructures: host APIs and devices as well‐typed models.
        self.host_apis: List[HostApi] = []
        self.current_host_api: Optional[int] = None

        self.all_input_devices: List[AudioDevice] = []
        self.all_output_devices: List[AudioDevice] = []
        self.input_devices: List[AudioDevice] = []
        self.output_devices: List[AudioDevice] = []

        # Stage management: first select input, then output.
        self.stage: str = "input"
        self.selected_input_device: Optional[AudioDevice] = None
        self.selected_output_device: Optional[AudioDevice] = None
        host_api_count: int = self.pyaudio_instance.get_host_api_count()
        for i in range(host_api_count):
            raw_api = self.pyaudio_instance.get_host_api_info_by_index(i)
            # Inject the index (if not already present)
            raw_api["index"] = i
            try:
                api = HostApi.parse_obj(raw_api)
                self.host_apis.append(api)
            except Exception as e:
                # Skip APIs that don't conform.
                continue

    def compose(self) -> ComposeResult:
        options: List[Tuple[str, Option]] = [
            (
                api.name,
                Option(
                    prompt=str(api.name) if api.name else f"Host API {api.index}",
                    id=str(api.index),
                ),
            )
            for api in self.host_apis
        ]

        yield Header()

        yield Footer()
        with Container(id="container"):
            yield Label("Select Host API:", id="host-api-label")
            # Create the Select widget with no options initially.
            self.host_api_select: Select[HostApi] = Select(options=options, id="host-api-select")
            yield self.host_api_select
            self.prompt = Label("Select Input Audio Device:", id="prompt")
            yield self.prompt
            self.list_view = ListView(id="device-list")
            yield self.list_view

    def on_mount(self) -> None:
        # Populate host APIs from PyAudio.

        # Build the dropdown options.

        self.host_api_select.refresh()  # Force a redraw

        # Determine the default host API.
        if self.default_host_api is not None:
            self.current_host_api = self.default_host_api
        else:
            default_api_info = self.pyaudio_instance.get_default_host_api_info()
            self.current_host_api = default_api_info["index"]

        # Delay setting the dropdown's value until the widget is fully initialized.
        self.set_timer(
            0,
            lambda: setattr(self.host_api_select, "value", str(self.current_host_api)),
        )

        # Load all devices and parse them into AudioDevice objects.
        device_count: int = self.pyaudio_instance.get_device_count()
        for i in range(device_count):
            raw_device = self.pyaudio_instance.get_device_info_by_index(i)
            raw_device["index"] = i
            try:
                device = AudioDevice.parse_obj(raw_device)
            except Exception as e:
                # Skip devices missing required fields.
                continue
            if device.max_input_channels > 0:
                self.all_input_devices.append(device)
            if device.max_output_channels > 0:
                self.all_output_devices.append(device)

        self.filter_devices()
        self.populate_list(self.input_devices)
        if self.default_input_device:
            self._select_default_in_list(self.default_input_device)

    def filter_devices(self) -> None:
        """Filter devices based on the selected host API."""
        self.input_devices = [
            d for d in self.all_input_devices if d.host_api == self.current_host_api
        ]
        self.output_devices = [
            d for d in self.all_output_devices if d.host_api == self.current_host_api
        ]

    def populate_list(self, devices: List[AudioDevice]) -> None:
        """Populate the ListView with a list of AudioDevice objects."""
        self.list_view.clear()
        for dev in devices:
            item_text: str = f"{dev.name} (Index: {dev.index})"
            item = ListItem(Label(item_text))
            # Attach the AudioDevice instance to the widget.
            item.device_info = dev  # type: ignore
            self.list_view.append(item)

    def _select_default_in_list(self, default_device: AudioDevice) -> None:
        """Pre-select the default device if present in the current list."""
        for idx, item in enumerate(self.list_view.children):
            if hasattr(item, "device_info") and item.device_info.index == default_device.index:
                self.list_view.index = idx
                break

    async def on_select_changed(self, event: Select.Changed) -> None:
        """Handle changes in the host API dropdown."""
        if event.select.id == "host-api-select":
            self.current_host_api = int(event.value.id)
            self.filter_devices()
            if self.stage == "input":
                self.populate_list(self.input_devices)
                if self.default_input_device:
                    self._select_default_in_list(self.default_input_device)
            elif self.stage == "output":
                self.populate_list(self.output_devices)
                if self.default_output_device:
                    self._select_default_in_list(self.default_output_device)

    async def on_list_view_selected(self, message: ListView.Selected) -> None:
        """Record device selection and switch stages."""
        selected_item = message.item
        device_info: AudioDevice = selected_item.device_info  # type: ignore
        if self.stage == "input":
            self.selected_input_device = device_info
            self.stage = "output"
            self.prompt.update("Select Output Audio Device:")
            self.populate_list(self.output_devices)
            if self.default_output_device:
                self._select_default_in_list(self.default_output_device)
        elif self.stage == "output":
            self.selected_output_device = device_info
            await self.action_quit()


# ─── HELPER FUNCTIONS ─────────────────────────────────────────────────────────


async def run_device_selector(
    default_host_api: Optional[int] = None,
    default_input_device: Optional[AudioDevice] = None,
    default_output_device: Optional[AudioDevice] = None,
) -> Tuple[AudioDevice, AudioDevice, int]:
    app = AudioDeviceSelectorApp(
        default_host_api=default_host_api,
        default_input_device=default_input_device,
        default_output_device=default_output_device,
    )
    await app.run_async()

    # The current_host_api is guaranteed to be set.
    return app.selected_input_device, app.selected_output_device, app.current_host_api  # type: ignore



================================================
FILE: local-smart-turn/README.md
================================================
# Smart Turn Detection Demo

This demo showcases Pipecat's Smart Turn Detection feature - an advanced conversational turn detection system that uses machine learning to identify when a speaker has finished their turn in a conversation. Unlike basic Voice Activity Detection (VAD) which only detects speech vs. silence, Smart Turn detects natural conversational cues like intonation patterns, pacing, and linguistic signals.

This demo uses the [pipecat-ai/smart-turn-v3](https://huggingface.co/pipecat-ai/smart-turn-v3) model - an open-source, community-driven conversational turn detection model designed to provide more natural turn-taking in voice interactions. This new version of the model allows fast CPU inference, with no GPU required.

In the client UI, you can see the transcription messages along with the smart-turn model's prediction results in real-time.

## Try the demo

Try the hosted version of the demo here: https://pcc-smart-turn.vercel.app/.

## Run the demo locally

### Run the Server

1. Open a terminal and navigate to the server directory:

   ```bash
   cd server
   ```

2. Set up and activate your virtual environment:

   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. Create your .env file and set your env vars:

   ```bash
   cp env.example .env
   ```

   Keys to provide:

   - GOOGLE_API_KEY
   - CARTESIA_API_KEY
   - DEEPGRAM_API_KEY
   - DAILY_API_KEY

4. Run the server:

   ```bash
   LOCAL_RUN=1 python server.py
   ```

### Run the client

1. Open a new terminal and navigate to the client directory:

   ```bash
   cd client
   ```

2. Install dependencies:

   ```bash
   npm install
   ```

3. Create your .env.local file:

   ```bash
   cp env.local.example .env.local
   ```

   > Note: No keys need to be modified. `NEXT_PUBLIC_API_BASE_URL` is already configured for local use.

4. Start the development server:

   ```bash
   npm run dev
   ```

5. Open [http://localhost:3000](http://localhost:3000) in your browser.

## Deploy the app

### Deploy the server to Pipecat Cloud

1. Navigate to server

   ```bash
   cd server
   ```

2. You should already have a .env set up from running locally. If not, do that now.

3. Update your build and deploy scripts.

   - In build.sh, set `DOCKER_USERNAME` and `AGENT_NAME`.
   - In pcc-deploy.toml, set `image`, which specifies where your Docker image is stored.

4. Build your Docker image by running the build script:

   ```bash
   ./build.sh
   ```

   > Note: This builds, tags and pushes your docker image and assumes Docker Hub is the container registry.

5. Make sure you have the Pipecat Cloud CLI installed:

   ```bash
   pip install pipecatcloud
   ```

6. Login via the Pipecat Cloud CLI:

   ```bash
   pcc auth login
   ```

   > Note: If you don't have an account, sign up at https://pipecat.daily.co.

7. Add a secrets set:

   ```bash
   pcc secrets set pcc-smart-turn-secrets --file .env
   ```

8. Deploy your agent:

   ```bash
   pcc deploy
   ```

   > Note: This uses your pcc-deploy.toml settings. Modify as needed.

### Deploy the client to Vercel

This project uses TypeScript, React, and Next.js, making it a perfect fit for [Vercel](https://vercel.com/).

- In your client directory, install Vercel's CLI tool: `npm install -g vercel`
- Verify it's installed using `vercel --version`
- Log in your Vercel account using `vercel login`
- Deploy your client to Vercel using `vercel`

Follow the vercel prompts to deploy your project.

### Test your deployed app

Now with the client and server deployed, you can join the call using your Vercel URL.

See the debug information for the Smart Turn data. It prints a log line for each smart-turn inference.



================================================
FILE: local-smart-turn/client/env.local.example
================================================
NEXT_PUBLIC_API_BASE_URL=http://localhost:7860
PIPECAT_CLOUD_API_KEY=
AGENT_NAME=pcc-smart-turn


================================================
FILE: local-smart-turn/client/eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;



================================================
FILE: local-smart-turn/client/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;



================================================
FILE: local-smart-turn/client/package.json
================================================
{
  "name": "my-nextjs-app",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/client-react": "^1.0.1",
    "@pipecat-ai/daily-transport": "^1.2.0",
    "next": "15.3.1",
    "react": "^19.0.0",
    "react-dom": "^19.0.0"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.2.3",
    "typescript": "^5"
  }
}



================================================
FILE: local-smart-turn/client/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/components/*": ["./src/components/*"],
      "@/providers/*": ["./src/providers/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
FILE: local-smart-turn/client/src/app/globals.css
================================================
body {
  margin: 0;
  padding: 20px;
  font-family: Arial, sans-serif;
  background-color: #f0f0f0;
}

.app {
  max-width: 1200px;
  margin: 0 auto;
}

.status-bar {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px;
  background-color: #fff;
  border-radius: 8px;
  margin-bottom: 20px;
}

.controls button {
  padding: 8px 16px;
  margin-left: 10px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.connect-btn {
  background-color: #4caf50;
  color: white;
}

.disconnect-btn {
  background-color: #f44336;
  color: white;
}

.main-content {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 20px;
}

.bot-container {
  display: flex;
  flex-direction: column;
  align-items: center;
}

.video-container {
  width: 640px;
  height: 360px;
  background-color: #ddd;
  margin-bottom: 20px;
  border-radius: 8px;
  overflow: hidden;
}

.video-container video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}

.mic-enabled {
  background-color: #4caf50;
  color: white;
}

.mic-disabled {
  background-color: #f44336;
  color: white;
}



================================================
FILE: local-smart-turn/client/src/app/layout.tsx
================================================
import './globals.css';
import { PipecatProvider } from '@/providers/PipecatProvider';

export const metadata = {
  title: 'Pipecat React Client',
  description: 'Pipecat RTVI Client using Next.js',
  icons: {
    icon: [{ url: '/favicon.svg', type: 'image/svg+xml' }],
  },
};

export default function RootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en">
      <head>
        <link rel="icon" href="/favicon.svg" type="image/svg+xml" />
      </head>
      <body>
        <PipecatProvider>{children}</PipecatProvider>
      </body>
    </html>
  );
}



================================================
FILE: local-smart-turn/client/src/app/page.tsx
================================================
'use client';

import {
  PipecatClientAudio,
  PipecatClientVideo,
  usePipecatClientTransportState,
} from '@pipecat-ai/client-react';
import { ConnectButton } from '../components/ConnectButton';
import { StatusDisplay } from '../components/StatusDisplay';
import { DebugDisplay } from '../components/DebugDisplay';

function BotVideo() {
  const transportState = usePipecatClientTransportState();
  const isConnected = transportState !== 'disconnected';

  return (
    <div className="bot-container">
      <div className="video-container">
        {isConnected && <PipecatClientVideo participant="bot" fit="cover" />}
      </div>
    </div>
  );
}

export default function Home() {
  return (
    <div className="app">
      <div className="status-bar">
        <StatusDisplay />
        <ConnectButton />
      </div>

      <div className="main-content">
        <BotVideo />
      </div>

      <DebugDisplay />
      <PipecatClientAudio />
    </div>
  );
}



================================================
FILE: local-smart-turn/client/src/app/api/connect/route.ts
================================================
import { NextResponse, NextRequest } from 'next/server';

export async function POST(request: NextRequest) {
  const { MY_CUSTOM_DATA } = await request.json();

  try {
    const response = await fetch(
      `https://api.pipecat.daily.co/v1/public/${process.env.AGENT_NAME}/start`,
      {
        method: 'POST',
        headers: {
          Authorization: `Bearer ${process.env.PIPECAT_CLOUD_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          // Create Daily room
          createDailyRoom: true,
          // Optionally set Daily room properties
          dailyRoomProperties: { start_video_off: true },
          // Optionally pass custom data to the bot
          body: { MY_CUSTOM_DATA },
        }),
      }
    );

    if (!response.ok) {
      throw new Error(`API responded with status: ${response.status}`);
    }

    const data = await response.json();

    // Transform the response to match what RTVI client expects
    return NextResponse.json(data);
  } catch (error) {
    console.error('API error:', error);
    return NextResponse.json(
      { error: 'Failed to start agent' },
      { status: 500 }
    );
  }
}



================================================
FILE: local-smart-turn/client/src/components/ConnectButton.tsx
================================================
import {
  usePipecatClient,
  usePipecatClientTransportState,
} from '@pipecat-ai/client-react';

// Get the API base URL from environment variables
// Default to "/api" if not specified
// "/api" is the default for Next.js API routes and used
// for the Pipecat Cloud deployed agent
const API_BASE_URL = process.env.NEXT_PUBLIC_API_BASE_URL || '/api';

export function ConnectButton() {
  const client = usePipecatClient();
  const transportState = usePipecatClientTransportState();
  const isConnected = ['connected', 'ready'].includes(transportState);

  const handleClick = async () => {
    if (!client) {
      console.error('RTVI client is not initialized');
      return;
    }

    try {
      if (isConnected) {
        await client.disconnect();
      } else {
        await client.startBotAndConnect({
          endpoint: `${API_BASE_URL}/connect`,
          requestData: { foo: 'bar' },
        });
      }
    } catch (error) {
      console.error('Connection error:', error);
    }
  };

  return (
    <div className="controls">
      <button
        className={isConnected ? 'disconnect-btn' : 'connect-btn'}
        onClick={handleClick}
        disabled={
          !client || ['connecting', 'disconnecting'].includes(transportState)
        }>
        {isConnected ? 'Disconnect' : 'Connect'}
      </button>
    </div>
  );
}



================================================
FILE: local-smart-turn/client/src/components/DebugDisplay.css
================================================
.debug-panel {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
}

.debug-panel h3 {
  margin: 0 0 10px 0;
  font-size: 16px;
  font-weight: bold;
}

.debug-log {
  height: 200px;
  overflow-y: auto;
  background-color: #f8f8f8;
  padding: 10px;
  border-radius: 4px;
  font-family: monospace;
  font-size: 12px;
  line-height: 1.4;
}

.debug-log div {
  margin-bottom: 4px;
}



================================================
FILE: local-smart-turn/client/src/components/DebugDisplay.tsx
================================================
import { useRef, useCallback } from 'react';
import {
  Participant,
  RTVIEvent,
  TransportState,
  TranscriptData,
  BotLLMTextData,
} from '@pipecat-ai/client-js';
import { usePipecatClient, useRTVIClientEvent } from '@pipecat-ai/client-react';
import './DebugDisplay.css';

interface SmartTurnResultData {
  type: 'smart_turn_result';
  is_complete: boolean;
  probability: number;
  inference_time_ms: number; // Pure model inference time
  server_total_time_ms: number; // Server processing time
  e2e_processing_time_ms: number; // Complete end-to-end time
}

export function DebugDisplay() {
  const debugLogRef = useRef<HTMLDivElement>(null);
  const client = usePipecatClient();

  const log = useCallback((message: string) => {
    if (!debugLogRef.current) return;

    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;

    // Add styling based on message type
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3'; // blue for user
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50'; // green for bot
    } else if (message.includes('Smart Turn:')) {
      entry.style.color = '#9C27B0'; // purple for smart turn
    }

    debugLogRef.current.appendChild(entry);
    debugLogRef.current.scrollTop = debugLogRef.current.scrollHeight;
  }, []);

  // Log transport state changes
  useRTVIClientEvent(
    RTVIEvent.TransportStateChanged,
    useCallback(
      (state: TransportState) => {
        log(`Transport state changed: ${state}`);
      },
      [log]
    )
  );

  // Log bot connection events
  useRTVIClientEvent(
    RTVIEvent.BotConnected,
    useCallback(
      (participant?: Participant) => {
        log(`Bot connected: ${JSON.stringify(participant)}`);
      },
      [log]
    )
  );

  useRTVIClientEvent(
    RTVIEvent.BotDisconnected,
    useCallback(
      (participant?: Participant) => {
        log(`Bot disconnected: ${JSON.stringify(participant)}`);
      },
      [log]
    )
  );

  // Log track events
  useRTVIClientEvent(
    RTVIEvent.TrackStarted,
    useCallback(
      (track: MediaStreamTrack, participant?: Participant) => {
        log(
          `Track started: ${track.kind} from ${participant?.name || 'unknown'}`
        );
      },
      [log]
    )
  );

  useRTVIClientEvent(
    RTVIEvent.TrackStopped,
    useCallback(
      (track: MediaStreamTrack, participant?: Participant) => {
        log(
          `Track stopped: ${track.kind} from ${participant?.name || 'unknown'}`
        );
      },
      [log]
    )
  );

  // Log bot ready state and check tracks
  useRTVIClientEvent(
    RTVIEvent.BotReady,
    useCallback(() => {
      log(`Bot ready`);

      if (!client) return;

      const tracks = client.tracks();
      log(
        `Available tracks: ${JSON.stringify({
          local: {
            audio: !!tracks.local.audio,
            video: !!tracks.local.video,
          },
          bot: {
            audio: !!tracks.bot?.audio,
            video: !!tracks.bot?.video,
          },
        })}`
      );
    }, [client, log])
  );

  // Log transcripts
  useRTVIClientEvent(
    RTVIEvent.UserTranscript,
    useCallback(
      (data: TranscriptData) => {
        // Only log final transcripts
        if (data.final) {
          log(`User: ${data.text}`);
        }
      },
      [log]
    )
  );

  useRTVIClientEvent(
    RTVIEvent.BotTranscript,
    useCallback(
      (data: BotLLMTextData) => {
        log(`Bot: ${data.text}`);
      },
      [log]
    )
  );

  useRTVIClientEvent(
    RTVIEvent.ServerMessage,
    useCallback(
      (data: SmartTurnResultData) => {
        log(
          `Smart Turn:
    ${data.is_complete ? 'COMPLETE' : 'INCOMPLETE'},
    Probability: ${(data.probability * 100).toFixed(1)}%,
    Model inference: ${data.inference_time_ms?.toFixed(2) || 'N/A'}ms,
    Server processing: ${data.server_total_time_ms?.toFixed(2) || 'N/A'}ms,
    End-to-end: ${data.e2e_processing_time_ms?.toFixed(2) || 'N/A'}ms`
        );
      },
      [log]
    )
  );
  return (
    <div className="debug-panel">
      <h3>Debug Info</h3>
      <div ref={debugLogRef} className="debug-log" />
    </div>
  );
}



================================================
FILE: local-smart-turn/client/src/components/StatusDisplay.tsx
================================================
import { usePipecatClientTransportState } from '@pipecat-ai/client-react';

export function StatusDisplay() {
  const transportState = usePipecatClientTransportState();

  return (
    <div className="status">
      Status: <span>{transportState}</span>
    </div>
  );
}



================================================
FILE: local-smart-turn/client/src/providers/PipecatProvider.tsx
================================================
'use client';

import { PipecatClient } from '@pipecat-ai/client-js';
import { DailyTransport } from '@pipecat-ai/daily-transport';
import { PipecatClientProvider } from '@pipecat-ai/client-react';
import { PropsWithChildren, useEffect, useState } from 'react';

export function PipecatProvider({ children }: PropsWithChildren) {
  const [client, setClient] = useState<PipecatClient | null>(null);

  useEffect(() => {
    const pcClient = new PipecatClient({
      transport: new DailyTransport(),
      enableMic: true,
      enableCam: false,
    });

    setClient(pcClient);
  }, []);

  if (!client) {
    return null;
  }

  return (
    <PipecatClientProvider client={client}>{children}</PipecatClientProvider>
  );
}



================================================
FILE: local-smart-turn/server/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import os
import sys

import aiohttp
from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    BotStartedSpeakingFrame,
    BotStoppedSpeakingFrame,
    Frame,
    LLMRunFrame,
    MetricsFrame,
    OutputImageRawFrame,
    SpriteFrame,
)
from pipecat.metrics.metrics import SmartTurnMetricsData
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import (
    RTVIConfig,
    RTVIObserver,
    RTVIProcessor,
    RTVIServerMessageFrame,
)
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from pipecatcloud.agent import DailySessionArguments

load_dotenv(override=True)

# Check if we're in local development mode
LOCAL = os.getenv("LOCAL_RUN")

logger.remove()
logger.add(sys.stderr, level="DEBUG")

sprites = []
script_dir = os.path.dirname(__file__)

# Load sequential animation frames
for i in range(1, 26):
    # Build the full path to the image file
    full_path = os.path.join(script_dir, f"assets/robot0{i}.png")
    # Get the filename without the extension to use as the dictionary key
    # Open the image and convert it to bytes
    with Image.open(full_path) as img:
        sprites.append(OutputImageRawFrame(image=img.tobytes(), size=img.size, format=img.format))

# Create a smooth animation by adding reversed frames
flipped = sprites[::-1]
sprites.extend(flipped)

# Define static and animated states
quiet_frame = sprites[0]  # Static frame for when bot is listening
talking_frame = SpriteFrame(images=sprites)  # Animation sequence for when bot is talking


class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states.

    Switches between static (listening) and animated (talking) states based on
    the bot's current speaking status.
    """

    def __init__(self):
        super().__init__()
        self._is_talking = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and update animation state.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Switch to talking animation when bot starts speaking
        if isinstance(frame, BotStartedSpeakingFrame):
            if not self._is_talking:
                await self.push_frame(talking_frame)
                self._is_talking = True
        # Return to static frame when bot stops speaking
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
            self._is_talking = False

        await self.push_frame(frame, direction)


class SmartTurnMetricsProcessor(FrameProcessor):
    """Processes the metrics data from Smart Turn Analyzer.

    This processor is responsible for handling smart turn metrics data
    and forwarding it to the client UI via RTVI.
    """

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and handle Smart Turn metrics.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Handle Smart Turn metrics
        if isinstance(frame, MetricsFrame):
            for metrics in frame.data:
                if isinstance(metrics, SmartTurnMetricsData):
                    logger.info(f"Smart Turn metrics: {metrics}")

                    # Create a payload with the smart turn prediction data
                    smart_turn_data = {
                        "type": "smart_turn_result",
                        "is_complete": metrics.is_complete,
                        "probability": metrics.probability,
                        "inference_time_ms": metrics.inference_time_ms,
                        "server_total_time_ms": metrics.server_total_time_ms,
                        "e2e_processing_time_ms": metrics.e2e_processing_time_ms,
                    }

                    # Send the data to the client via RTVI
                    rtvi_frame = RTVIServerMessageFrame(data=smart_turn_data)
                    await self.push_frame(rtvi_frame)

        await self.push_frame(frame, direction)


async def main(transport: DailyTransport):
    # Configure your STT, LLM, and TTS services here
    # Swap out different processors or properties to customize your bot
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
    llm = GoogleLLMService(api_key=os.getenv("GOOGLE_API_KEY"))
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    # Set up the initial context for the conversation
    # You can specified initial system and assistant messages here
    messages = [
        {
            "role": "system",
            "content": "You are Chatbot, a friendly, helpful robot. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by introducing yourself.",
        },
    ]

    # This sets up the LLM context by providing messages and tools
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    ta = TalkingAnimation()
    smart_turn_metrics_processor = SmartTurnMetricsProcessor()

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    # A core voice AI pipeline
    # Add additional processors to customize the bot's behavior
    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            smart_turn_metrics_processor,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            ta,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.debug("Client ready event received")
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        logger.info("First participant joined: {}", participant["id"])
        # Push a static frame to show the bot is listening
        await task.queue_frame(quiet_frame)

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        logger.info("Participant left: {}", participant)
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False, force_gc=True)

    await runner.run(task)


async def bot(args: DailySessionArguments):
    """Main bot entry point compatible with the FastAPI route handler.

    Args:
        room_url: The Daily room URL
        token: The Daily room token
        body: The configuration object from the request body
        session_id: The session ID for logging
    """
    from pipecat.audio.filters.krisp_filter import KrispFilter

    logger.info(f"Bot process initialized {args.room_url} {args.token}")
    async with aiohttp.ClientSession() as session:
        transport = DailyTransport(
            args.room_url,
            args.token,
            "Smart Turn Bot",
            params=DailyParams(
                audio_in_enabled=True,
                audio_in_filter=KrispFilter(),
                audio_out_enabled=True,
                video_out_enabled=True,
                video_out_width=1024,
                video_out_height=576,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                turn_analyzer=LocalSmartTurnAnalyzerV3(),
            ),
        )

        try:
            await main(transport)
            logger.info("Bot process completed")
        except Exception as e:
            logger.exception(f"Error in bot process: {str(e)}")
            raise


# Local development
async def local_daily():
    """Daily transport for local development."""
    from runner import configure

    try:
        async with aiohttp.ClientSession() as session:
            (room_url, token) = await configure(session)
            transport = DailyTransport(
                room_url,
                token,
                "Smart Turn Bot",
                params=DailyParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    video_out_enabled=True,
                    video_out_width=1024,
                    video_out_height=576,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )

            await main(transport)
    except Exception as e:
        logger.exception(f"Error in local development mode: {e}")


# Local development entry point
if LOCAL and __name__ == "__main__":
    try:
        asyncio.run(local_daily())
    except Exception as e:
        logger.exception(f"Failed to run in local mode: {e}")



================================================
FILE: local-smart-turn/server/build.sh
================================================
#!/bin/bash
set -e

VERSION="0.1"
DOCKER_USERNAME=""
AGENT_NAME="pcc-smart-turn"

# Build the Docker image with the correct context
echo "Building Docker image..."
docker build --platform=linux/arm64 -t "$DOCKER_USERNAME/$AGENT_NAME:$VERSION" -t "$DOCKER_USERNAME/$AGENT_NAME:latest" .

# Push the Docker images
echo "Pushing Docker image $DOCKER_USERNAME/$AGENT_NAME:$VERSION..."
docker push "$DOCKER_USERNAME/$AGENT_NAME:$VERSION"

echo "Pushing Docker image $DOCKER_USERNAME/$AGENT_NAME:latest..."
docker push "$DOCKER_USERNAME/$AGENT_NAME:latest"

echo "Successfully built and pushed $DOCKER_USERNAME/$AGENT_NAME:$VERSION and $DOCKER_USERNAME/$AGENT_NAME:latest"


================================================
FILE: local-smart-turn/server/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

COPY ./requirements.txt requirements.txt

RUN pip install --no-cache-dir --upgrade -r requirements.txt

COPY ./assets assets
COPY ./bot.py bot.py



================================================
FILE: local-smart-turn/server/env.example
================================================
GOOGLE_API_KEY=
CARTESIA_API_KEY=
DEEPGRAM_API_KEY=
DAILY_API_KEY=


================================================
FILE: local-smart-turn/server/pcc-deploy.toml
================================================
agent_name = "pcc-smart-turn"
image = "your-username/pcc-smart-turn:0.1"
secret_set = "pcc-smart-turn-secrets"
enable_krisp = true

[scaling]
	min_instances = 0



================================================
FILE: local-smart-turn/server/requirements.txt
================================================
pipecatcloud
pipecat-ai[google,daily,deepgram,cartesia,silero,local-smart-turn-v3]
python-dotenv



================================================
FILE: local-smart-turn/server/runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os

import aiohttp
from pipecat.transports.daily.utils import DailyRESTHelper


async def configure(aiohttp_session: aiohttp.ClientSession):
    """Configure the Daily room and Daily REST helper."""
    parser = argparse.ArgumentParser(description="Daily AI SDK Bot Sample")
    parser.add_argument(
        "-u", "--url", type=str, required=False, help="URL of the Daily room to join"
    )
    parser.add_argument(
        "-k",
        "--apikey",
        type=str,
        required=False,
        help="Daily API Key (needed to create an owner token for the room)",
    )

    args, unknown = parser.parse_known_args()

    url = args.url or os.getenv("DAILY_SAMPLE_ROOM_URL")
    key = args.apikey or os.getenv("DAILY_API_KEY")

    if not url:
        raise Exception(
            "No Daily room specified. use the -u/--url option from the command line, or set DAILY_SAMPLE_ROOM_URL in your environment to specify a Daily room URL."
        )

    if not key:
        raise Exception(
            "No Daily API key specified. use the -k/--apikey option from the command line, or set DAILY_API_KEY in your environment to specify a Daily API key, available from https://dashboard.daily.co/developers."
        )

    daily_rest_helper = DailyRESTHelper(
        daily_api_key=key,
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )

    # Create a meeting token for the given room with an expiration 1 hour in
    # the future.
    expiry_time: float = 60 * 60

    token = await daily_rest_helper.get_token(url, expiry_time)

    return (url, token)



================================================
FILE: local-smart-turn/server/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""RTVI Bot Server Implementation.

This FastAPI server manages RTVI bot instances and provides endpoints for both
direct browser access and RTVI client connections. It handles:
- Creating Daily rooms
- Managing bot processes
- Providing connection credentials
- Monitoring bot status

Requirements:
- Daily API key (set in .env file)
- Python 3.10+
- FastAPI
- Running bot implementation
"""

import argparse
import os
import subprocess
from contextlib import asynccontextmanager
from typing import Any, Dict

import aiohttp
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, RedirectResponse
from pipecat.transports.daily.utils import DailyRESTHelper, DailyRoomParams

# Load environment variables from .env file
load_dotenv(override=True)

# Maximum number of bot instances allowed per room
MAX_BOTS_PER_ROOM = 1

# Dictionary to track bot processes: {pid: (process, room_url)}
bot_procs = {}

# Store Daily API helpers
daily_helpers = {}


def cleanup():
    """Cleanup function to terminate all bot processes.

    Called during server shutdown.
    """
    for entry in bot_procs.values():
        proc = entry[0]
        proc.terminate()
        proc.wait()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan manager that handles startup and shutdown tasks.

    - Creates aiohttp session
    - Initializes Daily API helper
    - Cleans up resources on shutdown
    """
    aiohttp_session = aiohttp.ClientSession()
    daily_helpers["rest"] = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )
    yield
    await aiohttp_session.close()
    cleanup()


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


async def create_room_and_token() -> tuple[str, str]:
    """Helper function to create a Daily room and generate an access token.

    Returns:
        tuple[str, str]: A tuple containing (room_url, token)

    Raises:
        HTTPException: If room creation or token generation fails
    """
    room = await daily_helpers["rest"].create_room(DailyRoomParams())
    if not room.url:
        raise HTTPException(status_code=500, detail="Failed to create room")

    token = await daily_helpers["rest"].get_token(room.url)
    if not token:
        raise HTTPException(status_code=500, detail=f"Failed to get token for room: {room.url}")

    return room.url, token


@app.get("/")
async def start_agent(request: Request):
    """Endpoint for direct browser access to the bot.

    Creates a room, starts a bot instance, and redirects to the Daily room URL.

    Returns:
        RedirectResponse: Redirects to the Daily room URL

    Raises:
        HTTPException: If room creation, token generation, or bot startup fails
    """
    print("Creating room")
    room_url, token = await create_room_and_token()
    print(f"Room URL: {room_url}")

    # Check if there is already an existing process running in this room
    num_bots_in_room = sum(
        1 for proc in bot_procs.values() if proc[1] == room_url and proc[0].poll() is None
    )
    if num_bots_in_room >= MAX_BOTS_PER_ROOM:
        raise HTTPException(status_code=500, detail=f"Max bot limit reached for room: {room_url}")

    # Spawn a new bot process
    try:
        proc = subprocess.Popen(
            [f"python3 bot.py -u {room_url} -t {token}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )
        bot_procs[proc.pid] = (proc, room_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    return RedirectResponse(room_url)


@app.post("/connect")
async def rtvi_connect(request: Request) -> Dict[Any, Any]:
    """RTVI connect endpoint that creates a room and returns connection credentials.

    This endpoint is called by RTVI clients to establish a connection.

    Returns:
        Dict[Any, Any]: Authentication bundle containing room_url and token

    Raises:
        HTTPException: If room creation, token generation, or bot startup fails
    """
    print("Creating room for RTVI connection")
    room_url, token = await create_room_and_token()
    print(f"Room URL: {room_url}")

    # Start the bot process
    try:
        proc = subprocess.Popen(
            [f"python3 -m bot -u {room_url} -t {token}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )
        bot_procs[proc.pid] = (proc, room_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    # Return the authentication bundle in format expected by DailyTransport
    return {"room_url": room_url, "token": token}


@app.get("/status/{pid}")
def get_status(pid: int):
    """Get the status of a specific bot process.

    Args:
        pid (int): Process ID of the bot

    Returns:
        JSONResponse: Status information for the bot

    Raises:
        HTTPException: If the specified bot process is not found
    """
    # Look up the subprocess
    proc = bot_procs.get(pid)

    # If the subprocess doesn't exist, return an error
    if not proc:
        raise HTTPException(status_code=404, detail=f"Bot with process id: {pid} not found")

    # Check the status of the subprocess
    status = "running" if proc[0].poll() is None else "finished"
    return JSONResponse({"bot_id": pid, "status": status})


if __name__ == "__main__":
    import uvicorn

    # Parse command line arguments for server configuration
    default_host = os.getenv("HOST", "0.0.0.0")
    default_port = int(os.getenv("FAST_API_PORT", "7860"))

    parser = argparse.ArgumentParser(description="Daily Storyteller FastAPI server")
    parser.add_argument("--host", type=str, default=default_host, help="Host address")
    parser.add_argument("--port", type=int, default=default_port, help="Port number")
    parser.add_argument("--reload", action="store_true", help="Reload code on change")

    config = parser.parse_args()

    # Start the FastAPI server
    uvicorn.run(
        "server:app",
        host=config.host,
        port=config.port,
        reload=config.reload,
    )



================================================
FILE: news-chatbot/README.md
================================================
# News Chatbot

A simple AI-powered chatbot that leverages Gemini's real-time search capabilities in a voice AI application.

This example demonstrates Gemini's ability to query Google search in real time and return relevant responses, including links to the URLs that Gemini searched.

All the details about grounding with Google Search can be found [here](https://ai.google.dev/gemini-api/docs/grounding?lang=python).

## Quick Start

### First, start the bot server:

1. Navigate to the server directory:
   ```bash
   cd server
   ```
2. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install requirements:
   ```bash
   pip install -r requirements.txt
   ```
4. Copy env.example to .env and configure:
   - Add your API keys
5. Start the server:
   ```bash
   python server.py
   ```

### Next, connect using the client app:

For client-side setup, refer to the [JavaScript Guide](client/javascript/README.md).

## Important Note

Ensure the bot server is running before using any client implementations.

## Requirements

- Python 3.10+
- Node.js 16+ (for JavaScript and React implementations)
- Daily API key
- Gemini API key (for Gemini bot)
- Cartesia API key
- Modern web browser with WebRTC support


================================================
FILE: news-chatbot/client/javascript/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the bot server. See the [server README](../../README).

2. Navigate to the `client/javascript` directory:

```bash
cd client/javascript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: news-chatbot/client/javascript/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
  <div class="container">
    <div class="status-bar">
      <div class="status">
        Status: <span id="connection-status">Disconnected</span>
      </div>
      <div class="controls">
        <button id="connect-btn">Connect</button>
        <button id="disconnect-btn" disabled>Disconnect</button>
      </div>
    </div>

    <div class="main-content">
      <div class="bot-container">
        <div id="search-result-container">
        </div>
        <audio id="bot-audio" autoplay></audio>
      </div>
    </div>

    <div class="debug-panel">
      <h3>Debug Info</h3>
      <div id="debug-log"></div>
    </div>
  </div>

  <script type="module" src="/src/app.js"></script>
  <link rel="stylesheet" href="/src/style.css">
</body>

</html>


================================================
FILE: news-chatbot/client/javascript/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/daily-transport": "^1.2.0"
  }
}



================================================
FILE: news-chatbot/client/javascript/vite.config.js
================================================
import { defineConfig } from 'vite';

export default defineConfig({
    server: {
        proxy: {
            // Proxy /api requests to the backend server
            '/connect': {
                target: 'http://0.0.0.0:7860', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: news-chatbot/client/javascript/src/app.js
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

/**
 * Pipecat Client Implementation
 *
 * This client connects to an RTVI-compatible bot server using WebRTC (via Daily).
 * It handles audio/video streaming and manages the connection lifecycle.
 *
 * Requirements:
 * - A running RTVI bot server (defaults to http://localhost:7860)
 * - The server must implement the /connect endpoint that returns Daily.co room credentials
 * - Browser with WebRTC support
 */

import { LogLevel, PipecatClient, RTVIEvent } from '@pipecat-ai/client-js';
import { DailyTransport } from '@pipecat-ai/daily-transport';

/**
 * ChatbotClient handles the connection and media management for a real-time
 * voice and video interaction with an AI bot.
 */
class ChatbotClient {
  constructor() {
    // Initialize client state
    this.pcClient = null;
    this.setupDOMElements();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  setupDOMElements() {
    // Get references to UI control elements
    this.connectBtn = document.getElementById('connect-btn');
    this.disconnectBtn = document.getElementById('disconnect-btn');
    this.statusSpan = document.getElementById('connection-status');
    this.debugLog = document.getElementById('debug-log');
    this.searchResultContainer = document.getElementById(
      'search-result-container'
    );

    // Create an audio element for bot's voice output
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    this.botAudio.playsInline = true;
    document.body.appendChild(this.botAudio);
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  setupEventListeners() {
    this.connectBtn.addEventListener('click', () => this.connect());
    this.disconnectBtn.addEventListener('click', () => this.disconnect());
  }

  /**
   * Add a timestamped message to the debug log
   */
  log(message) {
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;

    // Add styling based on message type
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3'; // blue for user
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50'; // green for bot
    }

    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  updateStatus(status) {
    this.statusSpan.textContent = status;
    this.log(`Status: ${status}`);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.pcClient) return;

    // Get current tracks from the client
    const tracks = this.pcClient.tracks();

    // Set up any available bot tracks
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.pcClient) return;

    // Listen for new tracks starting
    this.pcClient.on(RTVIEvent.TrackStarted, (track, participant) => {
      // Only handle non-local (bot) tracks
      if (!participant?.local && track.kind === 'audio') {
        this.setupAudioTrack(track);
      }
    });

    // Listen for tracks stopping
    this.pcClient.on(RTVIEvent.TrackStopped, (track, participant) => {
      this.log(
        `Track stopped event: ${track.kind} from ${
          participant?.name || 'unknown'
        }`
      );
    });
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  setupAudioTrack(track) {
    this.log('Setting up audio track');
    // Check if we're already playing this track
    if (this.botAudio.srcObject) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    // Create a new MediaStream with the track and set it as the audio source
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the Pipecat client, initializes devices, and establishes the connection
   */
  async connect() {
    try {
      // Initialize the Pipecat client with a Daily WebRTC transport and our configuration
      this.pcClient = new PipecatClient({
        transport: new DailyTransport(),
        enableMic: true, // Enable microphone for user input
        enableCam: false,
        callbacks: {
          // Handle connection state changes
          onConnected: () => {
            this.updateStatus('Connected');
            this.connectBtn.disabled = true;
            this.disconnectBtn.disabled = false;
            this.log('Client connected');
          },
          onDisconnected: () => {
            this.updateStatus('Disconnected');
            this.connectBtn.disabled = false;
            this.disconnectBtn.disabled = true;
            this.log('Client disconnected');
          },
          // Handle transport state changes
          onTransportStateChanged: (state) => {
            this.updateStatus(`Transport: ${state}`);
            this.log(`Transport state changed: ${state}`);
            if (state === 'ready') {
              this.setupMediaTracks();
            }
          },
          // Handle search response events
          onBotLlmSearchResponse: this.handleSearchResponse.bind(this),
          // Handle bot connection events
          onBotConnected: (participant) => {
            this.log(`Bot connected: ${JSON.stringify(participant)}`);
          },
          onBotDisconnected: (participant) => {
            this.log(`Bot disconnected: ${JSON.stringify(participant)}`);
          },
          onBotReady: (data) => {
            this.log(`Bot ready: ${JSON.stringify(data)}`);
            this.setupMediaTracks();
          },
          // Transcript events
          onUserTranscript: (data) => {
            // Only log final transcripts
            if (data.final) {
              this.log(`User: ${data.text}`);
            }
          },
          onBotTranscript: (data) => {
            this.log(`Bot: ${data.text}`);
          },
          // Error handling
          onMessageError: (error) => {
            console.log('Message error:', error);
          },
          onError: (error) => {
            console.log('Error:', error);
          },
        },
      });

      //this.pcClient.setLogLevel(LogLevel.DEBUG)

      // Set up listeners for media track events
      this.setupTrackListeners();

      // Initialize audio devices
      this.log('Initializing devices...');
      await this.pcClient.initDevices();

      // Connect to the bot
      this.log('Connecting to bot...');
      await this.pcClient.startBotAndConnect({
        // The baseURL and endpoint of your bot server that the client will connect to
        endpoint: 'http://localhost:7860/connect',
      });

      this.log('Connection complete');
    } catch (error) {
      // Handle any errors during connection
      this.log(`Error connecting: ${error.message}`);
      this.log(`Error stack: ${error.stack}`);
      this.updateStatus('Error');

      // Clean up if there's an error
      if (this.pcClient) {
        try {
          await this.pcClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError.message}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  async disconnect() {
    if (this.pcClient) {
      try {
        // Disconnect the Pipecat client
        await this.pcClient.disconnect();
        this.pcClient = null;

        // Clean up audio
        if (this.botAudio.srcObject) {
          this.botAudio.srcObject.getTracks().forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }

        // Clean up video
        this.searchResultContainer.innerHTML = '';
      } catch (error) {
        this.log(`Error disconnecting: ${error.message}`);
      }
    }
  }

  handleSearchResponse(response) {
    console.log('SearchResponseHelper, received message:', response);
    // Clear existing content
    this.searchResultContainer.innerHTML = '';

    // Create a container for all content
    const contentContainer = document.createElement('div');
    contentContainer.className = 'content-container';

    // Add the search_result
    if (response.search_result) {
      const searchResultDiv = document.createElement('div');
      searchResultDiv.className = 'search-result';
      searchResultDiv.textContent = response.search_result;
      contentContainer.appendChild(searchResultDiv);
    }

    // Add the sources
    if (response.origins) {
      const sourcesDiv = document.createElement('div');
      sourcesDiv.className = 'sources';

      const sourcesTitle = document.createElement('h3');
      sourcesTitle.className = 'sources-title';
      sourcesTitle.textContent = 'Sources:';
      sourcesDiv.appendChild(sourcesTitle);

      response.origins.forEach((origin) => {
        const sourceLink = document.createElement('a');
        sourceLink.className = 'source-link';
        sourceLink.href = origin.site_uri;
        sourceLink.target = '_blank';
        sourceLink.textContent = origin.site_title;
        sourcesDiv.appendChild(sourceLink);
      });

      contentContainer.appendChild(sourcesDiv);
    }

    // Add the rendered_content in an iframe
    if (response.rendered_content) {
      const iframe = document.createElement('iframe');
      iframe.className = 'iframe-container';
      iframe.srcdoc = response.rendered_content;
      contentContainer.appendChild(iframe);
    }

    // Append the content container to the content panel
    this.searchResultContainer.appendChild(contentContainer);
  }
}

// Initialize the client when the page loads
window.addEventListener('DOMContentLoaded', () => {
  new ChatbotClient();
});



================================================
FILE: news-chatbot/client/javascript/src/style.css
================================================
body {
  margin: 0;
  padding: 20px;
  font-family: Arial, sans-serif;
  background-color: #f0f0f0;
}

.container {
  max-width: 1200px;
  margin: 0 auto;
}

.status-bar {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px;
  background-color: #fff;
  border-radius: 8px;
  margin-bottom: 20px;
}

.controls button {
  padding: 8px 16px;
  margin-left: 10px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

#connect-btn {
  background-color: #4caf50;
  color: white;
}

#disconnect-btn {
  background-color: #f44336;
  color: white;
}

button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.main-content {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 20px;
}

.bot-container {
  display: flex;
  flex-direction: column;
  align-items: center;
}

#search-result-container {
  background-color: #e0e0e0;
  padding: 20px;
  width: calc(100% - 40px);
  height: 450px;
  overflow: auto;
}

/* Container for all content */
.content-container {
    display: flex;
    flex-direction: column;
    gap: 20px; /* Space between elements */
    font-family: Arial, sans-serif;
}

/* Styles for the search result */
.search-result {
    font-size: 16px;
    line-height: 1.5;
    color: #333;
}

/* Styles for the sources container */
.sources {
    display: flex;
    flex-direction: column;
    gap: 8px; /* Space between source links */
}

.sources-title {
    font-size: 16px;
    font-weight: bold;
    color: #444;
}

/* Styles for source links */
.source-link {
    text-decoration: none;
    color: #1a73e8;
}

.source-link:hover {
    text-decoration: underline;
}

/* Styles for the iframe container */
.iframe-container {
    flex: none;
    width: 100%;
    height: 400px; /* Adjust height as needed */
    border: none;
}

.debug-panel {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
}

.debug-panel h3 {
  margin: 0 0 10px 0;
  font-size: 16px;
  font-weight: bold;
}

#debug-log {
  height: 200px;
  overflow-y: auto;
  background-color: #f8f8f8;
  padding: 10px;
  border-radius: 4px;
  font-family: monospace;
  font-size: 12px;
  line-height: 1.4;
}



================================================
FILE: news-chatbot/server/README.md
================================================
# News Chatbot Server

A FastAPI server that manages bot instances and provide endpoint for Pipecat client connections.

## Endpoints

- `POST /connect` - Pipecat client connection endpoint

## Environment Variables

Copy `env.example` to `.env` and configure:

```ini
# Required API Keys
DAILY_API_KEY=           # Your Daily API key
DEEPGRAM_API_KEY=        # Your Deepgram API key
GOOGLE_API_KEY=          # Your Google/Gemini API key
CARTESIA_API_KEY=        # Your Cartesia API key

# Optional Configuration
DAILY_API_URL=           # Optional: Daily API URL (defaults to https://api.daily.co/v1)
DAILY_SAMPLE_ROOM_URL=   # Optional: Fixed room URL for development
HOST=                    # Optional: Host address (defaults to 0.0.0.0)
FAST_API_PORT=           # Optional: Port number (defaults to 7860)
```

## Running the Server

Set up and activate your virtual environment:

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Install dependencies:

```bash
pip install -r requirements.txt
```

If you want to use the local version of `pipecat` in this repo rather than the last published version, also run:

```bash
pip install --editable "../../../[daily,deepgram,google,cartesia,openai,silero]"
```

Run the server:

```bash
python server.py
```



================================================
FILE: news-chatbot/server/env.example
================================================
DAILY_SAMPLE_ROOM_URL=https://yourdomain.daily.co/yourroom # (for joining the bot to the same room repeatedly for local dev)
DAILY_API_KEY=
CARTESIA_API_KEY=
DEEPGRAM_API_KEY=
GOOGLE_API_KEY=


================================================
FILE: news-chatbot/server/news_bot.py
================================================
#
# Copyright (c) 2024-2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import asyncio
import os
import sys
from pathlib import Path

import aiohttp
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import Frame, LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIProcessor
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.google.llm import GoogleLLMService, LLMSearchResponseFrame
from pipecat.services.google.rtvi import GoogleRTVIObserver
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from pipecat.utils.text.markdown_text_filter import MarkdownTextFilter

sys.path.append(str(Path(__file__).parent.parent))
from runner import configure

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


# Function handlers for the LLM
# https://ai.google.dev/gemini-api/docs/grounding?lang=python#dynamic-retrieval
# Some queries are likely to benefit more from Grounding with Google Search than others.
# The dynamic retrieval feature gives you additional control over when to use Grounding with Google Search.
# If the dynamic retrieval mode is unspecified, Grounding with Google Search is always triggered.
# If the mode is set to dynamic, the model decides when to use grounding based on a threshold that you can configure.
# The threshold is a floating-point value in the range [0,1] and defaults to 0.3.
# If the threshold value is 0, the response is always grounded with Google Search; if it's 1, it never is.
search_tool = {
    "google_search_retrieval": {
        "dynamic_retrieval_config": {
            "mode": "MODE_DYNAMIC",
            "dynamic_threshold": 0,
        }  # always grounding
    }
}
tools = [search_tool]

system_instruction = """
You are an expert at providing the most recent news from any place. Your responses will be converted to audio, so ensure they are formatted in plain text without special characters (e.g., *, _, -) or overly complex formatting.

Guidelines:
- Use the Google search API to retrieve the current date and provide the latest news.
- Always deliver accurate and concise responses.
- Ensure all responses are clear, using plain text only. Avoid any special characters or symbols.

Start every interaction by asking how you can assist the user.
"""


class LLMSearchLoggerProcessor(FrameProcessor):
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, LLMSearchResponseFrame):
            print(f"LLMSearchLoggerProcessor: {frame}")

        await self.push_frame(frame)


async def main():
    async with aiohttp.ClientSession() as session:
        (room_url, token) = await configure(session)

        transport = DailyTransport(
            room_url,
            token,
            "Latest news!",
            DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

        stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

        tts = CartesiaTTSService(
            api_key=os.getenv("CARTESIA_API_KEY"),
            voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
            text_filters=[MarkdownTextFilter()],
        )

        llm = GoogleLLMService(
            api_key=os.getenv("GOOGLE_API_KEY"),
            system_instruction=system_instruction,
            tools=tools,
            model="gemini-1.5-flash",
        )

        context = LLMContext(
            [
                {
                    "role": "user",
                    "content": "Start by greeting the user warmly, introducing yourself, and mentioning the current day. Be friendly and engaging to set a positive tone for the interaction.",
                }
            ],
        )
        context_aggregator = LLMContextAggregatorPair(context)

        llm_search_logger = LLMSearchLoggerProcessor()

        #
        # RTVI events for Pipecat client UI
        #
        rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

        pipeline = Pipeline(
            [
                transport.input(),
                stt,
                rtvi,
                context_aggregator.user(),
                llm,
                llm_search_logger,
                tts,
                transport.output(),
                context_aggregator.assistant(),
            ]
        )

        task = PipelineTask(
            pipeline,
            params=PipelineParams(
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
            observers=[GoogleRTVIObserver(rtvi)],
        )

        @rtvi.event_handler("on_client_ready")
        async def on_client_ready(rtvi):
            await rtvi.set_bot_ready()
            # Kick off the conversation
            await task.queue_frames([LLMRunFrame()])

        @transport.event_handler("on_first_participant_joined")
        async def on_first_participant_joined(transport, participant):
            logger.debug("First participant joined: {}", participant["id"])

        @transport.event_handler("on_participant_left")
        async def on_participant_left(transport, participant, reason):
            print(f"Participant left: {participant}")
            await task.cancel()

        runner = PipelineRunner()
        await runner.run(task)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: news-chatbot/server/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
pipecat-ai[daily,google,deepgram,cartesia,silero,openai]>=0.0.82



================================================
FILE: news-chatbot/server/runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os
from typing import Optional

import aiohttp
from pipecat.transports.daily.utils import DailyRESTHelper


async def configure(aiohttp_session: aiohttp.ClientSession):
    (url, token, _) = await configure_with_args(aiohttp_session)
    return (url, token)


async def configure_with_args(
    aiohttp_session: aiohttp.ClientSession, parser: Optional[argparse.ArgumentParser] = None
):
    if not parser:
        parser = argparse.ArgumentParser(description="Daily AI SDK Bot Sample")
    parser.add_argument(
        "-u", "--url", type=str, required=False, help="URL of the Daily room to join"
    )
    parser.add_argument(
        "-k",
        "--apikey",
        type=str,
        required=False,
        help="Daily API Key (needed to create an owner token for the room)",
    )

    args, unknown = parser.parse_known_args()

    url = args.url or os.getenv("DAILY_SAMPLE_ROOM_URL")
    key = args.apikey or os.getenv("DAILY_API_KEY")

    if not url:
        raise Exception(
            "No Daily room specified. use the -u/--url option from the command line, or set DAILY_SAMPLE_ROOM_URL in your environment to specify a Daily room URL."
        )

    if not key:
        raise Exception(
            "No Daily API key specified. use the -k/--apikey option from the command line, or set DAILY_API_KEY in your environment to specify a Daily API key, available from https://dashboard.daily.co/developers."
        )

    daily_rest_helper = DailyRESTHelper(
        daily_api_key=key,
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )

    # Create a meeting token for the given room with an expiration 1 hour in
    # the future.
    expiry_time: float = 60 * 60

    token = await daily_rest_helper.get_token(url, expiry_time)

    return (url, token, args)



================================================
FILE: news-chatbot/server/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os
import subprocess
from contextlib import asynccontextmanager
from typing import Any, Dict

import aiohttp
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pipecat.transports.daily.utils import DailyRESTHelper, DailyRoomParams

# Load environment variables from .env file
load_dotenv(override=True)

# Dictionary to track bot processes: {pid: (process, room_url)}
bot_procs = {}

# Store Daily API helpers
daily_helpers = {}


def cleanup():
    """Cleanup function to terminate all bot processes.

    Called during server shutdown.
    """
    for entry in bot_procs.values():
        proc = entry[0]
        proc.terminate()
        proc.wait()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """FastAPI lifespan manager that handles startup and shutdown tasks.

    - Creates aiohttp session
    - Initializes Daily API helper
    - Cleans up resources on shutdown
    """
    aiohttp_session = aiohttp.ClientSession()
    daily_helpers["rest"] = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )
    yield
    await aiohttp_session.close()
    cleanup()


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


async def create_room_and_token() -> tuple[str, str]:
    """Helper function to create a Daily room and generate an access token.

    Returns:
        tuple[str, str]: A tuple containing (room_url, token)

    Raises:
        HTTPException: If room creation or token generation fails
    """
    room = await daily_helpers["rest"].create_room(DailyRoomParams())
    if not room.url:
        raise HTTPException(status_code=500, detail="Failed to create room")

    token = await daily_helpers["rest"].get_token(room.url)
    if not token:
        raise HTTPException(status_code=500, detail=f"Failed to get token for room: {room.url}")

    return room.url, token


@app.post("/connect")
async def bot_connect(request: Request) -> Dict[Any, Any]:
    """Connect endpoint that creates a room and returns connection credentials.

    This endpoint is called by client to establish a connection.

    Returns:
        Dict[Any, Any]: Authentication bundle containing room_url and token

    Raises:
        HTTPException: If room creation, token generation, or bot startup fails
    """
    print("Creating room for RTVI connection")
    room_url, token = await create_room_and_token()
    print(f"Room URL: {room_url}")

    # Start the bot process
    try:
        bot_file = "news_bot"
        proc = subprocess.Popen(
            [f"python3 -m {bot_file} -u {room_url} -t {token}"],
            shell=True,
            bufsize=1,
            cwd=os.path.dirname(os.path.abspath(__file__)),
        )
        bot_procs[proc.pid] = (proc, room_url)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    # Return the authentication bundle in format expected by DailyTransport
    return {"room_url": room_url, "token": token}


if __name__ == "__main__":
    import uvicorn

    # Parse command line arguments for server configuration
    default_host = os.getenv("HOST", "0.0.0.0")
    default_port = int(os.getenv("FAST_API_PORT", "7860"))

    parser = argparse.ArgumentParser(description="Daily Travel Companion FastAPI server")
    parser.add_argument("--host", type=str, default=default_host, help="Host address")
    parser.add_argument("--port", type=int, default=default_port, help="Port number")
    parser.add_argument("--reload", action="store_true", help="Reload code on change")

    config = parser.parse_args()

    # Start the FastAPI server
    uvicorn.run(
        "server:app",
        host=config.host,
        port=config.port,
        reload=config.reload,
    )



================================================
FILE: open-telemetry/README.md
================================================
# OpenTelemetry Tracing with Pipecat

This repository demonstrates OpenTelemetry tracing integration for Pipecat services, with examples for different backends.

## Tracing Features in Pipecat

- **Hierarchical Tracing**: Track entire conversations, turns, and service calls
- **Service Tracing**: Detailed spans for TTS, STT, and LLM services with rich context
- **TTFB Metrics**: Capture Time To First Byte metrics for latency analysis
- **Usage Statistics**: Track character counts for TTS and token usage for LLMs

## Trace Structure

Traces are organized hierarchically:

```
Conversation (conversation)
├── turn
│   ├── stt_deepgramsttservice
│   ├── llm_openaillmservice
│   └── tts_cartesiattsservice
└── turn
    ├── stt_deepgramsttservice
    ├── llm_openaillmservice
    └── tts_cartesiattsservice
    turn
    └── ...
```

This organization helps you track conversation-to-conversation and turn-to-turn interactions.

## Available Demos

| Demo                            | Description                                                               |
| ------------------------------- | ------------------------------------------------------------------------- |
| [Jaeger Tracing](./jaeger/)     | Tracing with Jaeger, an open-source end-to-end distributed tracing system |
| [Langfuse Tracing](./langfuse/) | Tracing with Langfuse, a specialized platform for LLM observability       |

## Common Requirements

- Python 3.10+
- Pipecat and its dependencies
- API keys for the services used (Deepgram, Cartesia, OpenAI)
- The appropriate OpenTelemetry exporters

## How Tracing Works

The tracing system consists of:

1. **TurnTrackingObserver**: Detects conversation turns
2. **TurnTraceObserver**: Creates spans for turns and conversations
3. **Service Decorators**: `@traced_tts`, `@traced_stt`, `@traced_llm` for service-specific tracing
4. **Context Providers**: Share context between different parts of the pipeline

## Getting Started

1. Choose one of the demos from the table above
2. Follow the README instructions in the respective directory

## Common Troubleshooting

- **Debugging Traces**: Set `OTEL_CONSOLE_EXPORT=true` to print traces to the console for debugging
- **Missing Metrics**: Check that `enable_metrics=True` in PipelineParams
- **API Key Issues**: Verify your API keys are set correctly in the .env file

## References

- [OpenTelemetry Python Documentation](https://opentelemetry-python.readthedocs.io/)
- [Pipecat Documentation](https://docs.pipecat.ai/server/utilities/opentelemetry)



================================================
FILE: open-telemetry/jaeger/README.md
================================================
# Jaeger Tracing for Pipecat

This demo showcases OpenTelemetry tracing integration for Pipecat services using Jaeger, allowing you to visualize service calls, performance metrics, and dependencies.

## Setup Instructions

### 1. Start the Jaeger Container

Run Jaeger in Docker to collect and visualize traces:

```bash
docker run -d --name jaeger \
  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \
  -p 16686:16686 \
  -p 4317:4317 \
  -p 4318:4318 \
  jaegertracing/all-in-one:latest
```

### 2. Environment Configuration

Create a `.env` file with your API keys and enable tracing:

```
ENABLE_TRACING=true
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317  # Point to your Jaeger backend
# OTEL_CONSOLE_EXPORT=true  # Set to any value for debug output to console

# Service API keys
DEEPGRAM_API_KEY=your_key_here
CARTESIA_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here
```

### 3. Setup venv and install Dependencies

```bash
uv sync
```

> Install only the grpc exporter. If you have a conflict, uninstall the http exporter.

### 4. Run the Demo

```bash
uv run bot.py
```

### 5. View Traces in Jaeger

Open your browser to [http://localhost:16686](http://localhost:16686) and select the "pipecat-demo" service to view traces.

## Jaeger-Specific Configuration

In the `bot.py` file, note the GRPC exporter configuration:

```python
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter

# Create the exporter
otlp_exporter = OTLPSpanExporter(
    endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
    insecure=True,
)

# Set up tracing with the exporter
setup_tracing(
    service_name="pipecat-demo",
    exporter=otlp_exporter,
    console_export=bool(os.getenv("OTEL_CONSOLE_EXPORT")),
)
```

## Troubleshooting

- **No Traces in Jaeger**: Ensure the Docker container is running and the OTLP endpoint is correct
- **Connection Errors**: Verify network connectivity to the Jaeger container
- **Exporter Issues**: Try the Console exporter (`OTEL_CONSOLE_EXPORT=true`) to verify tracing works

## References

- [Jaeger Documentation](https://www.jaegertracing.io/docs/latest/)



================================================
FILE: open-telemetry/jaeger/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame, TTSSpeakFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams
from pipecat.utils.tracing.setup import setup_tracing

load_dotenv(override=True)

IS_TRACING_ENABLED = bool(os.getenv("ENABLE_TRACING"))

# Initialize tracing if enabled
if IS_TRACING_ENABLED:
    # Create the exporter
    otlp_exporter = OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
        insecure=True,
    )

    # Set up tracing with the exporter
    setup_tracing(
        service_name="pipecat-demo",
        exporter=otlp_exporter,
        console_export=bool(os.getenv("OTEL_CONSOLE_EXPORT")),
    )
    logger.info("OpenTelemetry tracing initialized")


async def fetch_weather_from_api(params: FunctionCallParams):
    await params.result_callback({"conditions": "nice", "temperature": "75"})


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_bot(transport: BaseTransport):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(
        api_key=os.getenv("OPENAI_API_KEY"), params=OpenAILLMService.InputParams(temperature=0.5)
    )

    # You can also register a function_name of None to get all functions
    # sent to the same callback with an additional function_name parameter.
    llm.register_function("get_current_weather", fetch_weather_from_api)

    @llm.event_handler("on_function_calls_started")
    async def on_function_calls_started(service, function_calls):
        await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

    weather_function = FunctionSchema(
        name="get_current_weather",
        description="Get the current weather",
        properties={
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
            "format": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "The temperature unit to use. Infer this from the user's location.",
            },
        },
        required=["location", "format"],
    )
    tools = ToolsSchema(standard_tools=[weather_function])

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        enable_tracing=IS_TRACING_ENABLED,
        # Optionally, add a conversation ID to track the conversation
        # conversation_id="8df26cc1-6db0-4a7a-9930-1e037c8f1fa2",
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: open-telemetry/jaeger/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: open-telemetry/jaeger/env.example
================================================
DEEPGRAM_API_KEY=your_deepgram_key
CARTESIA_API_KEY=your_cartesia_key
OPENAI_API_KEY=your_openai_key

# Set to any value to enable tracing
ENABLE_TRACING=true
# OTLP endpoint (defaults to localhost:4317 if not set)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
# Set to any value to enable console output for debugging
# OTEL_CONSOLE_EXPORT=true


================================================
FILE: open-telemetry/jaeger/pcc-deploy.toml
================================================
agent_name = "jaeger-tracing"
image = "your_username/jaeger-tracing:0.1"
secret_set = "jaeger-tracing-secrets"

[scaling]
	min_agents = 1



================================================
FILE: open-telemetry/jaeger/pyproject.toml
================================================
[project]
name = "pipecat-jaeger-tracing"
version = "0.1.0"
description = "A Pipecat example using Jaeger tracing"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,webrtc,websocket,silero,cartesia,deepgram,openai,tracing,runner]>=0.0.82",
    "pipecatcloud>=0.2.5",
    "opentelemetry-exporter-otlp-proto-grpc",
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: open-telemetry/langfuse/README.md
================================================
# Langfuse Tracing for Pipecat

This demo showcases [Langfuse](https://langfuse.com) tracing integration for Pipecat services via OpenTelemetry, allowing you to visualize service calls, performance metrics, and dependencies with a focus on LLM observability.

Pipecat trace in Langfuse:

https://github.com/user-attachments/assets/13dd7431-bf5e-42e3-8d6d-2ed84c51195d

## Setup Instructions

### 1. Create a Langfuse Project and get API keys

[Self-host](https://langfuse.com/self-hosting) Langfuse or create a free [Langfuse Cloud](https://cloud.langfuse.com) account.
Create a new project and get the API keys.

### 2. Environment Configuration

Base64 encode your Langfuse public and secret key:

```bash
echo -n "pk-lf-1234567890:sk-lf-1234567890" | base64
```

Create a `.env` file with your API keys to enable tracing:

```
ENABLE_TRACING=true
# OTLP endpoint for Langfuse
OTEL_EXPORTER_OTLP_ENDPOINT=https://cloud.langfuse.com/api/public/otel
OTEL_EXPORTER_OTLP_HEADERS=Authorization=Basic%20<base64_encoded_api_key>
# Set to any value to enable console output for debugging
# OTEL_CONSOLE_EXPORT=true

# Service API keys
DEEPGRAM_API_KEY=your_key_here
CARTESIA_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here
```

### 3. Set up a venv and install Dependencies

```bash
uv sync
```

> Install only the http exporter. If you have a conflict, uninstall the grpc exporter.

### 4. Run the Demo

```bash
uv run bot.py
```

### 5. View Traces in Langfuse

Open your browser to [https://cloud.langfuse.com](https://cloud.langfuse.com) to view traces.

## Langfuse-Specific Configuration

In the `bot.py` file, note the HTTP exporter configuration:

```python
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter

# Create the exporter - configured from environment variables
otlp_exporter = OTLPSpanExporter()

# Set up tracing with the exporter
setup_tracing(
    service_name="pipecat-demo",
    exporter=otlp_exporter,
    console_export=bool(os.getenv("OTEL_CONSOLE_EXPORT")),
)
```

## Troubleshooting

- **No Traces in Langfuse**: Ensure that your credentials are correct and follow this [troubleshooting guide](https://langfuse.com/faq/all/missing-traces)
- **Connection Errors**: Verify network connectivity to Langfuse
- **Authorization Issues**: Check that your base64 encoding is correct and the API keys are valid

## References

- [Langfuse OpenTelemetry Documentation](https://langfuse.com/docs/opentelemetry/get-started)



================================================
FILE: open-telemetry/langfuse/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame, TTSSpeakFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams
from pipecat.utils.tracing.setup import setup_tracing

load_dotenv(override=True)

IS_TRACING_ENABLED = bool(os.getenv("ENABLE_TRACING"))

# Initialize tracing if enabled
if IS_TRACING_ENABLED:
    # Create the exporter
    otlp_exporter = OTLPSpanExporter()

    # Set up tracing with the exporter
    setup_tracing(
        service_name="pipecat-demo",
        exporter=otlp_exporter,
        console_export=bool(os.getenv("OTEL_CONSOLE_EXPORT")),
    )
    logger.info("OpenTelemetry tracing initialized")


async def fetch_weather_from_api(params: FunctionCallParams):
    await params.result_callback({"conditions": "nice", "temperature": "75"})


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_bot(transport: BaseTransport):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(
        api_key=os.getenv("OPENAI_API_KEY"), params=OpenAILLMService.InputParams(temperature=0.5)
    )

    # You can also register a function_name of None to get all functions
    # sent to the same callback with an additional function_name parameter.
    llm.register_function("get_current_weather", fetch_weather_from_api)

    @llm.event_handler("on_function_calls_started")
    async def on_function_calls_started(service, function_calls):
        await tts.queue_frame(TTSSpeakFrame("Let me check on that."))

    weather_function = FunctionSchema(
        name="get_current_weather",
        description="Get the current weather",
        properties={
            "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA",
            },
            "format": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "The temperature unit to use. Infer this from the user's location.",
            },
        },
        required=["location", "format"],
    )
    tools = ToolsSchema(standard_tools=[weather_function])

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        enable_tracing=IS_TRACING_ENABLED,
        # Optionally, add a conversation ID to track the conversation
        # conversation_id="8df26cc1-6db0-4a7a-9930-1e037c8f1fa2",
        # Optionally, add a Langfuse session ID to the span attributes
        # additional_span_attributes={"langfuse.session.id": "8df26cc1-6db0-4a7a-9930-1e037c8f1fa2"},
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: open-telemetry/langfuse/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: open-telemetry/langfuse/env.example
================================================
DEEPGRAM_API_KEY=your_deepgram_key
CARTESIA_API_KEY=your_cartesia_key
OPENAI_API_KEY=your_openai_key

# Set to any value to enable tracing
ENABLE_TRACING=true

# 🇪🇺 EU data region
OTEL_EXPORTER_OTLP_ENDPOINT="https://cloud.langfuse.com/api/public/otel"
# 🇺🇸 US data region
# OTEL_EXPORTER_OTLP_ENDPOINT="https://us.cloud.langfuse.com/api/public/otel"
# 🏠 Local deployment (>= v3.22.0)
# OTEL_EXPORTER_OTLP_ENDPOINT="http://localhost:3000/api/public/otel"

OTEL_EXPORTER_OTLP_HEADERS="Authorization=Basic <base64_encoded_api_keys>"

# Set to any value to enable console output for debugging
# OTEL_CONSOLE_EXPORT=true


================================================
FILE: open-telemetry/langfuse/pcc-deploy.toml
================================================
agent_name = "langfuse-tracing"
image = "your_username/langfuse-tracing:0.1"
secret_set = "langfuse-tracing-secrets"

[scaling]
	min_agents = 1



================================================
FILE: open-telemetry/langfuse/pyproject.toml
================================================
[project]
name = "pipecat-langfuse-tracing"
version = "0.1.0"
description = "A Pipecat example using Langfuse tracing"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,webrtc,websocket,silero,cartesia,deepgram,openai,tracing,runner]>=0.0.82",
    "pipecatcloud>=0.2.5",
    "opentelemetry-exporter-otlp-proto-http",
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: p2p-webrtc/daily-interop-bridge/README.md
================================================
# SmallWebRTC and Daily

A Pipecat example demonstrating how to interoperate audio and video between `SmallWebRTCTransport` and `DailyTransport`.

## 🚀 Quick Start

### 1️⃣ Start the Bot Server

#### 🔧 Set Up the Environment
1. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure environment variables:
   - Copy `env.example` to `.env`
   ```bash
   cp env.example .env
   ```
   - Add your API keys

#### ▶️ Run the Server
```bash
python server.py
```

###  1️⃣ Connect the first client using Daily Prebuilt

- Open your browser and navigate to the same URL that you configured inside your `.env` file:
  - `DAILY_SAMPLE_ROOM_URL`

### 2️⃣ Connect the second client using SmallWebRTC Prebuilt UI

- Open your browser and navigate to:
👉 http://localhost:7860
  - (Or use your custom port, if configured)

## ⚠️ Important Note
Ensure the bot server is running before using any client implementations.

## 📌 Requirements

- Python **3.10+**
- Node.js **16+** (for JavaScript components)
- Google API Key
- Modern web browser with WebRTC support

---

### 💡 Notes
- Ensure all dependencies are installed before running the server.
- Check the `.env` file for missing configurations.
- WebRTC requires a secure environment (HTTPS) for full functionality in production.

Happy coding! 🎉


================================================
FILE: p2p-webrtc/daily-interop-bridge/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.frames.frames import (
    InputAudioRawFrame,
    InputImageRawFrame,
    OutputAudioRawFrame,
    OutputImageRawFrame,
)
from pipecat.pipeline.parallel_pipeline import ParallelPipeline
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.frame_processor import Frame, FrameDirection, FrameProcessor
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)


class MirrorProcessor(FrameProcessor):
    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, InputAudioRawFrame):
            await self.push_frame(
                OutputAudioRawFrame(
                    audio=frame.audio,
                    sample_rate=frame.sample_rate,
                    num_channels=frame.num_channels,
                )
            )
        elif isinstance(frame, InputImageRawFrame):
            await self.push_frame(
                OutputImageRawFrame(image=frame.image, size=frame.size, format=frame.format)
            )
        else:
            await self.push_frame(frame, direction)


async def run_bot(webrtc_connection):
    pipecat_transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            video_in_enabled=True,
            video_out_enabled=True,
            video_out_is_live=True,
            video_out_width=1280,
            video_out_height=720,
        ),
    )

    room_url = os.getenv("DAILY_SAMPLE_ROOM_URL", "")
    daily_transport = DailyTransport(
        room_url,
        None,
        "SmallWebRTC",
        params=DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            video_in_enabled=True,
            video_out_enabled=True,
            video_out_is_live=True,
            video_out_width=1280,
            video_out_height=720,
        ),
    )

    pipeline = Pipeline(
        [
            ParallelPipeline(
                [
                    daily_transport.input(),
                    MirrorProcessor(),
                    pipecat_transport.output(),
                ],
                [
                    pipecat_transport.input(),
                    MirrorProcessor(),
                    daily_transport.output(),
                ],
            )
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @daily_transport.event_handler("on_participant_joined")
    async def on_participant_joined(transport, participant):
        await transport.capture_participant_video(participant["id"])

    @pipecat_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")

    @pipecat_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)



================================================
FILE: p2p-webrtc/daily-interop-bridge/env.example
================================================
DAILY_API_KEY=
DAILY_SAMPLE_ROOM_URL=


================================================
FILE: p2p-webrtc/daily-interop-bridge/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
aiortc
pipecat-ai[silero, webrtc, daily]


================================================
FILE: p2p-webrtc/daily-interop-bridge/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import asyncio
import sys
from contextlib import asynccontextmanager
from typing import Dict

import uvicorn
from bot import run_bot
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI
from fastapi.responses import RedirectResponse
from loguru import logger
from pipecat.transports.smallwebrtc.connection import IceServer, SmallWebRTCConnection
from pipecat_ai_small_webrtc_prebuilt.frontend import SmallWebRTCPrebuiltUI

# Load environment variables
load_dotenv(override=True)

app = FastAPI()

# Store connections by pc_id
pcs_map: Dict[str, SmallWebRTCConnection] = {}

ice_servers = [
    IceServer(
        urls="stun:stun.l.google.com:19302",
    )
]

# Mount the frontend at /
app.mount("/prebuilt", SmallWebRTCPrebuiltUI)


@app.get("/", include_in_schema=False)
async def root_redirect():
    return RedirectResponse(url="/prebuilt/")


@app.post("/api/offer")
async def offer(request: dict, background_tasks: BackgroundTasks):
    pc_id = request.get("pc_id")

    if pc_id and pc_id in pcs_map:
        pipecat_connection = pcs_map[pc_id]
        logger.info(f"Reusing existing connection for pc_id: {pc_id}")
        await pipecat_connection.renegotiate(
            sdp=request["sdp"], type=request["type"], restart_pc=request.get("restart_pc", False)
        )
    else:
        pipecat_connection = SmallWebRTCConnection(ice_servers)
        await pipecat_connection.initialize(sdp=request["sdp"], type=request["type"])

        @pipecat_connection.event_handler("closed")
        async def handle_disconnected(webrtc_connection: SmallWebRTCConnection):
            logger.info(f"Discarding peer connection for pc_id: {webrtc_connection.pc_id}")
            pcs_map.pop(webrtc_connection.pc_id, None)

        background_tasks.add_task(run_bot, pipecat_connection)

    answer = pipecat_connection.get_answer()
    # Updating the peer connection inside the map
    pcs_map[answer["pc_id"]] = pipecat_connection

    return answer


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app
    coros = [pc.disconnect() for pc in pcs_map.values()]
    await asyncio.gather(*coros)
    pcs_map.clear()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WebRTC demo")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="TRACE")
    else:
        logger.add(sys.stderr, level="DEBUG")

    uvicorn.run(app, host=args.host, port=args.port)



================================================
FILE: p2p-webrtc/docker/README.md
================================================
# Voice Agent Docker example

A Pipecat example demonstrating the simplest way to create a voice agent using `SmallWebRTCTransport` with Docker.

## 🚀 Quick Start

### 1️⃣ Build the Bot Server

1. **Configure environment variables:**

   ```bash
   cp env.example .env
   ```

   - Then edit `.env` to include your API keys.

2. **Build the Docker container:**

   ```bash
   docker build -t small-webrtc-bot .
   ```

---

### ▶️ Run the Bot Server

#### ✅ **Linux**

On Linux, Docker allows advanced networking options (like exposing UDP port ranges), which are required for direct WebRTC peer-to-peer connections.

```bash
# Run in the background using host networking (preferred for local dev)
docker run -d --network=host --name small-webrtc-bot small-webrtc-bot:latest

# Or, expose individual ports explicitly
docker run --rm \
  --sysctl net.ipv4.ip_local_port_range="40000 40100" \
  -p 7860:7860 \
  -p 40000-40100:40000-40100/udp \
  --name small-webrtc-bot \
  small-webrtc-bot:latest
```
> ⚠️ **Port Range Limitation**
>
> `SmallWebRTCConnection` uses aiortc, which relies on aioice for ICE transport. There is a known limitation ([aiortc/aioice#47](https://github.com/aiortc/aioice/issues/47)) where port control for `gather_candidates` is not fully supported. The Docker configuration above provides a Linux-specific workaround by:
> 1. Limiting the ephemeral port range the container can use (`--sysctl net.ipv4.ip_local_port_range`)
> 2. Explicitly exposing only those allowed ports (`-p 40000-40100:40000-40100/udp`)
>
> This ensures predictable port usage for WebRTC connections on Linux.

#### 🍏 **macOS (and Windows via Docker Desktop)**

Docker Desktop on macOS does **not support `--network=host`**, and UDP port forwarding has known limitations.

```bash
docker run --rm \
  -p 7860:7860 \
  --name small-webrtc-bot \
  small-webrtc-bot:latest
```

> ⚠️ **On macOS, a TURN server is needed**  
> Because we can not configure which UDP ports aiortc it is going to use, direct WebRTC peer-to-peer connections are unlikely to succeed.  
> A TURN server is required to relay media traffic between the client and the bot when NAT traversal fails.

---

### 2️⃣ Connect Using the Client App

Open your browser and go to:

```
http://localhost:7860
```

---

## WebRTC ICE Servers Configuration

When implementing WebRTC in your project, **STUN** (Session Traversal Utilities for NAT) and **TURN** (Traversal Using Relays around NAT) 
servers are usually needed in cases where users are behind routers or firewalls.

In local networks (e.g., testing within the same home or office network), you usually don’t need to configure STUN or TURN servers. 
In such cases, WebRTC can often directly establish peer-to-peer connections without needing to traverse NAT or firewalls.

### What are STUN and TURN Servers?

- **STUN Server**: Helps clients discover their public IP address and port when they're behind a NAT (Network Address Translation) device (like a router). 
This allows WebRTC to attempt direct peer-to-peer communication by providing the public-facing IP and port.
  
- **TURN Server**: Used as a fallback when direct peer-to-peer communication isn't possible due to strict NATs or firewalls blocking connections. 
The TURN server relays media traffic between peers.

### Why are ICE Servers Important?

**ICE (Interactive Connectivity Establishment)** is a framework used by WebRTC to handle network traversal and NAT issues. 
The `iceServers` configuration provides a list of **STUN** and **TURN** servers that WebRTC uses to find the best way to connect two peers. 

### Example Configuration for ICE Servers

Here’s how you can configure a basic `iceServers` object in WebRTC for testing purposes, using Google's public STUN server:

```javascript
const config = {
  iceServers: [
    {
      urls: ["stun:stun.l.google.com:19302"], // Google's public STUN server
    }
  ],
};
```

> For testing purposes, you can either use public **STUN** servers (like Google's) or set up your own **TURN** server. 
If you're running your own TURN server, make sure to include your server URL, username, and credential in the configuration.

> 🧪 For local Linux tests, STUN alone may work.  
> 🍏 On macOS or behind strict NAT/firewall, TURN is **required**.

---

## 📋 Requirements

- Docker (Docker Desktop for macOS/Windows)
- Google API Key (or your own STT/TTS provider)
- Modern WebRTC-compatible browser (e.g., Chrome, Firefox)

---

## 📝 Notes

- Check `.env` for required API credentials.
- On macOS or Windows, don't rely on `--network=host`.
- In production, WebRTC apps **must** run over HTTPS.
- Consider deploying your own TURN server or using a provider like Twilio or coturn.

---

Happy coding! 🎉



================================================
FILE: p2p-webrtc/docker/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)

SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


async def run_bot(webrtc_connection):
    pipecat_transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            audio_out_10ms_chunks=2,
        ),
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_user_audio=True,
        transcribe_model_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
    )

    context = LLMContext(
        [
            {
                "role": "user",
                "content": "Start by greeting the user warmly and introducing yourself.",
            }
        ],
    )
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            pipecat_transport.input(),
            context_aggregator.user(),
            llm,  # LLM
            pipecat_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @pipecat_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @pipecat_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)



================================================
FILE: p2p-webrtc/docker/Dockerfile
================================================
# Use an official Python runtime as a parent image
FROM python:3.12-slim

# Set the working directory in the container
WORKDIR /app

# Install system dependencies required for OpenCV
RUN apt-get update && apt-get install -y \
    libgl1 \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Copy the current directory contents into the container
COPY . .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Expose the desired port
EXPOSE 7860

# Run the application
CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "7860"]



================================================
FILE: p2p-webrtc/docker/env.example
================================================
GOOGLE_API_KEY=


================================================
FILE: p2p-webrtc/docker/index.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebRTC Voice Agent</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; margin-top: 50px; }
        #status { font-size: 20px; margin: 20px; }
        button { padding: 10px 20px; font-size: 16px; }
    </style>
</head>
<body>
    <h1>WebRTC Voice Agent</h1>
    <p id="status">Disconnected</p>
    <button id="connect-btn">Connect</button>
    <audio id="audio-el" autoplay></audio>

    <script>
        const statusEl = document.getElementById("status")
        const buttonEl = document.getElementById("connect-btn")
        const audioEl = document.getElementById("audio-el")

        let connected = false
        let peerConnection = null

        const printSelectedIceCandidate = async (pc) => {
            const stats = await pc.getStats(null)
            stats.forEach(report => {
                if (
                  report.type === "transport" &&
                  report.selectedCandidatePairId
                ) {
                  const candidatePair = stats.get(report.selectedCandidatePairId);
                  const localCandidate = stats.get(candidatePair.localCandidateId);
                  const remoteCandidate = stats.get(candidatePair.remoteCandidateId);

                  console.log("Selected local candidate:", localCandidate);
                  console.log("Selected remote candidate:", remoteCandidate);
                }
            });
        }

        const sendIceCandidate = async (pc, candidate) => {
            await fetch('/api/offer', {
              method: "PATCH",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                pc_id: pc.pc_id,
                candidates:[{
                    candidate: candidate.candidate,
                    sdp_mid: candidate.sdpMid,
                    sdp_mline_index: candidate.sdpMLineIndex
                }]
              })
            });
        };

        const createSmallWebRTCConnection = async (audioTrack) => {
            // TODO: need to configure a TURN server if using inside Docker on Mac.
            const config = {
              iceServers:[
                  {
                    urls:"stun:stun.l.google.com:19302",
                  }
                ]
            };
            const pc = new RTCPeerConnection(config)
            
            // Queue to store ICE candidates until we have received the answer and have a session in progress
            pc.pendingIceCandidates = []
            pc.canSendIceCandidates = false
            
            addPeerConnectionEventListeners(pc)
            pc.ontrack = e => audioEl.srcObject = e.streams[0]
            // SmallWebRTCTransport expects to receive both transceivers
            pc.addTransceiver(audioTrack, { direction: 'sendrecv' })
            pc.addTransceiver('video', { direction: 'sendrecv' })
            await pc.setLocalDescription(await pc.createOffer())
            const offer = pc.localDescription
            const response = await fetch('/api/offer', {
                body: JSON.stringify({ sdp: offer.sdp, type: offer.type}),
                headers: { 'Content-Type': 'application/json' },
                method: 'POST',
            });
            const answer = await response.json()
            pc.pc_id = answer.pc_id
            await pc.setRemoteDescription(answer)
            
            // Now we can send ICE candidates
            pc.canSendIceCandidates = true
            
            // Send any queued ICE candidates
            for (const candidate of pc.pendingIceCandidates) {
                await sendIceCandidate(pc, candidate)
            }
            pc.pendingIceCandidates = []
            
            return pc
        }

        const connect = async () => {
            _onConnecting()
            const audioStream = await navigator.mediaDevices.getUserMedia({audio: true})
            peerConnection= await createSmallWebRTCConnection(audioStream.getAudioTracks()[0])
        }

        const addPeerConnectionEventListeners = (pc) => {
            pc.oniceconnectionstatechange = () => {
                console.log("oniceconnectionstatechange", pc?.iceConnectionState)
            }
            pc.onconnectionstatechange = () => {
                console.log("onconnectionstatechange", pc?.connectionState)
                let connectionState = pc?.connectionState
                if (connectionState === 'connected') {
                    _onConnected()
                } else if (connectionState === 'disconnected') {
                    _onDisconnected()
                }
            }
            pc.onicecandidate = async (event) => {
                if (event.candidate) {
                    console.log("New ICE candidate:", event.candidate);
                    // Check if we can send ICE candidates (we have received the answer with pc_id)
                    if (pc.canSendIceCandidates && pc.pc_id) {
                        // Send immediately
                        await sendIceCandidate(pc, event.candidate)
                    } else {
                        // Queue the candidate until we have pc_id
                        pc.pendingIceCandidates.push(event.candidate)
                    }
                } else {
                    console.log("All ICE candidates have been sent.");
                }
            };
        }

        const _onConnecting = () => {
            statusEl.textContent = "Connecting"
            buttonEl.textContent = "Disconnect"
            connected = true
        }

        const _onConnected = () => {
            printSelectedIceCandidate(peerConnection);
            statusEl.textContent = "Connected"
            buttonEl.textContent = "Disconnect"
            connected = true
        }

        const _onDisconnected = () => {
            statusEl.textContent = "Disconnected"
            buttonEl.textContent = "Connect"
            connected = false
        }

        const disconnect = () => {
            if (!peerConnection) {
                return
            }
            peerConnection.close()
            peerConnection = null
            _onDisconnected()
        }

        buttonEl.addEventListener("click", async () => {
            if (!connected) {
                await connect()
            } else {
                disconnect()
            }
        });
    </script>
</body>
</html>



================================================
FILE: p2p-webrtc/docker/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
aiortc
pipecat-ai[google,silero,webrtc]>=0.0.82


================================================
FILE: p2p-webrtc/docker/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import sys
from contextlib import asynccontextmanager

import uvicorn
from bot import run_bot
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, Request
from fastapi.responses import FileResponse
from loguru import logger
from pipecat.transports.smallwebrtc.request_handler import (
    SmallWebRTCPatchRequest,
    SmallWebRTCRequest,
    SmallWebRTCRequestHandler,
)

# Load environment variables
load_dotenv(override=True)

app = FastAPI()

# Initialize the SmallWebRTC request handler
small_webrtc_handler: SmallWebRTCRequestHandler = SmallWebRTCRequestHandler()


@app.post("/api/offer")
async def offer(request: SmallWebRTCRequest, background_tasks: BackgroundTasks):
    """Handle WebRTC offer requests via SmallWebRTCRequestHandler."""

    # Prepare runner arguments with the callback to run your bot
    async def webrtc_connection_callback(connection):
        background_tasks.add_task(run_bot, connection)

    # Delegate handling to SmallWebRTCRequestHandler
    answer = await small_webrtc_handler.handle_web_request(
        request=request,
        webrtc_connection_callback=webrtc_connection_callback,
    )
    return answer


@app.patch("/api/offer")
async def ice_candidate(request: SmallWebRTCPatchRequest):
    logger.debug(f"Received patch request: {request}")
    await small_webrtc_handler.handle_patch_request(request)
    return {"status": "success"}


@app.get("/")
async def serve_index():
    return FileResponse("index.html")


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app
    await small_webrtc_handler.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WebRTC demo")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="TRACE")
    else:
        logger.add(sys.stderr, level="DEBUG")

    uvicorn.run(app, host=args.host, port=args.port)



================================================
FILE: p2p-webrtc/docker/.dockerignore
================================================
venv


================================================
FILE: p2p-webrtc/pipecat-cloud/README.md
================================================
# Voice Bot Starter

A voice-based conversational agent built with Pipecat using `SmallWebRTCTransport`.

Any Pipecat bot that uses `SmallWebRTCTransport` can connect to WhatsApp voice calling, simply configure WhatsApp as described here:
- 👉 [**Pipecat WhatsApp Business Calling API**](https://docs.pipecat.ai/guides/features/whatsapp)  

---

## Features

- **Real-time voice conversations** powered by:
  - [Deepgram](https://deepgram.com) (Speech-to-Text, STT)
  - [OpenAI](https://openai.com) (Language Model, LLM)
  - [Cartesia](https://cartesia.ai) (Text-to-Speech, TTS)
- **Voice Activity Detection** with [Silero](https://github.com/snakers4/silero-vad)
- **Natural interruptions** – the bot can stop speaking when you talk

---

## Required API Keys

Before running the bot, set these environment variables:

- `OPENAI_API_KEY`
- `DEEPGRAM_API_KEY`
- `CARTESIA_API_KEY`

---

## Setup

1. **Install dependencies** inside a virtual environment:

    ```bash
    uv sync
    ```

2. **Configure environment variables:**

    ```bash
    cp env.example .env
    # Open .env and add your API keys
    ```

---

## Environment Configuration

The bot supports two deployment modes via the `ENV` variable:

### 🖥️ Local Development (`ENV=local`)

- Default mode for testing and iteration on your machine.

### Production (`ENV=production`)

- Use this mode when deploying to **Pipecat Cloud**.

---

## Run Your Bot Locally

Start your bot:

```bash
uv run bot.py
```

Then open **[http://localhost:7860](http://localhost:7860)** and click **Connect** to start talking to your bot.

> The first startup can take up to ~20 seconds as Pipecat downloads required models and dependencies.

By default, this uses the **SmallWebRTC** prebuilt web UI for quick local testing.

---

### Testing with WhatsApp

If you want to test **WhatsApp calling** instead of the web UI, follow these steps:

1. **Expose your local server** using [ngrok](https://ngrok.com/) or a similar tunneling tool:

   ```bash
   ngrok http --domain=YOUR_NGROK_DOMAIN http://localhost:7860
   ```

2. **Copy the generated HTTPS URL** and set your WhatsApp webhook to:

   ```
   https://YOUR_NGROK_DOMAIN/whatsapp
   ```

   > ✅ **Important:** Always include the `/whatsapp` path at the end of your webhook URL.

3. **Configure your webhook** in your WhatsApp Business account, following the Pipecat documentation:  
   👉 [Configure Webhook — Pipecat WhatsApp Guide](https://docs.pipecat.ai/guides/features/whatsapp#2-configure-webhook)

Once configured, you can make a WhatsApp voice call to your **registered business number** — your Pipecat bot will automatically answer!

---

## Deploying to Production

1. Update your production `.env` file with the Pipecat Cloud details:

    ```bash
    # Set to production mode
    ENV=production

    # Keep your existing AI service keys
    ```

2. Follow the official [Pipecat Quickstart Guide](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to **Pipecat Cloud**.

---

## Configure WhatsApp Webhook for Production

Before you can receive WhatsApp voice calls in production, you must **set up your webhook** in your WhatsApp Business account.  

Follow the official Pipecat documentation for step-by-step instructions:  
👉 [Configure Webhook — Pipecat WhatsApp Guide](https://docs.pipecat.ai/guides/features/whatsapp#2-configure-webhook)

When configuring for production, use the following format for your webhook URL:
```
https://api.pipecat.daily.co/v1/public/webhooks/$ORGANIZATION_NAME/$AGENT_NAME/whatsapp
```

> ✅ **Tip:**  
> - Replace `$ORGANIZATION_NAME` and `$AGENT_NAME` with the values from your Pipecat Cloud deployment.  
> - Ensure the URL ends with `/whatsapp` — this path is required for correct webhook routing.

---

## Call Your WhatsApp Bot

Once deployed, simply call your registered **WhatsApp Business number** from the WhatsApp app.  
Your Pipecat bot will automatically answer and start the conversation.

---

## Resources

- [Pipecat Documentation](https://docs.pipecat.ai)
- [Deepgram API](https://developers.deepgram.com)
- [OpenAI API](https://platform.openai.com/docs)
- [Cartesia API](https://cartesia.ai/docs)

---



================================================
FILE: p2p-webrtc/pipecat-cloud/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import NOT_GIVEN, LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    """Run your bot with the provided transport.

    Args:
        transport (BaseTransport): The transport to use for communication.
        runner_args: runner session arguments
    """
    logger.info(f"RunnerArguments custom data: {runner_args.body}")

    # Configure your STT, LLM, and TTS services here
    # Swap out different processors or properties to customize your bot
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"), model="gpt-4o")
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    # Set up the initial context for the conversation
    # You can specified initial system and assistant messages here
    messages = [
        {
            "role": "system",
            "content": "You are Chatbot, a friendly, helpful robot. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by introducing yourself.",
        },
    ]

    # Define and register tools as required
    tools = NOT_GIVEN

    # This sets up the LLM context by providing messages and tools
    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    # A core voice AI pipeline
    # Add additional processors to customize the bot's behavior
    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            allow_interruptions=True,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.debug("Client ready event received")
        await rtvi.set_bot_ready()

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected.")
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, participant):
        logger.info("Client disconnected: {}", participant)
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    logger.info(f"Starting the bot, received body: {runner_args.body}")
    webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection
    try:
        if os.environ.get("ENV") != "local":
            from pipecat.audio.filters.krisp_filter import KrispFilter

            krisp_filter = KrispFilter()
        else:
            krisp_filter = None

        transport = SmallWebRTCTransport(
            webrtc_connection=webrtc_connection,
            params=TransportParams(
                audio_in_enabled=True,
                audio_in_filter=krisp_filter,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

        if transport is None:
            logger.error("Failed to create transport")
            return

        await run_bot(transport, runner_args)
        logger.info("Bot process completed")
    except Exception as e:
        logger.exception(f"Error in bot process: {str(e)}")
        raise


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: p2p-webrtc/pipecat-cloud/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev


COPY ./bot.py bot.py



================================================
FILE: p2p-webrtc/pipecat-cloud/env.example
================================================
DEEPGRAM_API_KEY=your_deepgram_api_key
OPENAI_API_KEY=your_openai_api_key
CARTESIA_API_KEY=your_cartesia_api_key

# Environment configuration
ENV=production

# WhatsApp
WHATSAPP_TOKEN=
WHATSAPP_PHONE_NUMBER_ID=
WHATSAPP_APP_SECRET=
# Only for local development
WHATSAPP_WEBHOOK_VERIFICATION_TOKEN=


================================================
FILE: p2p-webrtc/pipecat-cloud/pcc-deploy.toml
================================================
agent_name = "smallwebrtc-poc"
image = "your_username/pipecat-smallwebrtc:latest"
#For private image pulls.
#More details: https://docs.pipecat.ai/deployment/pipecat-cloud/fundamentals/secrets#image-pull-secrets
image_credentials = "smallwebrtc-pull-secret"
secret_set = "smallwebrtc-secrets"
enable_krisp = true

[scaling]
	min_agents = 1



================================================
FILE: p2p-webrtc/pipecat-cloud/pyproject.toml
================================================
[project]
name = "smallwebrtc-example"
version = "0.1.0"
description = "Example for building voice AI bots with Pipecat built using SmallWebRTC"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[cartesia,daily,deepgram,openai,runner,silero,webrtc]>=0.0.91",
    "pipecatcloud>=0.2.10",
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: p2p-webrtc/pipecat-cloud/client/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the bot server. See the [server README](../../README).

2. Navigate to the `client/typescript` directory:

```bash
cd client/typescript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: p2p-webrtc/pipecat-cloud/client/env.example
================================================
VITE_PIPECAT_BASE_URL=https://api.pipecat.daily.co/v1/public/${AGENT_NAME}
VITE_PIPECAT_PUBLIC_API=your_api_key


================================================
FILE: p2p-webrtc/pipecat-cloud/client/index.html
================================================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>WebRTC demo</title>
  </head>
  <body>
    <div class="container">
      <!-- Status Bar -->
      <div class="status-bar">
        <div class="status">
          Status: <span id="connection-status">Disconnected</span>
        </div>
        <div class="controls">
          <button id="connect-btn">Connect</button>
          <button id="disconnect-btn" disabled>Disconnect</button>
        </div>
      </div>

      <!-- Main Content -->
      <div class="main-content">
        <div class="bot-container">
          <audio id="bot-audio" autoplay></audio>
        </div>
        <!-- Debug Panel -->
        <div class="debug-panel">
          <div id="debug-log"></div>
        </div>
      </div>
    </div>
    <script type="module" src="/src/app.ts"></script>
    <link rel="stylesheet" href="/src/style.css" />
  </body>
</html>



================================================
FILE: p2p-webrtc/pipecat-cloud/client/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "node_modules/.bin/vite",
    "build": "node_modules/.bin/tsc && vite build",
    "preview": "node_modules/.bin/vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.13.1",
    "@vitejs/plugin-react-swc": "^3.7.2",
    "typescript": "^5.7.3",
    "vite": "^6.3.6"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.4.1",
    "@pipecat-ai/small-webrtc-transport": "^1.7.1"
  }
}



================================================
FILE: p2p-webrtc/pipecat-cloud/client/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ESNext",
    "module": "ESNext",
    "moduleResolution": "NodeNext",   // <— important
    "types": ["vite/client"],
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true
  }
}


================================================
FILE: p2p-webrtc/pipecat-cloud/client/vite.config.js
================================================
/* jshint esversion: 6 */
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

export default defineConfig({
    plugins: [react()],
    server: {
        allowedHosts: true, // Allows external connections like ngrok
        proxy: {
            // Proxy /api requests to the backend server
            '/v1': {
                target: 'http://0.0.0.0:8000', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: p2p-webrtc/pipecat-cloud/client/src/app.ts
================================================
import { SmallWebRTCTransport } from '@pipecat-ai/small-webrtc-transport';
import {
  APIRequest,
  BotLLMTextData,
  Participant,
  PipecatClient,
  PipecatClientOptions,
  TranscriptData,
  TransportState,
} from '@pipecat-ai/client-js';

class WebRTCApp {
  private declare connectBtn: HTMLButtonElement;
  private declare disconnectBtn: HTMLButtonElement;
  private declare botAudioElement: HTMLAudioElement;

  private debugLog: HTMLElement | null = null;
  private statusSpan: HTMLElement | null = null;

  private declare smallWebRTCTransport: SmallWebRTCTransport;
  private declare pcClient: PipecatClient;

  private declare baseUrl: string;
  private declare startUrl: string;
  private declare apiKey: string;

  constructor() {
    this.setupEnvironmentVariables();
    this.setupDOMElements();
    this.setupDOMEventListeners();
    this.initializePipecatClient();
  }

  private setupEnvironmentVariables() {
    this.baseUrl = import.meta.env.VITE_PIPECAT_BASE_URL
    this.startUrl = `${this.baseUrl}/start`
    this.apiKey = import.meta.env.VITE_PIPECAT_PUBLIC_API;
  }

  private initializePipecatClient(): void {
    const opts: PipecatClientOptions = {
      transport: new SmallWebRTCTransport(),
      enableMic: true,
      enableCam: true,
      callbacks: {
        onTransportStateChanged: (state: TransportState) => {
          this.log(`Transport state: ${state}`);
        },
        onConnected: () => {
          this.onConnectedHandler();
        },
        onBotReady: () => {
          this.log('Bot is ready.');
        },
        onDisconnected: () => {
          this.onDisconnectedHandler();
        },
        onUserStartedSpeaking: () => {
          this.log('User started speaking.');
        },
        onUserStoppedSpeaking: () => {
          this.log('User stopped speaking.');
        },
        onBotStartedSpeaking: () => {
          this.log('Bot started speaking.');
        },
        onBotStoppedSpeaking: () => {
          this.log('Bot stopped speaking.');
        },
        onUserTranscript: (transcript: TranscriptData) => {
          if (transcript.final) {
            this.log(`User transcript: ${transcript.text}`);
          }
        },
        onBotTranscript: (data: BotLLMTextData) => {
          this.log(`Bot transcript: ${data.text}`);
        },
        onTrackStarted: (
          track: MediaStreamTrack,
          participant?: Participant
        ) => {
          if (!participant?.local) {
            this.onBotTrackStarted(track);
          }
        },
        onServerMessage: (msg: unknown) => {
          this.log(`Server message: ${msg}`);
        },
      },
    };
    this.pcClient = new PipecatClient(opts);
    // @ts-ignore
    window.webapp = this;
    // @ts-ignore
    window.client = this.pcClient; // Expose client for debugging
    this.smallWebRTCTransport = this.pcClient.transport as SmallWebRTCTransport;
  }

  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      'connect-btn'
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      'disconnect-btn'
    ) as HTMLButtonElement;
    this.debugLog = document.getElementById('debug-log');
    this.statusSpan = document.getElementById('connection-status');
    this.botAudioElement = document.getElementById('bot-audio') as HTMLAudioElement;
  }

  private setupDOMEventListeners(): void {
    this.connectBtn.addEventListener('click', () => this.start());
    this.disconnectBtn.addEventListener('click', () => this.stop());
  }

  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3';
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50';
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
  }

  private clearAllLogs() {
    this.debugLog!.innerText = '';
  }

  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  private onConnectedHandler() {
    this.updateStatus('Connected');
    if (this.connectBtn) this.connectBtn.disabled = true;
    if (this.disconnectBtn) this.disconnectBtn.disabled = false;
  }

  private onDisconnectedHandler() {
    this.updateStatus('Disconnected');
    if (this.connectBtn) this.connectBtn.disabled = false;
    if (this.disconnectBtn) this.disconnectBtn.disabled = true;
  }

  private onBotTrackStarted(track: MediaStreamTrack) {
    if (track.kind === 'audio') {
      this.botAudioElement.srcObject = new MediaStream([track]);
    }
  }

  private async start(): Promise<void> {
    this.clearAllLogs();
    await this.pcClient.initDevices()
    this.connectBtn.disabled = true;
    try {
      this.updateStatus('Starting the bot');
      const headers = new Headers();
      headers.append("Authorization", `Bearer ${this.apiKey}`);
      await this.pcClient.startBotAndConnect({
          endpoint: this.startUrl,
          headers: headers,
          requestData: {
            createDailyRoom: false,
            enableDefaultIceServers: true,
            transport: "webrtc"
          }
      });
    } catch (e) {
      console.log(`Failed to connect ${e}`);
      this.stop();
    }
  }

  private stop(): void {
    void this.pcClient.disconnect();
  }
}

// Create the WebRTCConnection instance
const webRTCConnection = new WebRTCApp();


================================================
FILE: p2p-webrtc/pipecat-cloud/client/src/style.css
================================================
body {
  margin: 0;
  padding: 20px;
  font-family: Arial, sans-serif;
  background-color: #f0f0f0;
  display: flex;
  flex-direction: row;
  width: 100%;
}

.container {
  margin: 0 auto;
  width: 90%;
}

.option {
  display: flex;
  flex-direction: row;
  align-items: center;
}

label {
  margin: 5px;
}

select {
  padding: 8px;
  margin: 10px;
  border-radius: 4px;
  border: 1px solid #ccc;
}

#mute-mic {
  padding: 8px;
  margin: 10px;
  border-radius: 4px;
  border: 1px solid #ccc;
  background-color: white;
  cursor: pointer;
}

.status-bar {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  align-items: center;
  padding: 10px;
  background-color: #fff;
  border-radius: 8px;
  margin-bottom: 20px;
}

.controls button {
  padding: 8px 16px;
  margin-left: 10px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

#connect-btn {
  background-color: #4caf50;
  color: white;
}

#disconnect-btn {
  background-color: #f44336;
  color: white;
}

button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.main-content {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 20px;
  display: flex;
  flex-wrap: wrap;
}

.bot-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 50%;
}

.debug-panel {
  background-color: #fff;
  border-radius: 8px;
  width: 100%;
}

@media (max-width: 768px) {
  .bot-container {
    width: 100%;
  }
  .debug-panel {
    width: 100%;
  }
}

.debug-panel h3 {
  margin: 0 0 10px 0;
  font-size: 16px;
  font-weight: bold;
}

#debug-log {
  height: 360px;
  overflow-y: auto;
  background-color: #f8f8f8;
  border-radius: 4px;
  font-family: monospace;
  font-size: 12px;
  line-height: 1.4;
}




================================================
FILE: p2p-webrtc/video-transform/README.md
================================================
# Video Transform

A Pipecat example demonstrating how to send and receive audio and video using `SmallWebRTCTransport`. This project also applies image processing to video frames using OpenCV.

## 🚀 Quick Start

### 1️⃣ Start the Bot Server

#### 📂 Navigate to the Server Directory
```bash
cd server
```

#### 🔧 Set Up the Environment
1. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure environment variables:
   - Copy `env.example` to `.env`
   ```bash
   cp env.example .env
   ```
   - Add your API keys

#### ▶️ Run the Server
```bash
python server.py
```

### 2️⃣ Test with SmallWebRTC Prebuilt UI

You can quickly test your bot using the `SmallWebRTCPrebuiltUI`:

- Open your browser and navigate to:
👉 http://localhost:7860
  - (Or use your custom port, if configured)

### 3️⃣ Connect Using a Custom Client App

For client-side setup, refer to the:
- [Typescript Guide](client/typescript/README.md).
- [iOS Guide](client/ios/README.md).

## ⚠️ Important Note
Ensure the bot server is running before using any client implementations.

## 📌 Requirements

- Python **3.10+**
- Node.js **16+** (for JavaScript components)
- Google API Key
- Modern web browser with WebRTC support

---

### 💡 Notes
- Ensure all dependencies are installed before running the server.
- Check the `.env` file for missing configurations.
- WebRTC requires a secure environment (HTTPS) for full functionality in production.

Happy coding! 🎉


================================================
FILE: p2p-webrtc/video-transform/client/android/README.md
================================================
# Pipecat Small WebRTC Client for Android

Demo app which connects to the `video-transform` backend over the small WebRTC transport.

## How to build

```bash
./gradlew installDebug
```

Ensure that the `video-transform` server is running as described in the parent README.




================================================
FILE: p2p-webrtc/video-transform/client/android/build.gradle.kts
================================================
plugins {
    alias(libs.plugins.jetbrains.kotlin.android) apply false
    alias(libs.plugins.android.application) apply false
    alias(libs.plugins.compose.compiler) apply false
}



================================================
FILE: p2p-webrtc/video-transform/client/android/gradle.properties
================================================
# Project-wide Gradle settings.
# IDE (e.g. Android Studio) users:
# Gradle settings configured through the IDE *will override*
# any settings specified in this file.
# For more details on how to configure your build environment visit
# http://www.gradle.org/docs/current/userguide/build_environment.html
# Specifies the JVM arguments used for the daemon process.
# The setting is particularly useful for tweaking memory settings.
org.gradle.jvmargs=-Xmx2048m -Dfile.encoding=UTF-8
# When configured, Gradle will run in incubating parallel mode.
# This option should only be used with decoupled projects. For more details, visit
# https://developer.android.com/r/tools/gradle-multi-project-decoupled-projects
# org.gradle.parallel=true
# AndroidX package structure to make it clearer which packages are bundled with the
# Android operating system, and which are packaged with your app's APK
# https://developer.android.com/topic/libraries/support-library/androidx-rn
android.useAndroidX=true
# Kotlin code style for this project: "official" or "obsolete":
kotlin.code.style=official
# Enables namespacing of each library's R class so that its R class includes only the
# resources declared in the library itself and none from the library's dependencies,
# thereby reducing the size of the R class for that library
android.nonTransitiveRClass=true


================================================
FILE: p2p-webrtc/video-transform/client/android/gradlew
================================================
#!/usr/bin/env sh

#
# Copyright 2015 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

##############################################################################
##
##  Gradle start up script for UN*X
##
##############################################################################

# Attempt to set APP_HOME
# Resolve links: $0 may be a link
PRG="$0"
# Need this for relative symlinks.
while [ -h "$PRG" ] ; do
    ls=`ls -ld "$PRG"`
    link=`expr "$ls" : '.*-> \(.*\)$'`
    if expr "$link" : '/.*' > /dev/null; then
        PRG="$link"
    else
        PRG=`dirname "$PRG"`"/$link"
    fi
done
SAVED="`pwd`"
cd "`dirname \"$PRG\"`/" >/dev/null
APP_HOME="`pwd -P`"
cd "$SAVED" >/dev/null

APP_NAME="Gradle"
APP_BASE_NAME=`basename "$0"`

# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
DEFAULT_JVM_OPTS='"-Xmx64m" "-Xms64m"'

# Use the maximum available, or set MAX_FD != -1 to use that value.
MAX_FD="maximum"

warn () {
    echo "$*"
}

die () {
    echo
    echo "$*"
    echo
    exit 1
}

# OS specific support (must be 'true' or 'false').
cygwin=false
msys=false
darwin=false
nonstop=false
case "`uname`" in
  CYGWIN* )
    cygwin=true
    ;;
  Darwin* )
    darwin=true
    ;;
  MINGW* )
    msys=true
    ;;
  NONSTOP* )
    nonstop=true
    ;;
esac

CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar


# Determine the Java command to use to start the JVM.
if [ -n "$JAVA_HOME" ] ; then
    if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
        # IBM's JDK on AIX uses strange locations for the executables
        JAVACMD="$JAVA_HOME/jre/sh/java"
    else
        JAVACMD="$JAVA_HOME/bin/java"
    fi
    if [ ! -x "$JAVACMD" ] ; then
        die "ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
else
    JAVACMD="java"
    which java >/dev/null 2>&1 || die "ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
fi

# Increase the maximum file descriptors if we can.
if [ "$cygwin" = "false" -a "$darwin" = "false" -a "$nonstop" = "false" ] ; then
    MAX_FD_LIMIT=`ulimit -H -n`
    if [ $? -eq 0 ] ; then
        if [ "$MAX_FD" = "maximum" -o "$MAX_FD" = "max" ] ; then
            MAX_FD="$MAX_FD_LIMIT"
        fi
        ulimit -n $MAX_FD
        if [ $? -ne 0 ] ; then
            warn "Could not set maximum file descriptor limit: $MAX_FD"
        fi
    else
        warn "Could not query maximum file descriptor limit: $MAX_FD_LIMIT"
    fi
fi

# For Darwin, add options to specify how the application appears in the dock
if $darwin; then
    GRADLE_OPTS="$GRADLE_OPTS \"-Xdock:name=$APP_NAME\" \"-Xdock:icon=$APP_HOME/media/gradle.icns\""
fi

# For Cygwin or MSYS, switch paths to Windows format before running java
if [ "$cygwin" = "true" -o "$msys" = "true" ] ; then
    APP_HOME=`cygpath --path --mixed "$APP_HOME"`
    CLASSPATH=`cygpath --path --mixed "$CLASSPATH"`

    JAVACMD=`cygpath --unix "$JAVACMD"`

    # We build the pattern for arguments to be converted via cygpath
    ROOTDIRSRAW=`find -L / -maxdepth 1 -mindepth 1 -type d 2>/dev/null`
    SEP=""
    for dir in $ROOTDIRSRAW ; do
        ROOTDIRS="$ROOTDIRS$SEP$dir"
        SEP="|"
    done
    OURCYGPATTERN="(^($ROOTDIRS))"
    # Add a user-defined pattern to the cygpath arguments
    if [ "$GRADLE_CYGPATTERN" != "" ] ; then
        OURCYGPATTERN="$OURCYGPATTERN|($GRADLE_CYGPATTERN)"
    fi
    # Now convert the arguments - kludge to limit ourselves to /bin/sh
    i=0
    for arg in "$@" ; do
        CHECK=`echo "$arg"|egrep -c "$OURCYGPATTERN" -`
        CHECK2=`echo "$arg"|egrep -c "^-"`                                 ### Determine if an option

        if [ $CHECK -ne 0 ] && [ $CHECK2 -eq 0 ] ; then                    ### Added a condition
            eval `echo args$i`=`cygpath --path --ignore --mixed "$arg"`
        else
            eval `echo args$i`="\"$arg\""
        fi
        i=`expr $i + 1`
    done
    case $i in
        0) set -- ;;
        1) set -- "$args0" ;;
        2) set -- "$args0" "$args1" ;;
        3) set -- "$args0" "$args1" "$args2" ;;
        4) set -- "$args0" "$args1" "$args2" "$args3" ;;
        5) set -- "$args0" "$args1" "$args2" "$args3" "$args4" ;;
        6) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" ;;
        7) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" ;;
        8) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" ;;
        9) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" "$args8" ;;
    esac
fi

# Escape application args
save () {
    for i do printf %s\\n "$i" | sed "s/'/'\\\\''/g;1s/^/'/;\$s/\$/' \\\\/" ; done
    echo " "
}
APP_ARGS=`save "$@"`

# Collect all arguments for the java command, following the shell quoting and substitution rules
eval set -- $DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS "\"-Dorg.gradle.appname=$APP_BASE_NAME\"" -classpath "\"$CLASSPATH\"" org.gradle.wrapper.GradleWrapperMain "$APP_ARGS"

exec "$JAVACMD" "$@"



================================================
FILE: p2p-webrtc/video-transform/client/android/gradlew.bat
================================================
@rem
@rem Copyright 2015 the original author or authors.
@rem
@rem Licensed under the Apache License, Version 2.0 (the "License");
@rem you may not use this file except in compliance with the License.
@rem You may obtain a copy of the License at
@rem
@rem      https://www.apache.org/licenses/LICENSE-2.0
@rem
@rem Unless required by applicable law or agreed to in writing, software
@rem distributed under the License is distributed on an "AS IS" BASIS,
@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@rem See the License for the specific language governing permissions and
@rem limitations under the License.
@rem

@if "%DEBUG%" == "" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%" == "" set DIRNAME=.
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Resolve any "." and ".." in APP_HOME to make it shorter.
for %%i in ("%APP_HOME%") do set APP_HOME=%%~fi

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS="-Xmx64m" "-Xms64m"

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if "%ERRORLEVEL%" == "0" goto execute

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto execute

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar


@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %*

:end
@rem End local scope for the variables with windows NT shell
if "%ERRORLEVEL%"=="0" goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
if  not "" == "%GRADLE_EXIT_CONSOLE%" exit 1
exit /b 1

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega



================================================
FILE: p2p-webrtc/video-transform/client/android/LICENSE
================================================
BSD 2-Clause License

Copyright (c) 2024–2025, Daily

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



================================================
FILE: p2p-webrtc/video-transform/client/android/settings.gradle.kts
================================================
pluginManagement {
    repositories {
        google {
            content {
                includeGroupByRegex("com\\.android.*")
                includeGroupByRegex("com\\.google.*")
                includeGroupByRegex("androidx.*")
            }
        }
        mavenCentral()
        gradlePluginPortal()
    }
}
dependencyResolutionManagement {
    repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
    repositories {
        google()
        mavenCentral()
        mavenLocal()
    }
}

rootProject.name = "Pipecat Small WebRTC Client"
include(":small-webrtc-client")



================================================
FILE: p2p-webrtc/video-transform/client/android/gradle/libs.versions.toml
================================================
[versions]
accompanistPermissions = "0.34.0"
agp = "8.7.3"
constraintlayoutCompose = "1.1.0"
pipecatClient = "1.1.0"
kotlin = "2.0.20"
coreKtx = "1.15.0"
lifecycleRuntimeKtx = "2.8.7"
activityCompose = "1.9.3"
composeBom = "2024.12.01"
kotlinxSerializationJson = "1.7.1"
kotlinxSerializationPlugin = "2.0.20"

[libraries]
accompanist-permissions = { module = "com.google.accompanist:accompanist-permissions", version.ref = "accompanistPermissions" }
androidx-constraintlayout-compose = { module = "androidx.constraintlayout:constraintlayout-compose", version.ref = "constraintlayoutCompose" }
androidx-core-ktx = { group = "androidx.core", name = "core-ktx", version.ref = "coreKtx" }
pipecat-client-smallwebrtc = { module = "ai.pipecat:small-webrtc-transport", version.ref = "pipecatClient" }
androidx-lifecycle-runtime-ktx = { group = "androidx.lifecycle", name = "lifecycle-runtime-ktx", version.ref = "lifecycleRuntimeKtx" }
androidx-activity-compose = { group = "androidx.activity", name = "activity-compose", version.ref = "activityCompose" }
androidx-compose-bom = { group = "androidx.compose", name = "compose-bom", version.ref = "composeBom" }
androidx-ui = { group = "androidx.compose.ui", name = "ui" }
androidx-ui-graphics = { group = "androidx.compose.ui", name = "ui-graphics" }
androidx-ui-tooling = { group = "androidx.compose.ui", name = "ui-tooling" }
androidx-ui-tooling-preview = { group = "androidx.compose.ui", name = "ui-tooling-preview" }
androidx-ui-test-manifest = { group = "androidx.compose.ui", name = "ui-test-manifest" }
androidx-material3 = { group = "androidx.compose.material3", name = "material3" }
kotlinx-serialization-json = { module = "org.jetbrains.kotlinx:kotlinx-serialization-json", version.ref = "kotlinxSerializationJson" }

[plugins]
jetbrains-kotlin-android = { id = "org.jetbrains.kotlin.android", version.ref = "kotlin" }
android-application = { id = "com.android.application", version.ref = "agp" }
compose-compiler = { id = "org.jetbrains.kotlin.plugin.compose", version.ref = "kotlin" }
jetbrains-kotlin-serialization = { id = "org.jetbrains.kotlin.plugin.serialization", version.ref = "kotlinxSerializationPlugin" }



================================================
FILE: p2p-webrtc/video-transform/client/android/gradle/wrapper/gradle-wrapper.properties
================================================
#Mon Aug 05 13:01:27 BST 2024
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.10.2-bin.zip
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/build.gradle.kts
================================================
plugins {
    alias(libs.plugins.android.application)
    alias(libs.plugins.jetbrains.kotlin.android)
    alias(libs.plugins.jetbrains.kotlin.serialization)
    alias(libs.plugins.compose.compiler)
}

android {
    namespace = "ai.pipecat.small_webrtc_client"
    compileSdk = 35

    defaultConfig {
        applicationId = "ai.pipecat.small_webrtc_client"
        minSdk = 26
        targetSdk = 35
        versionCode = 1
        versionName = "1.0"

        vectorDrawables {
            useSupportLibrary = true
        }
    }

    buildTypes {
        release {
            isMinifyEnabled = false
            proguardFiles(
                getDefaultProguardFile("proguard-android-optimize.txt"),
                "proguard-rules.pro"
            )
        }
    }

    compileOptions {
        sourceCompatibility = JavaVersion.VERSION_11
        targetCompatibility = JavaVersion.VERSION_11
    }

    kotlinOptions {
        jvmTarget = "11"
    }

    buildFeatures {
        compose = true
        buildConfig = true
    }

    composeOptions {
        kotlinCompilerExtensionVersion = "1.5.1"
    }

    packaging {
        resources {
            excludes += "/META-INF/{AL2.0,LGPL2.1}"
        }
    }
}

dependencies {
    implementation(libs.pipecat.client.smallwebrtc)
    implementation(libs.androidx.core.ktx)
    implementation(libs.androidx.lifecycle.runtime.ktx)
    implementation(libs.androidx.activity.compose)
    implementation(platform(libs.androidx.compose.bom))
    implementation(libs.androidx.ui)
    implementation(libs.androidx.ui.graphics)
    implementation(libs.androidx.ui.tooling.preview)
    implementation(libs.androidx.material3)
    implementation(libs.accompanist.permissions)
    implementation(libs.androidx.constraintlayout.compose)
    implementation(libs.kotlinx.serialization.json)
    androidTestImplementation(platform(libs.androidx.compose.bom))
    debugImplementation(libs.androidx.ui.tooling)
    debugImplementation(libs.androidx.ui.test.manifest)
}



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/proguard-rules.pro
================================================
# Add project specific ProGuard rules here.
# You can control the set of applied configuration files using the
# proguardFiles setting in build.gradle.
#
# For more details, see
#   http://developer.android.com/guide/developing/tools/proguard.html

# If your project uses WebView with JS, uncomment the following
# and specify the fully qualified class name to the JavaScript interface
# class:
#-keepclassmembers class fqcn.of.javascript.interface.for.webview {
#   public *;
#}

# Uncomment this to preserve the line number information for
# debugging stack traces.
#-keepattributes SourceFile,LineNumberTable

# If you keep the line number information, uncomment this to
# hide the original source file name.
#-renamesourcefileattribute SourceFile


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/AndroidManifest.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android">

    <uses-feature
        android:name="android.hardware.camera"
        android:required="false" />

    <uses-permission android:name="android.permission.INTERNET" />
    <uses-permission android:name="android.permission.CAMERA" />
    <uses-permission android:name="android.permission.RECORD_AUDIO" />
    <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />
    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />

    <application
        android:name=".RTVIApplication"
        android:allowBackup="true"
        android:icon="@mipmap/ic_launcher"
        android:label="@string/app_name"
        android:roundIcon="@mipmap/ic_launcher_round"
        android:supportsRtl="true"
        android:usesCleartextTraffic="true"
        android:theme="@style/Theme.RTVIClient">
        <activity
            android:name=".MainActivity"
            android:exported="true"
            android:windowSoftInputMode="adjustResize"
            android:configChanges="keyboard|keyboardHidden|orientation|screenSize"
            android:theme="@style/Theme.RTVIClient">
            <intent-filter>
                <action android:name="android.intent.action.MAIN" />

                <category android:name="android.intent.category.LAUNCHER" />
            </intent-filter>
        </activity>
    </application>

</manifest>



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/MainActivity.kt
================================================
package ai.pipecat.small_webrtc_client

import ai.pipecat.small_webrtc_client.ui.InCallLayout
import ai.pipecat.small_webrtc_client.ui.PermissionScreen
import ai.pipecat.small_webrtc_client.ui.theme.Colors
import ai.pipecat.small_webrtc_client.ui.theme.RTVIClientTheme
import ai.pipecat.small_webrtc_client.ui.theme.TextStyles
import ai.pipecat.small_webrtc_client.ui.theme.textFieldColors
import android.os.Bundle
import androidx.activity.ComponentActivity
import androidx.activity.compose.setContent
import androidx.activity.enableEdgeToEdge
import androidx.annotation.DrawableRes
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.height
import androidx.compose.foundation.layout.imePadding
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.rememberScrollState
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.foundation.text.KeyboardOptions
import androidx.compose.foundation.verticalScroll
import androidx.compose.material3.AlertDialog
import androidx.compose.material3.Button
import androidx.compose.material3.Icon
import androidx.compose.material3.Scaffold
import androidx.compose.material3.Text
import androidx.compose.material3.TextField
import androidx.compose.runtime.Composable
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.text.input.ImeAction
import androidx.compose.ui.text.input.KeyboardType
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp


class MainActivity : ComponentActivity() {

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        enableEdgeToEdge()

        val voiceClientManager = VoiceClientManager(this)

        setContent {
            RTVIClientTheme {
                Scaffold(modifier = Modifier.fillMaxSize()) { innerPadding ->
                    Box(
                        Modifier
                            .fillMaxSize()
                            .padding(innerPadding)
                    ) {
                        PermissionScreen()

                        val vcState = voiceClientManager.state.value

                        if (vcState != null) {
                            InCallLayout(voiceClientManager)

                        } else {
                            ConnectSettings(voiceClientManager)
                        }

                        voiceClientManager.errors.firstOrNull()?.let { errorText ->

                            val dismiss: () -> Unit = { voiceClientManager.errors.removeAt(0) }

                            AlertDialog(
                                onDismissRequest = dismiss,
                                confirmButton = {
                                    Button(onClick = dismiss) {
                                        Text(
                                            text = "OK",
                                            fontSize = 14.sp,
                                            fontWeight = FontWeight.W700,
                                            color = Color.White,
                                            style = TextStyles.base
                                        )
                                    }
                                },
                                containerColor = Color.White,
                                title = {
                                    Text(
                                        text = "Error",
                                        fontSize = 22.sp,
                                        fontWeight = FontWeight.W600,
                                        color = Color.Black,
                                        style = TextStyles.base
                                    )
                                },
                                text = {
                                    Text(
                                        text = errorText.message,
                                        fontSize = 16.sp,
                                        fontWeight = FontWeight.W400,
                                        color = Color.Black,
                                        style = TextStyles.base
                                    )
                                }
                            )
                        }
                    }
                }
            }
        }
    }
}

@Composable
fun ConnectSettings(
    voiceClientManager: VoiceClientManager,
) {
    val scrollState = rememberScrollState()

    val start = {
        val backendUrl = Preferences.backendUrl.value ?: ""
        val apiKey = Preferences.apiKey.value ?: ""

        voiceClientManager.start(
            baseUrl = backendUrl,
            apiKey = apiKey
        )
    }

    Box(
        modifier = Modifier
            .fillMaxSize()
            .verticalScroll(scrollState)
            .imePadding()
            .padding(20.dp),
        contentAlignment = Alignment.Center
    ) {
        Box(
            Modifier
                .fillMaxWidth()
                .shadow(2.dp, RoundedCornerShape(16.dp))
                .clip(RoundedCornerShape(16.dp))
                .background(Colors.mainSurfaceBackground)
        ) {
            Column(
                Modifier
                    .fillMaxWidth()
                    .padding(
                        vertical = 24.dp,
                        horizontal = 28.dp
                    )
            ) {
                Spacer(modifier = Modifier.height(12.dp))

                Text(
                    modifier = Modifier.align(Alignment.CenterHorizontally),
                    text = "Connect to WebRTC server",
                    fontSize = 22.sp,
                    fontWeight = FontWeight.W700,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(36.dp))

                Text(
                    text = "Backend URL",
                    fontSize = 16.sp,
                    fontWeight = FontWeight.W400,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(12.dp))

                TextField(
                    modifier = Modifier
                        .fillMaxWidth()
                        .border(1.dp, Colors.textFieldBorder, RoundedCornerShape(12.dp)),
                    value = Preferences.backendUrl.value ?: "",
                    onValueChange = { Preferences.backendUrl.value = it },
                    keyboardOptions = KeyboardOptions(
                        keyboardType = KeyboardType.Uri,
                        imeAction = ImeAction.Next
                    ),
                    colors = textFieldColors(),
                    shape = RoundedCornerShape(12.dp)
                )

                Spacer(modifier = Modifier.height(36.dp))

                Text(
                    text = "API key",
                    fontSize = 16.sp,
                    fontWeight = FontWeight.W400,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(12.dp))

                TextField(
                    modifier = Modifier
                        .fillMaxWidth()
                        .border(1.dp, Colors.textFieldBorder, RoundedCornerShape(12.dp)),
                    value = Preferences.apiKey.value ?: "",
                    onValueChange = { Preferences.apiKey.value = it },
                    keyboardOptions = KeyboardOptions(
                        keyboardType = KeyboardType.Unspecified,
                        imeAction = ImeAction.Next
                    ),
                    colors = textFieldColors(),
                    shape = RoundedCornerShape(12.dp)
                )

                Spacer(modifier = Modifier.height(36.dp))

                Row(
                    modifier = Modifier.fillMaxWidth(),
                    horizontalArrangement = Arrangement.spacedBy(16.dp)
                ) {
                    ConnectDialogButton(
                        modifier = Modifier.weight(1f),
                        onClick = start,
                        text = "Connect",
                        foreground = Color.White,
                        background = Colors.buttonNormal,
                        border = Colors.buttonNormal
                    )
                }
            }
        }
    }
}

@Composable
private fun ConnectDialogButton(
    onClick: () -> Unit,
    text: String,
    foreground: Color,
    background: Color,
    border: Color,
    modifier: Modifier = Modifier,
    @DrawableRes icon: Int? = null,
) {
    val shape = RoundedCornerShape(8.dp)

    Row(
        modifier
            .border(1.dp, border, shape)
            .clip(shape)
            .background(background)
            .clickable(onClick = onClick)
            .padding(vertical = 10.dp, horizontal = 24.dp),
        verticalAlignment = Alignment.CenterVertically,
        horizontalArrangement = Arrangement.Center
    ) {
        if (icon != null) {
            Icon(
                modifier = Modifier.size(24.dp),
                painter = painterResource(icon),
                tint = foreground,
                contentDescription = null
            )

            Spacer(modifier = Modifier.width(8.dp))
        }

        Text(
            text = text,
            fontSize = 16.sp,
            fontWeight = FontWeight.W500,
            color = foreground
        )
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/Preferences.kt
================================================
package ai.pipecat.small_webrtc_client

import android.content.Context
import android.content.SharedPreferences
import androidx.compose.runtime.mutableStateOf
import kotlinx.serialization.KSerializer
import kotlinx.serialization.json.Json

private val JSON_INSTANCE = Json { ignoreUnknownKeys = true }

object Preferences {

    private const val PREF_BACKEND_URL = "backend_url"
    private const val PREF_API_KEY = "api_key"

    private lateinit var prefs: SharedPreferences

    fun initAppStart(context: Context) {
        prefs = context.applicationContext.getSharedPreferences("prefs", Context.MODE_PRIVATE)

        listOf(backendUrl, apiKey).forEach { it.init() }
    }

    private fun getString(key: String): String? = prefs.getString(key, null)

    interface BasePref {
        fun init()
    }

    class StringPref(private val key: String): BasePref {
        private val cachedValue = mutableStateOf<String?>(null)

        override fun init() {
            cachedValue.value = getString(key)
            prefs.registerOnSharedPreferenceChangeListener { _, changedKey ->
                if (key == changedKey) {
                    cachedValue.value = getString(key)
                }
            }
        }

        var value: String?
            get() = cachedValue.value
            set(newValue) {
                cachedValue.value = newValue
                prefs.edit().putString(key, newValue).apply()
            }
    }

    class JsonPref<E>(private val key: String, private var serializer: KSerializer<E>): BasePref {
        private val cachedValue = mutableStateOf<E?>(null)

        private fun lookupValue(): E? =
            getString(key)?.let { JSON_INSTANCE.decodeFromString(serializer, it) }

        override fun init() {
            cachedValue.value = lookupValue()
            prefs.registerOnSharedPreferenceChangeListener { _, changedKey ->
                if (key == changedKey) {
                    cachedValue.value = lookupValue()
                }
            }
        }

        var value: E?
            get() = cachedValue.value
            set(newValue) {
                cachedValue.value = newValue
                prefs.edit()
                    .putString(key, newValue?.let { JSON_INSTANCE.encodeToString(serializer, it) })
                    .apply()
            }
    }

    val backendUrl = StringPref(PREF_BACKEND_URL)
    val apiKey = StringPref(PREF_API_KEY)
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/RTVIApplication.kt
================================================
package ai.pipecat.small_webrtc_client

import android.app.Application

class RTVIApplication : Application() {
    override fun onCreate() {
        super.onCreate()
        Preferences.initAppStart(this)
    }
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/VoiceClientManager.kt
================================================
package ai.pipecat.small_webrtc_client

import ai.pipecat.client.PipecatClient
import ai.pipecat.client.PipecatClientOptions
import ai.pipecat.client.PipecatEventCallbacks
import ai.pipecat.client.result.Future
import ai.pipecat.client.result.RTVIError
import ai.pipecat.client.small_webrtc_transport.PipecatClientSmallWebRTC
import ai.pipecat.client.small_webrtc_transport.SmallWebRTCTransport
import ai.pipecat.client.types.APIRequest
import ai.pipecat.client.types.BotOutputData
import ai.pipecat.client.types.BotReadyData
import ai.pipecat.client.types.Participant
import ai.pipecat.client.types.PipecatMetrics
import ai.pipecat.client.types.Tracks
import ai.pipecat.client.types.Transcript
import ai.pipecat.client.types.TransportState
import ai.pipecat.client.types.Value
import ai.pipecat.small_webrtc_client.utils.Timestamp
import android.content.Context
import android.util.Log
import androidx.compose.runtime.Immutable
import androidx.compose.runtime.Stable
import androidx.compose.runtime.mutableFloatStateOf
import androidx.compose.runtime.mutableStateListOf
import androidx.compose.runtime.mutableStateOf

@Immutable
data class Error(val message: String)

@Stable
class VoiceClientManager(private val context: Context) {

    companion object {
        private const val TAG = "VoiceClientManager"
    }

    private val client = mutableStateOf<PipecatClientSmallWebRTC?>(null)

    val state = mutableStateOf<TransportState?>(null)

    val errors = mutableStateListOf<Error>()


    val expiryTime = mutableStateOf<Timestamp?>(null)

    val botReady = mutableStateOf(false)
    val botIsTalking = mutableStateOf(false)
    val userIsTalking = mutableStateOf(false)
    val botAudioLevel = mutableFloatStateOf(0f)
    val userAudioLevel = mutableFloatStateOf(0f)

    val mic = mutableStateOf(false)
    val camera = mutableStateOf(false)
    val tracks = mutableStateOf<Tracks?>(null)

    private fun <E> Future<E, RTVIError>.displayErrors() = withErrorCallback {
        Log.e(TAG, "Future resolved with error: ${it.description}", it.exception)
        errors.add(Error(it.description))
    }

    fun start(baseUrl: String, apiKey: String) {

        if (client.value != null) {
            return
        }

        state.value = TransportState.Disconnected

        val callbacks = object : PipecatEventCallbacks() {
            override fun onTransportStateChanged(state: TransportState) {
                this@VoiceClientManager.state.value = state
            }

            override fun onBackendError(message: String) {
                "Error from backend: $message".let {
                    Log.e(TAG, it)
                    errors.add(Error(it))
                }
            }

            override fun onBotReady(data: BotReadyData) {
                Log.i(TAG, "Bot ready: $data")
                botReady.value = true
            }

            override fun onMetrics(data: PipecatMetrics) {
                Log.i(TAG, "Pipecat metrics: $data")
            }

            override fun onBotOutput(data: BotOutputData) {
                Log.i(TAG, "Bot output: $data")
            }

            override fun onUserTranscript(data: Transcript) {
                Log.i(TAG, "User transcript: $data")
            }

            override fun onBotStartedSpeaking() {
                Log.i(TAG, "Bot started speaking")
                botIsTalking.value = true
            }

            override fun onBotStoppedSpeaking() {
                Log.i(TAG, "Bot stopped speaking")
                botIsTalking.value = false
            }

            override fun onUserStartedSpeaking() {
                Log.i(TAG, "User started speaking")
                userIsTalking.value = true
            }

            override fun onUserStoppedSpeaking() {
                Log.i(TAG, "User stopped speaking")
                userIsTalking.value = false
            }

            override fun onTracksUpdated(tracks: Tracks) {
                this@VoiceClientManager.tracks.value = tracks
            }

            override fun onInputsUpdated(camera: Boolean, mic: Boolean) {
                this@VoiceClientManager.camera.value = camera
                this@VoiceClientManager.mic.value = mic
            }

            override fun onDisconnected() {
                expiryTime.value = null
                botIsTalking.value = false
                userIsTalking.value = false
                state.value = null
                botReady.value = false
                tracks.value = null

                client.value?.release()
                client.value = null
            }

            override fun onUserAudioLevel(level: Float) {
                userAudioLevel.floatValue = level
            }

            override fun onRemoteAudioLevel(level: Float, participant: Participant) {
                botAudioLevel.floatValue = level
            }
        }

        val options = PipecatClientOptions(
            enableMic = true,
            enableCam = true,
            callbacks = callbacks
        )

        val client = PipecatClient(
            transport = SmallWebRTCTransport(context),
            options = options
        )

        client.startBotAndConnect(APIRequest(
            endpoint = baseUrl,
            requestData = Value.Object(),
            headers = listOfNotNull(
                apiKey.trim().takeIf { it.isNotEmpty() }?.let {"Authorization" to "Bearer $it"}
            ).toMap()
        )).displayErrors().withErrorCallback {
            callbacks.onDisconnected()
        }

        this.client.value = client
    }

    fun enableCamera(enabled: Boolean) {
        client.value?.enableCam(enabled)?.displayErrors()
    }

    fun enableMic(enabled: Boolean) {
        client.value?.enableMic(enabled)?.displayErrors()
    }

    fun toggleCamera() = enableCamera(!camera.value)
    fun toggleMic() = enableMic(!mic.value)

    fun stop() {
        client.value?.disconnect()?.displayErrors()
    }
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/AudioIndicator.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import androidx.compose.animation.core.LinearEasing
import androidx.compose.animation.core.animateFloat
import androidx.compose.animation.core.animateFloatAsState
import androidx.compose.animation.core.infiniteRepeatable
import androidx.compose.animation.core.rememberInfiniteTransition
import androidx.compose.animation.core.tween
import androidx.compose.foundation.Canvas
import androidx.compose.runtime.Composable
import androidx.compose.runtime.getValue
import androidx.compose.ui.Modifier
import androidx.compose.ui.geometry.Offset
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.graphics.StrokeCap
import androidx.compose.ui.semantics.clearAndSetSemantics

@Composable
fun ListeningAnimation(
    modifier: Modifier,
    active: Boolean,
    level: Float,
    color: Color,
) {
    val infiniteTransition = rememberInfiniteTransition("listeningAnimation")

    val loopState by infiniteTransition.animateFloat(
        initialValue = 0f,
        targetValue = Math.PI.toFloat() * 2f,
        animationSpec = infiniteRepeatable(tween(durationMillis = 1000, easing = LinearEasing)),
        label = "listeningAnimationLoopState"
    )

    val activeFraction by animateFloatAsState(
        if (active) {
            Math.pow(level.toDouble(), 0.3).toFloat()
        } else {
            0f
        }
    )

    Canvas(modifier.clearAndSetSemantics { }) {

        val strokeWidthPx = size.width / 12

        val lineCount = 5

        for (i in 1..lineCount) {

            val sine = Math.sin(loopState + 0.9 * i)
            val fraction = activeFraction * ((sine + 1) / 2).toFloat()

            val x = (size.width / (lineCount + 1)) * i

            val yMax = size.height * 0.25f
            val yMin = size.height * 0.5f

            val y = yMin + (yMax - yMin) * fraction
            val yEnd = size.height - y

            this.drawLine(
                start = Offset(x, y),
                end = Offset(x, yEnd),
                color = color,
                strokeWidth = strokeWidthPx,
                cap = StrokeCap.Round
            )
        }
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/BotIndicator.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.client.small_webrtc_transport.views.VideoView
import ai.pipecat.client.types.Tracks
import ai.pipecat.small_webrtc_client.ui.theme.Colors
import android.view.ViewGroup.LayoutParams
import androidx.compose.animation.AnimatedContent
import androidx.compose.animation.animateColorAsState
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.aspectRatio
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.material3.CircularProgressIndicator
import androidx.compose.runtime.Composable
import androidx.compose.runtime.FloatState
import androidx.compose.runtime.State
import androidx.compose.runtime.derivedStateOf
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableFloatStateOf
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.graphics.StrokeCap
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.compose.ui.viewinterop.AndroidView

@Composable
fun BotIndicator(
    modifier: Modifier,
    isReady: Boolean,
    isTalking: State<Boolean>,
    audioLevel: FloatState,
    tracks: State<Tracks?>
) {
    Box(
        modifier = modifier.padding(15.dp),
        contentAlignment = Alignment.Center
    ) {
        val color by animateColorAsState(if (isTalking.value || !isReady) {
            Color.Black
        } else {
            Colors.botIndicatorBackground
        })

        Box(
            Modifier
                .aspectRatio(1f)
                .fillMaxSize()
                .shadow(20.dp, CircleShape)
                .border(12.dp, Color.White, CircleShape)
                .border(1.dp, Colors.lightGrey, CircleShape)
                .clip(CircleShape)
                .background(color)
                .padding(50.dp),
            contentAlignment = Alignment.Center,
        ) {
            AnimatedContent(
                targetState = isReady
            ) { isReadyVal ->
                if (isReadyVal) {

                    val botVideoTrack by remember { derivedStateOf {
                        tracks.value?.bot?.video
                    }}

                    if (botVideoTrack != null) {
                        AndroidView(
                            modifier = Modifier.fillMaxSize(),
                            factory = { context ->
                                VideoView(context).apply {
                                    layoutParams = LayoutParams(
                                        LayoutParams.MATCH_PARENT,
                                        LayoutParams.MATCH_PARENT
                                    )
                                }
                            },
                            update = {
                                it.track = botVideoTrack
                            }
                        )
                    } else {
                        ListeningAnimation(
                            modifier = Modifier.fillMaxSize(),
                            active = isTalking.value,
                            level = audioLevel.floatValue,
                            color = Color.White
                        )
                    }
                } else {
                    CircularProgressIndicator(
                        modifier = Modifier.size(180.dp),
                        color = Color.White,
                        strokeWidth = 12.dp,
                        strokeCap = StrokeCap.Round,
                        trackColor = color
                    )
                }
            }
        }
    }
}

@Composable
@Preview
fun PreviewBotIndicator() {
    BotIndicator(
        modifier = Modifier,
        isReady = false,
        isTalking = remember { mutableStateOf(true) },
        audioLevel = remember { mutableFloatStateOf(1.0f) },
        tracks = remember { mutableStateOf(null) }
    )
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/InCallFooter.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.small_webrtc_client.R
import ai.pipecat.small_webrtc_client.ui.theme.Colors
import androidx.annotation.DrawableRes
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.ColumnScope
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material3.Icon
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp

@Composable
private fun FooterButton(
    modifier: Modifier,
    onClick: () -> Unit,
    @DrawableRes icon: Int,
    text: String,
    foreground: Color,
    background: Color,
    border: Color,
) {
    val shape = RoundedCornerShape(12.dp)

    Row(
        modifier
            .border(1.dp, border, shape)
            .clip(shape)
            .background(background)
            .clickable(onClick = onClick)
            .padding(vertical = 10.dp, horizontal = 18.dp),
        verticalAlignment = Alignment.CenterVertically,
        horizontalArrangement = Arrangement.Center
    ) {
        Icon(
            modifier = Modifier.size(24.dp),
            painter = painterResource(icon),
            tint = foreground,
            contentDescription = null
        )

        Spacer(modifier = Modifier.width(8.dp))

        Text(
            text = text,
            fontSize = 14.sp,
            fontWeight = FontWeight.W600,
            color = foreground
        )
    }
}

@Composable
fun ColumnScope.InCallFooter(
    onClickEnd: () -> Unit,
) {
    Row(Modifier
        .fillMaxWidth(0.5f)
        .padding(15.dp)
        .align(Alignment.CenterHorizontally)
    ) {
        FooterButton(
            modifier = Modifier.weight(1f),
            onClick = onClickEnd,
            icon = R.drawable.phone_hangup,
            text = "End",
            foreground = Color.White,
            background = Colors.endButton,
            border = Colors.endButton
        )
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/InCallHeader.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.small_webrtc_client.utils.Timestamp
import androidx.compose.animation.AnimatedContent
import androidx.compose.animation.fadeIn
import androidx.compose.animation.fadeOut
import androidx.compose.animation.togetherWith
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.padding
import androidx.compose.runtime.Composable
import androidx.compose.ui.Modifier
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.constraintlayout.compose.ConstraintLayout

@Composable
fun InCallHeader(
    expiryTime: Timestamp?
) {
    ConstraintLayout(
        Modifier
            .fillMaxWidth()
            .padding(vertical = 15.dp)
    ) {
        val refTimer = createRef()

        AnimatedContent(
            modifier = Modifier.constrainAs(refTimer) {
                top.linkTo(parent.top)
                bottom.linkTo(parent.bottom)
                end.linkTo(parent.end)
            },
            targetState = expiryTime,
            transitionSpec = { fadeIn() togetherWith fadeOut() }
        ) { expiryTimeVal ->
            if (expiryTimeVal != null) {
                Timer(expiryTime = expiryTimeVal, modifier = Modifier)
            }
        }
    }
}

@Composable
@Preview
fun PreviewInCallHeader() {
    InCallHeader(
        Timestamp.now() + java.time.Duration.ofMinutes(3)
    )
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/InCallLayout.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.small_webrtc_client.VoiceClientManager
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.runtime.Composable
import androidx.compose.runtime.derivedStateOf
import androidx.compose.runtime.getValue
import androidx.compose.runtime.remember
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.unit.dp

@Composable
fun InCallLayout(voiceClientManager: VoiceClientManager) {

    val localCam by remember { derivedStateOf { voiceClientManager.tracks.value?.local?.video } }

    Column(Modifier.fillMaxSize()) {

        InCallHeader(expiryTime = voiceClientManager.expiryTime.value)

        Box(
            modifier = Modifier
                .weight(1f)
                .fillMaxWidth(),
            contentAlignment = Alignment.Center
        ) {
            Column(
                modifier = Modifier.fillMaxWidth(),
                horizontalAlignment = Alignment.CenterHorizontally,
                verticalArrangement = Arrangement.spacedBy(12.dp, Alignment.CenterVertically)
            ) {
                BotIndicator(
                    modifier = Modifier,
                    isReady = voiceClientManager.botReady.value,
                    isTalking = voiceClientManager.botIsTalking,
                    audioLevel = voiceClientManager.botAudioLevel,
                    tracks = voiceClientManager.tracks
                )

                Row(
                    verticalAlignment = Alignment.CenterVertically
                ) {
                    UserMicButton(
                        onClick = voiceClientManager::toggleMic,
                        micEnabled = voiceClientManager.mic.value,
                        modifier = Modifier,
                        isTalking = voiceClientManager.userIsTalking,
                        audioLevel = voiceClientManager.userAudioLevel
                    )

                    UserCamButton(
                        onClick = voiceClientManager::toggleCamera,
                        camEnabled = voiceClientManager.camera.value,
                        camTrackId = localCam,
                        modifier = Modifier
                    )
                }
            }
        }

        InCallFooter(
            onClickEnd = voiceClientManager::stop
        )
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/PermissionScreen.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.small_webrtc_client.ui.theme.Colors
import ai.pipecat.small_webrtc_client.ui.theme.TextStyles
import android.Manifest
import android.util.Log
import androidx.activity.compose.rememberLauncherForActivityResult
import androidx.activity.result.contract.ActivityResultContracts
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.height
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material3.Button
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import androidx.compose.ui.window.Dialog
import com.google.accompanist.permissions.ExperimentalPermissionsApi
import com.google.accompanist.permissions.isGranted
import com.google.accompanist.permissions.rememberPermissionState

@OptIn(ExperimentalPermissionsApi::class)
@Composable
fun PermissionScreen() {
    val cameraPermission = rememberPermissionState(Manifest.permission.CAMERA)
    val micPermission = rememberPermissionState(Manifest.permission.RECORD_AUDIO)

    val requestPermissionLauncher = rememberLauncherForActivityResult(
        ActivityResultContracts.RequestMultiplePermissions()
    ) { isGranted ->
        Log.i("MainActivity", "Permissions granted: $isGranted")
    }

    if (!cameraPermission.status.isGranted || !micPermission.status.isGranted) {

        Dialog(
            onDismissRequest = {},
        ) {
            val dialogShape = RoundedCornerShape(16.dp)

            Column(
                Modifier
                    .shadow(6.dp, dialogShape)
                    .border(2.dp, Colors.logoBorder, dialogShape)
                    .clip(dialogShape)
                    .background(Color.White)
                    .padding(28.dp)
            ) {
                Text(
                    text = "Permissions",
                    fontSize = 24.sp,
                    fontWeight = FontWeight.W700,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(8.dp))

                Text(
                    text = "Please grant camera and mic permissions to continue",
                    fontSize = 18.sp,
                    fontWeight = FontWeight.W400,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(36.dp))

                Button(
                    modifier = Modifier.align(Alignment.End),
                    shape = RoundedCornerShape(12.dp),
                    onClick = {
                        requestPermissionLauncher.launch(
                            arrayOf(
                                Manifest.permission.CAMERA,
                                Manifest.permission.RECORD_AUDIO
                            )
                        )
                    }
                ) {
                    Text(
                        text = "Grant permissions",
                        fontSize = 16.sp,
                        fontWeight = FontWeight.W700,
                        style = TextStyles.base
                    )
                }
            }
        }
    }
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/Timer.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.small_webrtc_client.R
import ai.pipecat.small_webrtc_client.ui.theme.Colors
import ai.pipecat.small_webrtc_client.utils.Timestamp
import ai.pipecat.small_webrtc_client.utils.formatTimer
import ai.pipecat.small_webrtc_client.utils.rtcStateSecs
import androidx.compose.foundation.background
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.layout.widthIn
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material3.Icon
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.runtime.getValue
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import java.time.Duration

@Composable
fun Timer(
    expiryTime: Timestamp,
    modifier: Modifier,
) {
    val now by rtcStateSecs()

    val shape = RoundedCornerShape(
        topStart = 12.dp,
        bottomStart = 12.dp,
    )

    Row(
        modifier = modifier
            .widthIn(min = 100.dp)
            .clip(shape)
            .background(Colors.lightGrey)
            .padding(top = 12.dp, bottom = 12.dp, start = 12.dp, end = 16.dp),
        verticalAlignment = Alignment.CenterVertically
    ) {
        Icon(
            painter = painterResource(id = R.drawable.timer_outline),
            contentDescription = null,
            modifier = Modifier.size(20.dp),
            tint = Colors.expiryTimerForeground
        )

        Spacer(Modifier.width(8.dp))

        Text(
            text = formatTimer(duration = expiryTime - now),
            fontSize = 16.sp,
            fontWeight = FontWeight.W600,
            color = Colors.expiryTimerForeground
        )
    }
}

@Composable
@Preview
fun PreviewExpiryTimer() {
    Timer(Timestamp.now() + Duration.ofMinutes(5), Modifier)
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/UserCamButton.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.client.small_webrtc_transport.views.VideoView
import ai.pipecat.client.types.MediaTrackId
import ai.pipecat.small_webrtc_client.R
import ai.pipecat.small_webrtc_client.ui.theme.Colors
import androidx.compose.animation.animateColorAsState
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.material3.Icon
import androidx.compose.runtime.Composable
import androidx.compose.runtime.getValue
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.compose.ui.viewinterop.AndroidView

@Composable
fun UserCamButton(
    onClick: () -> Unit,
    camEnabled: Boolean,
    camTrackId: MediaTrackId?,
    modifier: Modifier,
) {
    Box(
        modifier = modifier.padding(15.dp).size(96.dp),
        contentAlignment = Alignment.Center
    ) {
        val color by animateColorAsState(
            if (camEnabled) {
                Colors.unmutedMicBackground
            } else {
                Colors.mutedMicBackground
            }
        )

        Box(
            Modifier
                .fillMaxSize()
                .shadow(3.dp, CircleShape)
                .border(6.dp, Color.White, CircleShape)
                .border(1.dp, Colors.lightGrey, CircleShape)
                .clip(CircleShape)
                .background(color)
                .clickable(onClick = onClick),
            contentAlignment = Alignment.Center,
        ) {
            if (camTrackId != null) {
                AndroidView(
                    factory = { context ->
                        VideoView(context)
                    },
                    update = { view ->
                        view.track = camTrackId
                    }
                )
            } else {
                Icon(
                    modifier = Modifier.size(30.dp),
                    painter = painterResource(
                        if (camEnabled) {
                            R.drawable.video
                        } else {
                            R.drawable.video_off
                        }
                    ),
                    tint = Color.White,
                    contentDescription = if (camEnabled) {
                        "Disable camera"
                    } else {
                        "Enable camera"
                    },
                )
            }
        }
    }
}

@Composable
@Preview
fun PreviewUserCamButton() {
    UserCamButton(
        onClick = {},
        camTrackId = null,
        camEnabled = true,
        modifier = Modifier,
    )
}

@Composable
@Preview
fun PreviewUserCamButtonMuted() {
    UserCamButton(
        onClick = {},
        camTrackId = null,
        camEnabled = false,
        modifier = Modifier,
    )
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/UserMicButton.kt
================================================
package ai.pipecat.small_webrtc_client.ui

import ai.pipecat.small_webrtc_client.R
import ai.pipecat.small_webrtc_client.ui.theme.Colors
import androidx.compose.animation.animateColorAsState
import androidx.compose.animation.core.animateDpAsState
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.material3.Icon
import androidx.compose.runtime.Composable
import androidx.compose.runtime.FloatState
import androidx.compose.runtime.State
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableFloatStateOf
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp

@Composable
fun UserMicButton(
    onClick: () -> Unit,
    micEnabled: Boolean,
    modifier: Modifier,
    isTalking: State<Boolean>,
    audioLevel: FloatState,
) {
    Box(
        modifier = modifier.padding(15.dp),
        contentAlignment = Alignment.Center
    ) {
        val borderThickness by animateDpAsState(
            if (isTalking.value) {
                (24.dp * Math.pow(audioLevel.floatValue.toDouble(), 0.3).toFloat()) + 3.dp
            } else {
                6.dp
            }
        )

        val color by animateColorAsState(
            if (!micEnabled) {
                Colors.mutedMicBackground
            } else if (isTalking.value) {
                Color.Black
            } else {
                Colors.unmutedMicBackground
            }
        )

        Box(
            Modifier
                .shadow(3.dp, CircleShape)
                .border(borderThickness, Color.White, CircleShape)
                .border(1.dp, Colors.lightGrey, CircleShape)
                .clip(CircleShape)
                .background(color)
                .clickable(onClick = onClick)
                .padding(36.dp),
            contentAlignment = Alignment.Center,
        ) {
            Icon(
                modifier = Modifier.size(48.dp),
                painter = painterResource(
                    if (micEnabled) {
                        R.drawable.microphone
                    } else {
                        R.drawable.microphone_off
                    }
                ),
                tint = Color.White,
                contentDescription = if (micEnabled) {
                    "Mute microphone"
                } else {
                    "Unmute microphone"
                },
            )
        }
    }
}

@Composable
@Preview
fun PreviewUserMicButton() {
    UserMicButton(
        onClick = {},
        micEnabled = true,
        modifier = Modifier,
        isTalking = remember { mutableStateOf(false) },
        audioLevel = remember { mutableFloatStateOf(1.0f) }
    )
}

@Composable
@Preview
fun PreviewUserMicButtonMuted() {
    UserMicButton(
        onClick = {},
        micEnabled = false,
        modifier = Modifier,
        isTalking = remember { mutableStateOf(false) },
        audioLevel = remember { mutableFloatStateOf(1.0f) }
    )
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/theme/Color.kt
================================================
package ai.pipecat.small_webrtc_client.ui.theme

import androidx.compose.ui.graphics.Color

object Colors {
    val buttonNormal = Color(0xFF374151)
    val buttonWarning = Color(0xFFE53935)
    val buttonSection = Color(0xFFDFF1FF)

    val activityBackground = Color(0xFFF9FAFB)
    val mainSurfaceBackground = Color.White

    val lightGrey = Color(0x7FE5E7EB)
    val expiryTimerForeground = Color.Black
    val logoBorder = Color(0xFFE2E8F0)
    val endButton = Color(0xFF0F172A)
    val textFieldBorder = Color(0xFFDFE6EF)

    val botIndicatorBackground = Color(0xFF374151)
    val mutedMicBackground = Color(0xFFF04A4A)
    val unmutedMicBackground = Color(0xFF616978)
}


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/theme/Theme.kt
================================================
package ai.pipecat.small_webrtc_client.ui.theme

import androidx.compose.material3.MaterialTheme
import androidx.compose.material3.TextFieldDefaults
import androidx.compose.material3.lightColorScheme
import androidx.compose.runtime.Composable
import androidx.compose.ui.graphics.Color

private val LightColorScheme = lightColorScheme(
    primary = Colors.buttonNormal,
    secondary = Colors.buttonWarning,
    background = Colors.activityBackground,
    surface = Colors.mainSurfaceBackground
)

@Composable
fun RTVIClientTheme(
    content: @Composable () -> Unit
) {
    val colorScheme = LightColorScheme

    MaterialTheme(
        colorScheme = colorScheme,
        typography = Typography,
        content = content
    )
}

@Composable
fun textFieldColors() = TextFieldDefaults.colors().copy(
    unfocusedContainerColor = Colors.activityBackground,
    focusedContainerColor = Colors.activityBackground,
    focusedIndicatorColor = Color.Transparent,
    disabledIndicatorColor = Color.Transparent,
    unfocusedIndicatorColor = Color.Transparent,
)


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/ui/theme/Type.kt
================================================
package ai.pipecat.small_webrtc_client.ui.theme

import ai.pipecat.small_webrtc_client.R
import androidx.compose.material3.Typography
import androidx.compose.ui.text.TextStyle
import androidx.compose.ui.text.font.Font
import androidx.compose.ui.text.font.FontFamily
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.sp

object TextStyles {
    val base = TextStyle(fontFamily = FontFamily(Font(R.font.inter)))
}

// Set of Material typography styles to start with
val Typography = Typography(
    bodyLarge = TextStyle(
        fontFamily = FontFamily.Default,
        fontWeight = FontWeight.Normal,
        fontSize = 16.sp,
        lineHeight = 24.sp,
        letterSpacing = 0.5.sp
    )
    /* Other default text styles to override
    titleLarge = TextStyle(
        fontFamily = FontFamily.Default,
        fontWeight = FontWeight.Normal,
        fontSize = 22.sp,
        lineHeight = 28.sp,
        letterSpacing = 0.sp
    ),
    labelSmall = TextStyle(
        fontFamily = FontFamily.Default,
        fontWeight = FontWeight.Medium,
        fontSize = 11.sp,
        lineHeight = 16.sp,
        letterSpacing = 0.5.sp
    )
    */
)


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/utils/RealTimeClock.kt
================================================
package ai.pipecat.small_webrtc_client.utils

import androidx.compose.runtime.Composable
import androidx.compose.runtime.collectAsState
import kotlinx.coroutines.delay
import kotlinx.coroutines.flow.flow

private val rtcFlowSecs = flow {
    while(true) {
        val now = Timestamp.now().toEpochMilli()

        val rounded = ((now + 500) / 1000) * 1000
        emit(Timestamp.ofEpochMilli(rounded))

        val target = rounded + 1000
        delay(target - now)
    }
}

@Composable
fun rtcStateSecs() = rtcFlowSecs.collectAsState(initial = Timestamp.now())


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/java/ai/pipecat/small_webrtc_client/utils/TimeUtils.kt
================================================
package ai.pipecat.small_webrtc_client.utils

import androidx.compose.runtime.Composable
import androidx.compose.runtime.Immutable
import java.time.Duration
import java.time.Instant
import java.time.format.DateTimeFormatter
import java.util.Date

// Wrapper for Compose stability
@Immutable
@JvmInline
value class Timestamp(
    val value: Instant
) : Comparable<Timestamp> {
    val isInPast: Boolean
        get() = value < Instant.now()

    val isInFuture: Boolean
        get() = value > Instant.now()

    fun toEpochMilli() = value.toEpochMilli()

    operator fun plus(duration: Duration) = Timestamp(value + duration)

    operator fun minus(duration: Duration) = Timestamp(value - duration)

    operator fun minus(rhs: Timestamp) = Duration.between(rhs.value, value)

    override operator fun compareTo(other: Timestamp) = value.compareTo(other.value)

    fun toISOString(): String = DateTimeFormatter.ISO_INSTANT.format(value)

    override fun toString() = toISOString()

    companion object {
        fun now() = Timestamp(Instant.now())

        fun ofEpochMilli(value: Long) = Timestamp(Instant.ofEpochMilli(value))

        fun ofEpochSecs(value: Long) = ofEpochMilli(value * 1000)

        fun parse(value: CharSequence) = Timestamp(Instant.parse(value))

        fun from(date: Date) = Timestamp(date.toInstant())
    }
}

@Composable
fun formatTimer(duration: Duration): String {

    if (duration.seconds < 0) {
        return "0s"
    }

    val mins = duration.seconds / 60
    val secs = duration.seconds % 60

    return if (mins == 0L) {
        "${secs}s"
    } else {
        "${mins}m ${secs}s"
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/ic_launcher_background.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    android:width="108dp"
    android:height="108dp"
    android:viewportWidth="108"
    android:viewportHeight="108">
    <path
        android:fillColor="#3DDC84"
        android:pathData="M0,0h108v108h-108z" />
    <path
        android:fillColor="#00000000"
        android:pathData="M9,0L9,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,0L19,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M29,0L29,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M39,0L39,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M49,0L49,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M59,0L59,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M69,0L69,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M79,0L79,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M89,0L89,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M99,0L99,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,9L108,9"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,19L108,19"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,29L108,29"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,39L108,39"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,49L108,49"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,59L108,59"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,69L108,69"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,79L108,79"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,89L108,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,99L108,99"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,29L89,29"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,39L89,39"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,49L89,49"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,59L89,59"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,69L89,69"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,79L89,79"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M29,19L29,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M39,19L39,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M49,19L49,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M59,19L59,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M69,19L69,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M79,19L79,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
</vector>



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/ic_launcher_foreground.xml
================================================
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:aapt="http://schemas.android.com/aapt"
    android:width="108dp"
    android:height="108dp"
    android:viewportWidth="108"
    android:viewportHeight="108">
    <path android:pathData="M31,63.928c0,0 6.4,-11 12.1,-13.1c7.2,-2.6 26,-1.4 26,-1.4l38.1,38.1L107,108.928l-32,-1L31,63.928z">
        <aapt:attr name="android:fillColor">
            <gradient
                android:endX="85.84757"
                android:endY="92.4963"
                android:startX="42.9492"
                android:startY="49.59793"
                android:type="linear">
                <item
                    android:color="#44000000"
                    android:offset="0.0" />
                <item
                    android:color="#00000000"
                    android:offset="1.0" />
            </gradient>
        </aapt:attr>
    </path>
    <path
        android:fillColor="#FFFFFF"
        android:fillType="nonZero"
        android:pathData="M65.3,45.828l3.8,-6.6c0.2,-0.4 0.1,-0.9 -0.3,-1.1c-0.4,-0.2 -0.9,-0.1 -1.1,0.3l-3.9,6.7c-6.3,-2.8 -13.4,-2.8 -19.7,0l-3.9,-6.7c-0.2,-0.4 -0.7,-0.5 -1.1,-0.3C38.8,38.328 38.7,38.828 38.9,39.228l3.8,6.6C36.2,49.428 31.7,56.028 31,63.928h46C76.3,56.028 71.8,49.428 65.3,45.828zM43.4,57.328c-0.8,0 -1.5,-0.5 -1.8,-1.2c-0.3,-0.7 -0.1,-1.5 0.4,-2.1c0.5,-0.5 1.4,-0.7 2.1,-0.4c0.7,0.3 1.2,1 1.2,1.8C45.3,56.528 44.5,57.328 43.4,57.328L43.4,57.328zM64.6,57.328c-0.8,0 -1.5,-0.5 -1.8,-1.2s-0.1,-1.5 0.4,-2.1c0.5,-0.5 1.4,-0.7 2.1,-0.4c0.7,0.3 1.2,1 1.2,1.8C66.5,56.528 65.6,57.328 64.6,57.328L64.6,57.328z"
        android:strokeWidth="1"
        android:strokeColor="#00000000" />
</vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/microphone.xml
================================================
<!-- drawable/microphone.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M12,2A3,3 0 0,1 15,5V11A3,3 0 0,1 12,14A3,3 0 0,1 9,11V5A3,3 0 0,1 12,2M19,11C19,14.53 16.39,17.44 13,17.93V21H11V17.93C7.61,17.44 5,14.53 5,11H7A5,5 0 0,0 12,16A5,5 0 0,0 17,11H19Z" /></vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/microphone_off.xml
================================================
<!-- drawable/microphone_off.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M19,11C19,12.19 18.66,13.3 18.1,14.28L16.87,13.05C17.14,12.43 17.3,11.74 17.3,11H19M15,11.16L9,5.18V5A3,3 0 0,1 12,2A3,3 0 0,1 15,5V11L15,11.16M4.27,3L21,19.73L19.73,21L15.54,16.81C14.77,17.27 13.91,17.58 13,17.72V21H11V17.72C7.72,17.23 5,14.41 5,11H6.7C6.7,14 9.24,16.1 12,16.1C12.81,16.1 13.6,15.91 14.31,15.58L12.65,13.92L12,14A3,3 0 0,1 9,11V10.28L3,4.27L4.27,3Z" /></vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/phone_hangup.xml
================================================
<!-- drawable/phone_hangup.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M12,9C10.4,9 8.85,9.25 7.4,9.72V12.82C7.4,13.22 7.17,13.56 6.84,13.72C5.86,14.21 4.97,14.84 4.17,15.57C4,15.75 3.75,15.86 3.5,15.86C3.2,15.86 2.95,15.74 2.77,15.56L0.29,13.08C0.11,12.9 0,12.65 0,12.38C0,12.1 0.11,11.85 0.29,11.67C3.34,8.77 7.46,7 12,7C16.54,7 20.66,8.77 23.71,11.67C23.89,11.85 24,12.1 24,12.38C24,12.65 23.89,12.9 23.71,13.08L21.23,15.56C21.05,15.74 20.8,15.86 20.5,15.86C20.25,15.86 20,15.75 19.82,15.57C19.03,14.84 18.14,14.21 17.16,13.72C16.83,13.56 16.6,13.22 16.6,12.82V9.72C15.15,9.25 13.6,9 12,9Z" /></vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/timer_outline.xml
================================================
<!-- drawable/timer_outline.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M12,20A7,7 0 0,1 5,13A7,7 0 0,1 12,6A7,7 0 0,1 19,13A7,7 0 0,1 12,20M19.03,7.39L20.45,5.97C20,5.46 19.55,5 19.04,4.56L17.62,6C16.07,4.74 14.12,4 12,4A9,9 0 0,0 3,13A9,9 0 0,0 12,22C17,22 21,17.97 21,13C21,10.88 20.26,8.93 19.03,7.39M11,14H13V8H11M15,1H9V3H15V1Z" /></vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/video.xml
================================================
<!-- drawable/video.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M17,10.5V7A1,1 0 0,0 16,6H4A1,1 0 0,0 3,7V17A1,1 0 0,0 4,18H16A1,1 0 0,0 17,17V13.5L21,17.5V6.5L17,10.5Z" /></vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/drawable/video_off.xml
================================================
<!-- drawable/video_off.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M3.27,2L2,3.27L4.73,6H4A1,1 0 0,0 3,7V17A1,1 0 0,0 4,18H16C16.2,18 16.39,17.92 16.54,17.82L19.73,21L21,19.73M21,6.5L17,10.5V7A1,1 0 0,0 16,6H9.82L21,17.18V6.5Z" /></vector>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-anydpi-v26/ic_launcher.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<adaptive-icon xmlns:android="http://schemas.android.com/apk/res/android">
    <background android:drawable="@drawable/ic_launcher_background" />
    <foreground android:drawable="@drawable/ic_launcher_foreground" />
    <monochrome android:drawable="@drawable/ic_launcher_foreground" />
</adaptive-icon>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-anydpi-v26/ic_launcher_round.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<adaptive-icon xmlns:android="http://schemas.android.com/apk/res/android">
    <background android:drawable="@drawable/ic_launcher_background" />
    <foreground android:drawable="@drawable/ic_launcher_foreground" />
    <monochrome android:drawable="@drawable/ic_launcher_foreground" />
</adaptive-icon>


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-hdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-hdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-mdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-mdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-xhdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-xhdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-xxhdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-xxxhdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/values/strings.xml
================================================
<resources>
    <string name="app_name">Pipecat Small WebRTC Client</string>
</resources>



================================================
FILE: p2p-webrtc/video-transform/client/android/small-webrtc-client/src/main/res/values/themes.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<resources>

    <style name="Theme.RTVIClient" parent="android:Theme.Material.Light.NoActionBar" />
</resources>


================================================
FILE: p2p-webrtc/video-transform/client/ios/README.md
================================================
# iOS implementation

Basic implementation using the [Pipecat iOS SDK](https://docs.pipecat.ai/client/ios/introduction).

## Prerequisites

1. Run the bot server. See the [server README](../../server).
2. Install [Xcode](https://developer.apple.com/xcode/), and set up your device [to run your own applications](https://developer.apple.com/documentation/xcode/distributing-your-app-to-registered-devices).

## Running locally

1. Clone this repository locally.
2. Open the SimpleChatbot.xcodeproj in Xcode.
3. Tell Xcode to update its Package Dependencies by clicking File -> Packages -> Update to Latest Package Versions.
4. Build the project.
5. Run the project on your device.
6. Connect to the URL you are testing.




================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>UIBackgroundModes</key>
	<array>
		<string>voip</string>
	</array>
	<key>NSCameraUsageDescription</key>
    <string>Camera is necessary for transmitting video in a call</string>
    <key>NSMicrophoneUsageDescription</key>
    <string>Microphone is necessary for transmitting audio in a call</string>
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSAllowsLocalNetworking</key>
        <true/>
    </dict>
</dict>
</plist>



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/SimpleChatbotApp.swift
================================================
import SwiftUI

@main
struct SimpleChatbotApp: App {

    @StateObject var callContainerModel = CallContainerModel()

    var body: some Scene {
        WindowGroup {
            if (!callContainerModel.isInCall) {
                PreJoinView().environmentObject(callContainerModel)
            } else {
                MeetingView().environmentObject(callContainerModel)
            }
        }
    }

}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "appstore.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/Assets.xcassets/pipecat.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "Square Black.svg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/Assets.xcassets/vision.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "vision.svg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/model/CallContainerModel.swift
================================================
import SwiftUI

import PipecatClientIOSSmallWebrtc
import PipecatClientIOS

class CallContainerModel: ObservableObject {
    
    @Published var voiceClientStatus: String = TransportState.disconnected.description
    @Published var isInCall: Bool = false
    @Published var isBotReady: Bool = false
    
    @Published var isMicEnabled: Bool = false
    @Published var isCamEnabled: Bool = false
    @Published var localCamId: MediaTrackId? = nil
    @Published var botCamId: MediaTrackId? = nil
    
    @Published var toastMessage: String? = nil
    @Published var showToast: Bool = false
    
    @Published var messages: [LiveMessage] = []
    @Published var liveBotMessage: LiveMessage?
    @Published var liveUserMessage: LiveMessage?
    
    var pipecatClientIOS: PipecatClient?
    
    @Published var selectedMic: MediaDeviceId? = nil {
        didSet {
            guard let selectedMic else { return } // don't store nil
            var settings = SettingsManager.getSettings()
            settings.selectedMic = selectedMic.id
            SettingsManager.updateSettings(settings: settings)
        }
    }
    @Published var availableMics: [MediaDeviceInfo] = []
    
    init() {
        // Changing the log level
        PipecatClientIOS.setLogLevel(.warn)
        PipecatClientIOSSmallWebrtc.setLogLevel(.info)
    }
    
    @MainActor
    func connect(backendURL: String, apiKey: String) {
        self.resetLiveMessages()
        
        let baseUrl = backendURL.trimmingCharacters(in: .whitespacesAndNewlines)
        if(baseUrl.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty){
            self.showError(message: "Need to fill the backendURL")
            return
        }
        
        let currentSettings = SettingsManager.getSettings()
        let pipecatClientOptions = PipecatClientOptions.init(
            transport: SmallWebRTCTransport.init(),
            enableMic: currentSettings.enableMic,
            enableCam: currentSettings.enableCam,
        )
        self.pipecatClientIOS = PipecatClient.init(
            options: pipecatClientOptions
        )
        self.pipecatClientIOS?.delegate = self
        
        let authorizationToken = apiKey.trimmingCharacters(in: .whitespacesAndNewlines)
        print("authorizationToken: \(authorizationToken)")
        let headers = [["Authorization": "Bearer \(authorizationToken)"]]
        let startParams = APIRequest.init(
            endpoint: URL(string: baseUrl + "/start")!,
            headers: headers,
            requestData: Value.object([
                "enableDefaultIceServers": .boolean(true),
                "transport": .string("webrtc")
            ])
        )
        self.pipecatClientIOS?.startBotAndConnect(startBotParams: startParams) { (result: Result<SmallWebRTCStartBotResult, AsyncExecutionError>) in
            switch result {
            case .failure(let error):
                self.showError(message: error.localizedDescription)
                self.pipecatClientIOS = nil
            case .success(_):
                // Apply initial mic preference
                if let selectedMic = SettingsManager.getSettings().selectedMic {
                    self.selectMic(MediaDeviceId(id: selectedMic))
                }
                // Populate available devices list
                self.availableMics = self.pipecatClientIOS?.getAllMics() ?? []
            }
        }
        self.saveCredentials(backendURL: baseUrl, apiKey: authorizationToken)
    }
    
    @MainActor
    func disconnect() {
        self.pipecatClientIOS?.disconnect(completion: nil)
        self.pipecatClientIOS?.release()
        self.pipecatClientIOS = nil
    }
    
    func showError(message: String) {
        self.toastMessage = message
        self.showToast = true
        // Hide the toast after 5 seconds
        DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
            self.showToast = false
            self.toastMessage = nil
        }
    }
    
    @MainActor
    func toggleMicInput() {
        self.pipecatClientIOS?.enableMic(enable: !self.isMicEnabled) { result in
            switch result {
            case .success():
                self.isMicEnabled = self.pipecatClientIOS?.isMicEnabled ?? false
            case .failure(let error):
                self.showError(message: error.localizedDescription)
            }
        }
    }
    
    @MainActor
    func toggleCamInput() {
        print("Is cam enabled: \(self.isCamEnabled)")
        self.pipecatClientIOS?.enableCam(enable: !self.isCamEnabled) { result in
            switch result {
            case .success():
                self.isCamEnabled = self.pipecatClientIOS?.isCamEnabled ?? false
            case .failure(let error):
                self.showError(message: error.localizedDescription)
            }
        }
    }
    
    func saveCredentials(backendURL: String, apiKey: String) {
        var currentSettings = SettingsManager.getSettings()
        currentSettings.backendURL = backendURL
        currentSettings.apiKey = apiKey
        // Saving the settings
        SettingsManager.updateSettings(settings: currentSettings)
    }
    
    @MainActor
    func selectMic(_ mic: MediaDeviceId) {
        self.selectedMic = mic
        self.pipecatClientIOS?.updateMic(micId: mic, completion: nil)
    }
    
    private func createLiveMessage(content:String = "", type:MessageType) {
        // Creating a new one
        DispatchQueue.main.async {
            let liveMessage = LiveMessage(content: content, type: type, updatedAt: Date())
            self.messages.append(liveMessage)
            if type == .bot {
                self.liveBotMessage = liveMessage
            } else if type == .user {
                self.liveUserMessage = liveMessage
            }
        }
    }
    
    private func appendTextToLiveMessage(fromBot: Bool, content:String) {
        DispatchQueue.main.async {
            // Updating the last message with the new content
            if fromBot {
                self.liveBotMessage?.content += content
            } else {
                self.liveUserMessage?.content += content
            }
        }
    }
    
    private func resetLiveMessages() {
        DispatchQueue.main.async {
            self.messages = []
        }
    }
}

extension CallContainerModel:PipecatClientDelegate {
    
    private func handleEvent(eventName: String, eventValue: Any? = nil) {
        if let value = eventValue {
            print("Pipecat Demo, received event: \(eventName), value:\(value)")
        } else {
            print("Pipecat Demo, received event: \(eventName)")
        }
    }
    
    func onTransportStateChanged(state: TransportState) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTransportStateChanged", eventValue: state)
            self.voiceClientStatus = state.description
            self.isInCall = ( state == .connecting || state == .connected || state == .ready || state == .authenticating )
            self.createLiveMessage(content: state.description, type: .system)
        }
    }
    
    func onBotReady(botReadyData: BotReadyData) {
        Task { @MainActor in
            self.handleEvent(eventName: "onBotReady")
            self.isBotReady = true
        }
    }
    
    func onConnected() {
        Task { @MainActor in
            self.handleEvent(eventName: "onConnected")
            self.isMicEnabled = self.pipecatClientIOS?.isMicEnabled ?? false
            self.isCamEnabled = self.pipecatClientIOS?.isCamEnabled ?? false
        }
    }
    
    func onDisconnected() {
        Task { @MainActor in
            self.handleEvent(eventName: "onDisconnected")
            self.isBotReady = false
        }
    }
    
    func onError(message: RTVIMessageInbound) {
        Task { @MainActor in
            self.handleEvent(eventName: "onError", eventValue: message)
            self.showError(message: message.data ?? "")
        }
    }
    
    func onAvailableMicsUpdated(mics: [MediaDeviceInfo]) {
        Task { @MainActor in
            self.availableMics = mics
        }
    }
    
    func onMicUpdated(mic: MediaDeviceInfo?) {
        Task { @MainActor in
            self.selectedMic = mic?.id
        }
    }
    
    func onTrackStarted(track: MediaStreamTrack, participant: Participant?) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTrackStarted", eventValue: track)
            
            guard track.kind == .video else { return }
            
            // Use optional binding to simplify the check for local participant
            if participant?.local ?? true {
                self.localCamId = track.id
            } else {
                self.botCamId = track.id
            }
        }
    }

    func onTrackStopped(track: MediaStreamTrack, participant: Participant?) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTrackStopped", eventValue: track)
            
            guard track.kind == .video else { return }
            
            // Use optional binding to simplify the check for local participant
            if participant?.local ?? true {
                self.localCamId = nil
            } else {
                self.botCamId = nil
            }
        }
    }
    
    func onUserStartedSpeaking() {
        self.createLiveMessage(content: "User started speaking", type: .system)
        self.handleEvent(eventName: "onUserStartedSpeaking")
        self.createLiveMessage(type: .user)
    }
    
    func onUserStoppedSpeaking() {
        self.createLiveMessage(content: "User stopped speaking", type: .system)
        self.handleEvent(eventName: "onUserStoppedSpeaking")
    }
    
    func onBotStartedSpeaking() {
        self.createLiveMessage(content: "Bot started speaking", type: .system)
        self.handleEvent(eventName: "onBotStartedSpeaking")
        self.createLiveMessage(type: .bot)
    }
    
    func onBotStoppedSpeaking() {
        self.createLiveMessage(content: "Bot stopped speaking", type: .system)
        self.handleEvent(eventName: "onBotStoppedSpeaking")
    }
    
    func onUserTranscript(data: Transcript) {
        if data.final ?? false {
            self.handleEvent(eventName: "onUserTranscript", eventValue: data.text)
            self.appendTextToLiveMessage(fromBot: false, content: data.text)
        }
    }
    
    func onBotTranscript(data: BotLLMText) {
        self.handleEvent(eventName: "onBotTranscript", eventValue: data)
        self.appendTextToLiveMessage(fromBot: true, content: data.text)
    }
    
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/model/MockCallContainerModel.swift
================================================
import SwiftUI
import PipecatClientIOS

class MockCallContainerModel: CallContainerModel {
    
    override init() {
        super.init()
        let liveMessageFromSystem = LiveMessage(
            content: "System message",
            type: .system,
            updatedAt: Date()
        )
        let liveMessageFromUser = LiveMessage(
            content: "Message from User",
            type: .user,
            updatedAt: Date()
        )
        let liveMessageFromBot = LiveMessage(
            content: "Message from bot",
            type: .bot,
            updatedAt: Date()
        )
        self.messages = [ liveMessageFromSystem, liveMessageFromUser, liveMessageFromBot ]
    }

    override func connect(backendURL: String, apiKey: String) {
        print("connect")
    }
    
    override func disconnect() {
        print("disconnect")
    }
    
    override func showError(message: String) {
        self.toastMessage = message
        self.showToast = true
        // Hide the toast after 5 seconds
        DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
            self.showToast = false
            self.toastMessage = nil
        }
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/types/LiveMessage.swift
================================================
import Foundation

enum MessageType {
    case bot, user, system
}

class LiveMessage: ObservableObject, Identifiable, Equatable {
    @Published var content: String
    let type: MessageType
    let updatedAt: Date
    
    init(content: String, type: MessageType, updatedAt: Date) {
        self.content = content
        self.type = type
        self.updatedAt = updatedAt
    }
    
    static func == (lhs: LiveMessage, rhs: LiveMessage) -> Bool {
        lhs.updatedAt == rhs.updatedAt
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/MeetingView.swift
================================================
import SwiftUI
import PipecatClientIOSSmallWebrtc

struct MeetingView: View {
    
    @State private var showingSettings = false
    @EnvironmentObject private var model: CallContainerModel
    
    var body: some View {
        VStack {
            ZStack {
                SmallWebRTCVideoViewSwiftUI(videoTrack: self.model.botCamId, videoScaleMode: .fill)
                    .edgesIgnoringSafeArea(.all)
                
                VStack {
                    ChatView()
                        .frame(maxHeight: .infinity)
                    
                    HStack {
                        MicrophoneView(audioLevel: 0, isMuted: !self.model.isMicEnabled)
                            .frame(width: 100, height: 100)
                            .onTapGesture {
                                self.model.toggleMicInput()
                            }
                        CameraButtonView(trackId: self.model.localCamId, isMuted: !self.model.isCamEnabled)
                            .frame(width: 120, height: 120)
                            .onTapGesture {
                                self.model.toggleCamInput()
                            }
                    }
                    .padding()
                }
            }
            Button(action: {
                self.showingSettings = true
            }) {
                HStack {
                    Image(systemName: "gearshape")
                        .resizable()
                        .frame(width: 24, height: 24)
                    Text("Settings")
                }
                .frame(maxWidth: .infinity)
                .padding()
                .sheet(isPresented: $showingSettings) {
                    SettingsView(showingSettings: $showingSettings).environmentObject(self.model)
                }
            }
            .foregroundColor(.black)
            .background(Color.white)
            .border(Color.buttonsBorder, width: 1)
            .cornerRadius(12)
            .padding([.horizontal])
            
            Button(action: {
                self.model.disconnect()
            }) {
                HStack {
                    Image(systemName: "rectangle.portrait.and.arrow.right")
                        .resizable()
                        .frame(width: 24, height: 24)
                    Text("End")
                }
                .frame(maxWidth: .infinity)
                .padding()
            }
            .foregroundColor(.white)
            .background(Color.black)
            .cornerRadius(12)
            .padding([.bottom, .horizontal])
        }
        .background(Color.backgroundApp)
        .toast(message: model.toastMessage, isShowing: model.showToast)
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = MeetingView().environmentObject(mockModel as CallContainerModel)
    return result
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/PreJoinView.swift
================================================
import SwiftUI

struct PreJoinView: View {

    @State var backendURL: String
    @State var apiKey: String

    @EnvironmentObject private var model: CallContainerModel

    init() {
        let currentSettings = SettingsManager.getSettings()
        self.backendURL = currentSettings.backendURL
        self.apiKey = currentSettings.apiKey
    }

    var body: some View {
        VStack(spacing: 20) {
            Image("pipecat")
                .resizable()
                .frame(width: 80, height: 80)
            Text("Pipecat Client iOS.")
                .font(.headline)
            TextField("Server URL", text: $backendURL)
                .textFieldStyle(RoundedBorderTextFieldStyle())
                .frame(maxWidth: .infinity)
                .padding([.bottom, .horizontal])
            SecureField("Authorization token", text: $apiKey)
                .textFieldStyle(RoundedBorderTextFieldStyle())
                .frame(maxWidth: .infinity)
                .padding([.horizontal])
            Button("Connect") {
                Task {
                    self.model.connect(backendURL: self.backendURL, apiKey: self.apiKey)
                }
            }
            .padding()
            .background(Color.black)
            .foregroundColor(.white)
            .cornerRadius(8)
        }
        .padding()
        .frame(maxHeight: .infinity)
        .background(Color.backgroundApp)
        .toast(message: model.toastMessage, isShowing: model.showToast)
    }
}

#Preview {
    PreJoinView().environmentObject(MockCallContainerModel() as CallContainerModel)
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/components/CameraButtonView.swift
================================================
import SwiftUI
import PipecatClientIOS
import PipecatClientIOSSmallWebrtc

struct CameraButtonView: View {
    var trackId: MediaTrackId?
    var isMuted: Bool

    var body: some View {
        GeometryReader { geometry in
            let width = geometry.size.width
            let circleSize = width * 0.9
            let innerCircleSize = width * 0.82

            ZStack {
                Circle()
                    .stroke(Color.gray, lineWidth: 1)
                    .frame(width: circleSize)
                
                if (!isMuted){
                    SmallWebRTCVideoViewSwiftUI(videoTrack: trackId, videoScaleMode: .fill)
                        .aspectRatio(1, contentMode: .fit)
                        .clipShape(Circle())
                } else {
                    Circle()
                        .fill(Color.disabledVision)
                        .frame(width: innerCircleSize)
                    Image("vision")
                        .resizable()
                        .scaledToFit()
                        .frame(width: width * 0.3)
                        .foregroundColor(.green)
                }
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity) // Ensures the ZStack is centered
        }
    }
}

#Preview {
    CameraButtonView(trackId: nil, isMuted: true)
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/components/ChatView.swift
================================================
import SwiftUI

struct ChatView: View {
    @EnvironmentObject private var model: CallContainerModel
    @State private var timer = Timer.publish(every: 0.5, on: .main, in: .common).autoconnect()
    
    var body: some View {
        VStack {
            ScrollViewReader { scrollViewProxy in
                ScrollView {
                    VStack(spacing: 10) {
                        ForEach(self.model.messages) { message in
                            MessageView(message: message)
                            .frame(maxWidth: .infinity, alignment: messageAlignment(for: message.type))
                            .padding(.horizontal)
                            .id(message.id)
                        }
                    }
                    .onChange(of: self.model.messages) { _, _ in
                        scrollToLastMessage(scrollViewProxy)
                    }
                }
                .onReceive(timer) { _ in
                    scrollToLastMessage(scrollViewProxy)
                }
                .onAppear {
                    scrollToLastMessage(scrollViewProxy)
                }
            }
        }
        .edgesIgnoringSafeArea(.bottom)
    }
    
    private func messageAlignment(for type: MessageType) -> Alignment {
        switch type {
        case .bot: return .leading
        case .user: return .trailing
        case .system: return .center
        }
    }
    
    private func scrollToLastMessage(_ scrollViewProxy: ScrollViewProxy) {
        if let lastMessageId = self.model.messages.last?.id {
            withAnimation {
                scrollViewProxy.scrollTo(lastMessageId, anchor: .bottom)
            }
        }
    }
}

struct MessageView: View {
    @ObservedObject var message: LiveMessage
    
    var body: some View {
        HStack {
            if message.type == .bot {
                Image(systemName: "gearshape")
                    .resizable()
                    .frame(width: 24, height: 24)
            }
            
            Text(message.content)
                .padding(message.type == .system ? 5 : 10)
                .foregroundColor(.white)
                .background(messageBackgroundColor(for: message.type))
                .cornerRadius(15)
                .overlay(
                    RoundedRectangle(cornerRadius: 15)
                        .stroke(Color.gray.opacity(0.5), lineWidth: 1)
                )
        }
        .padding(messagePadding(for: message.type))
    }
    
    private func messageBackgroundColor(for type: MessageType) -> Color {
        switch type {
        case .bot: return .black
        case .user: return .gray
        case .system: return .blue.opacity(0.6)
        }
    }
    
    private func messagePadding(for type: MessageType) -> EdgeInsets {
        switch type {
        case .bot: return EdgeInsets(top: 0, leading: 0, bottom: 0, trailing: 40)
        case .user: return EdgeInsets(top: 0, leading: 40, bottom: 0, trailing: 0)
        case .system: return EdgeInsets(top: 0, leading: 0, bottom: 0, trailing: 0)
        }
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = ChatView().environmentObject(mockModel as CallContainerModel)
    return result
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/components/MicrophoneView.swift
================================================
import SwiftUI

struct MicrophoneView: View {
    var audioLevel: Float // Current audio level
    var isMuted: Bool // Muted state

    var body: some View {
        GeometryReader { geometry in
            let width = geometry.size.width
            let circleSize = width * 0.9
            let innerCircleSize = width * 0.82
            let audioCircleSize = CGFloat(audioLevel) * (width * 0.95)

            ZStack {
                Circle()
                    .stroke(Color.gray, lineWidth: 1)
                    .frame(width: circleSize)

                Circle()
                    .fill(isMuted ? Color.disabledMic : Color.backgroundCircle)
                    .frame(width: innerCircleSize)

                if !isMuted {
                    Circle()
                        .fill(Color.micVolume)
                        .opacity(0.5)
                        .frame(width: audioCircleSize)
                        .animation(.easeInOut(duration: 0.2), value: audioLevel)
                }

                Image(systemName: isMuted ? "mic.slash.fill" : "mic.fill")
                    .resizable()
                    .scaledToFit()
                    .frame(width: width * 0.2)
                    .foregroundColor(.white)
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity) // Ensures the ZStack is centered
        }
    }
}

#Preview {
    MicrophoneView(audioLevel: 1, isMuted: false)
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/components/ToastModifier.swift
================================================
import SwiftUI

struct ToastModifier: ViewModifier {
    var message: String?
    var isShowing: Bool
    
    func body(content: Content) -> some View {
        ZStack {
            content
            if isShowing, let message = message {
                VStack {
                    Text(message)
                        .padding()
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(8)
                        .transition(.slide)
                        .padding(.top, 50)
                    Spacer()
                }
                .animation(.easeInOut(duration: 0.5), value: isShowing)
            }
        }
    }
}

extension View {
    func toast(message: String?, isShowing: Bool) -> some View {
        self.modifier(ToastModifier(message: message, isShowing: isShowing))
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/extensions/CustomColors.swift
================================================
import SwiftUI

public extension Color {
    
    static let backgroundCircle = Color(hex: "#374151")
    static let backgroundCircleNotConnected = Color(hex: "#D1D5DB")
    static let backgroundApp = Color(hex: "#F9FAFB")
    static let buttonsBorder = Color(hex: "#E5E7EB")
    static let micVolume = Color(hex: "#86EFAC")
    static let disabledMic = Color(hex: "#ee6b6e")
    static let disabledVision = Color(hex: "#BBF7D0")
        
    init(hex: String) {
        let scanner = Scanner(string: hex)
        _ = scanner.scanString("#")
        
        var rgb: UInt64 = 0
        scanner.scanHexInt64(&rgb)
        
        let red = Double((rgb >> 16) & 0xFF) / 255.0
        let green = Double((rgb >> 8) & 0xFF) / 255.0
        let blue = Double(rgb & 0xFF) / 255.0
        
        self.init(red: red, green: green, blue: blue)
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/settings/SettingsManager.swift
================================================
import Foundation

class SettingsManager {
    private static let preferencesKey = "settingsPreference"

    static func getSettings() -> SettingsPreference {
        if let data = UserDefaults.standard.data(forKey: preferencesKey),
           let settings = try? JSONDecoder().decode(SettingsPreference.self, from: data) {
            return settings
        } else {
            // default values in case we don't have any settings
            return SettingsPreference(enableMic: true, enableCam: true, backendURL: "http://YOUR_IP:7860", apiKey: "Authorization token")
        }
    }
    
    static func updateSettings(settings: SettingsPreference) {
        if let data = try? JSONEncoder().encode(settings) {
            UserDefaults.standard.set(data, forKey: preferencesKey)
        }
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/settings/SettingsPreference.swift
================================================
import Foundation

struct SettingsPreference: Codable {
    var selectedMic: String?
    var enableMic: Bool
    var enableCam: Bool
    var backendURL: String
    var apiKey: String
}




================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbot/views/settings/SettingsView.swift
================================================
import SwiftUI

struct SettingsView: View {
    
    @EnvironmentObject private var model: CallContainerModel
    
    @Binding var showingSettings: Bool
    
    @State private var isMicEnabled: Bool = true
    @State private var isCamEnabled: Bool = true
    @State private var backendURL: String = ""
    @State private var apiKey: String = ""
    
    var body: some View {
        NavigationView {
            Form {
                Section(header: Text("Credentials")) {
                    SecureField("API Key", text: $apiKey)
                }
                Section {
                    List(model.availableMics, id: \.self.id.id) { mic in
                        Button(action: {
                            model.selectMic(mic.id)
                        }) {
                            HStack {
                                Text(mic.name)
                                Spacer()
                                if mic.id == model.selectedMic {
                                    Image(systemName: "checkmark")
                                }
                            }
                        }
                    }
                } header: {
                    VStack(alignment: .leading) {
                        Text("Audio Settings")
                        Text("(No selection = system default)")
                    }
                }
                Section(header: Text("Start options")) {
                    Toggle("Enable Microphone", isOn: $isMicEnabled)
                    Toggle("Enable Cam", isOn: $isCamEnabled)
                }
                Section(header: Text("Server")) {
                    TextField("Backend URL", text: $backendURL)
                        .keyboardType(.URL)
                }
            }
            .navigationTitle("Settings")
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Close") {
                        self.saveSettings()
                        self.showingSettings = false
                    }
                }
            }
            .onAppear {
                self.loadSettings()
            }
        }
    }
    
    private func saveSettings() {
        let newSettings = SettingsPreference(
            selectedMic: model.selectedMic?.id,
            enableMic: isMicEnabled,
            enableCam: isCamEnabled,
            backendURL: backendURL,
            apiKey: apiKey
        )
        SettingsManager.updateSettings(settings: newSettings)
    }
    
    private func loadSettings() {
        let savedSettings = SettingsManager.getSettings()
        self.isMicEnabled = savedSettings.enableMic
        self.isCamEnabled = savedSettings.enableCam
        self.backendURL = savedSettings.backendURL
        self.apiKey = savedSettings.apiKey
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = SettingsView(showingSettings: .constant(true)).environmentObject(mockModel as CallContainerModel)
    return result
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbotTests/SimpleChatbotTests.swift
================================================
import XCTest
@testable import SimpleChatbot

final class SimpleChatbotTests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        // This is an example of a functional test case.
        // Use XCTAssert and related functions to verify your tests produce the correct results.
        // Any test you write for XCTest can be annotated as throws and async.
        // Mark your test throws to produce an unexpected failure when your test encounters an uncaught error.
        // Mark your test async to allow awaiting for asynchronous code to complete. Check the results with assertions afterwards.
    }

    func testPerformanceExample() throws {
        // This is an example of a performance test case.
        self.measure {
            // Put the code you want to measure the time of here.
        }
    }

}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbotUITests/SimpleChatbotUITests.swift
================================================
import XCTest

final class SimpleChatbotUITests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.

        // In UI tests it is usually best to stop immediately when a failure occurs.
        continueAfterFailure = false

        // In UI tests it’s important to set the initial state - such as interface orientation - required for your tests before they run. The setUp method is a good place to do this.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        // UI tests must launch the application that they test.
        let app = XCUIApplication()
        app.launch()

        // Use XCTAssert and related functions to verify your tests produce the correct results.
    }

    func testLaunchPerformance() throws {
        if #available(macOS 10.15, iOS 13.0, tvOS 13.0, watchOS 7.0, *) {
            // This measures how long it takes to launch your application.
            measure(metrics: [XCTApplicationLaunchMetric()]) {
                XCUIApplication().launch()
            }
        }
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/ios/SimpleChatbotUITests/SimpleChatbotUITestsLaunchTests.swift
================================================
import XCTest

final class SimpleChatbotUITestsLaunchTests: XCTestCase {

    override class var runsForEachTargetApplicationUIConfiguration: Bool {
        true
    }

    override func setUpWithError() throws {
        continueAfterFailure = false
    }

    func testLaunch() throws {
        let app = XCUIApplication()
        app.launch()

        // Insert steps here to perform after app launch but before taking a screenshot,
        // such as logging into a test account or navigating somewhere in the app

        let attachment = XCTAttachment(screenshot: app.screenshot())
        attachment.name = "Launch Screen"
        attachment.lifetime = .keepAlways
        add(attachment)
    }
}



================================================
FILE: p2p-webrtc/video-transform/client/react-native/README.md
================================================
# React Native implementation

Basic implementation using the [Pipecat RN SDK](https://docs.pipecat.ai/client/react-native/introduction).

## Usage

### Expo requirements

This project cannot be used with an [Expo Go](https://docs.expo.dev/workflow/expo-go/) app because [it requires custom native code](https://docs.expo.io/workflow/customizing/).

When a project requires custom native code or a config plugin, we need to transition from using [Expo Go](https://docs.expo.dev/workflow/expo-go/)
to a [development build](https://docs.expo.dev/development/introduction/).

More details about the custom native code used by this demo can be found in [rn-daily-js-expo-config-plugin](https://github.com/daily-co/rn-daily-js-expo-config-plugin).

### Building remotely

If you do not have experience with Xcode and Android Studio builds or do not have them installed locally on your computer, you will need to follow [this guide from Expo to use EAS Build](https://docs.expo.dev/development/create-development-builds/#create-and-install-eas-build).

### Building locally

You will need to have installed locally on your computer:

- [Xcode](https://developer.apple.com/xcode/) to build for iOS;
- [Android Studio](https://developer.android.com/studio) to build for Android;

#### Install the demo dependencies

```bash
# Use the version of node specified in .nvmrc
nvm i

# Install dependencies
yarn install

# Before a native app can be compiled, the native source code must be generated.
npx expo prebuild

# Copy the template and configure the environment variables
cp env.example .env
```

#### Running on Android

After plugging in an Android device [configured for debugging](https://developer.android.com/studio/debug/dev-options), run the following command:

```
npm run android
```

#### Running on iOS

Run the following command:

```
npm run ios
```

**Troubleshooting common errors:**

- If you see the error `Change your bundle identifier to a unique string to try again`, update the "Bundle Identifier" input in `Signing & Capabilities` to make it unique. This should resolve the error.

- If you see an error that says `Xcode was unable to launch because it has an invalid code signature, inadequate entitlements or its profile has not been explicitly trusted by the user`, you may need to update the settings on your iPhone to enable the required permissions as follows:

1. Open `Settings` on your iPhone
1. Select `General`, then `Device Management`
1. Click `Trust` for DailyPlayground

- You may also be prompted to enter you login keychain password. Be sure to click `Always trust` to avoid the prompt showing multiple times.


================================================
FILE: p2p-webrtc/video-transform/client/react-native/app.json
================================================
{
  "expo": {
    "name": "RN Video Transform",
    "slug": "pipecat-ai-react-native-small-webrtc-example",
    "newArchEnabled": true,
    "version": "1.0.0",
    "orientation": "portrait",
    "icon": "./assets/images/pipecat.png",
    "userInterfaceStyle": "light",
    "splash": {
      "image": "./assets/images/splash.png",
      "resizeMode": "contain",
      "backgroundColor": "#ffffff"
    },
    "updates": {
      "fallbackToCacheTimeout": 0
    },
    "assetBundlePatterns": [
      "**/*"
    ],
    "ios": {
      "supportsTablet": true,
      "bitcode": false,
      "bundleIdentifier": "co.daily.expo.SmallWebRTCDemo",
      "infoPlist": {
        "UIBackgroundModes": [
          "voip"
        ]
      }
    },
    "android": {
      "adaptiveIcon": {
        "foregroundImage": "./assets/images/pipecat.png",
        "backgroundColor": "#FFFFFF"
      },
      "package": "co.daily.expo.SmallWebRTCDemo",
      "permissions": [
        "android.permission.ACCESS_NETWORK_STATE",
        "android.permission.BLUETOOTH",
        "android.permission.CAMERA",
        "android.permission.INTERNET",
        "android.permission.MODIFY_AUDIO_SETTINGS",
        "android.permission.RECORD_AUDIO",
        "android.permission.SYSTEM_ALERT_WINDOW",
        "android.permission.WAKE_LOCK",
        "android.permission.FOREGROUND_SERVICE",
        "android.permission.FOREGROUND_SERVICE_CAMERA",
        "android.permission.FOREGROUND_SERVICE_MICROPHONE",
        "android.permission.FOREGROUND_SERVICE_MEDIA_PROJECTION",
        "android.permission.POST_NOTIFICATIONS"
      ]
    },
    "web": {
      "favicon": "./assets/images/pipecat.png"
    },
    "plugins": [
      "@daily-co/config-plugin-rn-daily-js",
      [
        "expo-build-properties",
        {
          "android": {
            "minSdkVersion": 24
          },
          "ios": {
            "deploymentTarget": "15.1"
          }
        }
      ]
    ]
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/react-native/babel.config.js
================================================
module.exports = function(api) {
  api.cache(true);
  return {
    presets: ['babel-preset-expo'],
  };
};



================================================
FILE: p2p-webrtc/video-transform/client/react-native/env.example
================================================
EXPO_PUBLIC_BASE_URL=http://$SERVER_IP:$SERVER_PORT
EXPO_PUBLIC_AUTHORIZATION_TOKEN=



================================================
FILE: p2p-webrtc/video-transform/client/react-native/index.js
================================================
// Disabling the logs from react-native-webrtc
import debug from 'debug';
debug.disable('rn-webrtc:*');

// Ignoring the warnings from react-native-background-timer while they don't fix this issue:
// https://github.com/ocetnik/react-native-background-timer/issues/366
import { LogBox } from 'react-native';
LogBox.ignoreLogs([
  '`new NativeEventEmitter()` was called with a non-null argument without the required `addListener` method.',
  '`new NativeEventEmitter()` was called with a non-null argument without the required `removeListeners` method.',
]);

import { registerRootComponent } from 'expo';

import App from './src/App';

// registerRootComponent calls AppRegistry.registerComponent('main', () => App);
// It also ensures that whether you load the app in Expo Go or in a native build,
// the environment is set up appropriately
registerRootComponent(App);



================================================
FILE: p2p-webrtc/video-transform/client/react-native/metro.config.js
================================================
const { getDefaultConfig } = require('expo/metro-config');
module.exports = getDefaultConfig(__dirname);



================================================
FILE: p2p-webrtc/video-transform/client/react-native/package.json
================================================
{
  "name": "@pipecat-ai/react-native-small-webrtc-example",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "start": "expo start --dev-client",
    "android": "expo run:android --device",
    "ios": "expo run:ios --device",
    "web": "expo start --web"
  },
  "dependencies": {
    "@daily-co/config-plugin-rn-daily-js": "0.0.11",
    "@daily-co/react-native-daily-js": "^0.82.0",
    "@daily-co/react-native-webrtc": "^124.0.6-daily.1",
    "@pipecat-ai/react-native-daily-media-manager": "^0.0.1",
    "@pipecat-ai/react-native-small-webrtc-transport": "^1.4.1",
    "@react-native-async-storage/async-storage": "1.24.0",
    "@react-navigation/native": "^7.1.19",
    "@react-navigation/stack": "^7.6.1",
    "expo": "~54.0.21",
    "expo-asset": "~12.0.9",
    "expo-status-bar": "~3.0.8",
    "react": "19.1.0",
    "react-native": "^0.81.5",
    "react-native-background-timer": "^2.4.1",
    "react-native-gesture-handler": "^2.29.0",
    "react-native-get-random-values": "^1.11.0",
    "react-native-safe-area-context": "^5.6.2",
    "react-native-toast-message": "^2.3.3"
  },
  "devDependencies": {
    "@babel/core": "^7.28.5",
    "@types/react": "19.1.0",
    "@types/react-native": "^0.73.0",
    "typescript": "~5.9.3"
  },
  "private": true,
  "resolutions": {
    "@daily-co/react-native-webrtc/debug": "^4.0.0",
    "@daily-co/react-native-webrtc/@types/react-native": "^0.73.0"
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/react-native/tsconfig.json
================================================
{
  "compilerOptions": {
    "rootDir": ".",
    "allowUnreachableCode": false,
    "allowUnusedLabels": false,
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "jsx": "react-jsx",
    "lib": [
      "ESNext"
    ],
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "noEmit": true,
    "noFallthroughCasesInSwitch": true,
    "noImplicitReturns": true,
    "noImplicitUseStrict": false,
    "noStrictGenericChecks": false,
    "noUncheckedIndexedAccess": true,
    "noUnusedLocals": false,
    "noUnusedParameters": true,
    "resolveJsonModule": true,
    "skipLibCheck": true,
    "strict": true,
    "target": "ESNext",
    "verbatimModuleSyntax": false
  },
  "extends": "expo/tsconfig.base"
}



================================================
FILE: p2p-webrtc/video-transform/client/react-native/.jshintrc
================================================
{
  "esversion": 6
}


================================================
FILE: p2p-webrtc/video-transform/client/react-native/.nvmrc
================================================
22.14



================================================
FILE: p2p-webrtc/video-transform/client/react-native/dev_scripts/clear_and_rebuild.sh
================================================
#!/bin/bash
# clear
rm yarn.lock
rm -rf .expo
rm -rf ./ios/
rm -rf ./android/
rm -rf node_modules/
# Install dependencies
yarn install
# Before a native app can be compiled, the native source code must be generated.
npx expo prebuild



================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/App.tsx
================================================
import React from "react"

import { NavigationContainer } from '@react-navigation/native';
import { createStackNavigator } from '@react-navigation/stack';
import PreJoinView from './views/PreJoinView';
import MeetingView from './views/MeetingView';
import { VoiceClientProvider } from './context/VoiceClientContext';
import Toast from 'react-native-toast-message';

import { useVoiceClientNavigation } from './hooks/useVoiceClientNavigation';

const Stack = createStackNavigator();

const NavigationManager: React.FC = () => {
  useVoiceClientNavigation();  // This hook now controls the navigation based on the connection state.
  return null; // This component doesn't render anything but manages navigation.
};

const App: React.FC = () => {
  return (
    <VoiceClientProvider>
      <NavigationContainer>
        <Stack.Navigator initialRouteName="Prejoin">
          <Stack.Screen name="Prejoin" component={PreJoinView} options={{ headerShown: false }}/>
          <Stack.Screen name="Meeting" component={MeetingView} options={{ headerShown: false }}/>
        </Stack.Navigator>
        <NavigationManager />
        <Toast />
      </NavigationContainer>
    </VoiceClientProvider>
  );
};

export default App;



================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/components/CameraButtonView.tsx
================================================
import { View, Image, StyleSheet, LayoutChangeEvent, ImageStyle, ViewStyle } from 'react-native';

import React, { useMemo, useState } from 'react';

import { Icons } from '../theme/Assets';
import Colors from '../theme/Colors';

import { useVoiceClient } from '../context/VoiceClientContext';
import { PipecatClientVideoView } from '@pipecat-ai/react-native-small-webrtc-transport';

interface CameraButtonViewProps {
  style?: ViewStyle; // Optional additional styles for the button container
}

const CameraButtonView: React.FC<CameraButtonViewProps> = ({ style }) => {
  const { localVideoTrack, isCamEnabled } = useVoiceClient();
  const [dimensions, setDimensions] = useState({ width: 0, height: 0 });

  const onLayout = (event: LayoutChangeEvent) => {
    const { width, height } = event.nativeEvent.layout;
    setDimensions({ width, height });
  };

  const mediaComponent = useMemo(() => {
    return (
      <PipecatClientVideoView
        videoTrack={localVideoTrack || null}
        audioTrack={null}
        mirror={true}
        zOrder={1}
        style={styles.media}
        objectFit="cover"
      />
    );
  }, [localVideoTrack]);

  const { width } = dimensions;
  const circleSize = width * 0.9;
  const innerCircleSize = width * 0.82;

  return (
    <View style={[styles.container, style]} onLayout={onLayout}>
      <View
        style={[
          styles.outerCircle,
          { width: circleSize, height: circleSize, borderRadius: circleSize / 2 },
        ]}
      >
        {isCamEnabled ? (
          <View style={[styles.videoView, { borderRadius: circleSize / 2 }]}>
            {mediaComponent}
          </View>
        ) : (
          <>
            <View
              style={[
                styles.innerCircle,
                {
                  width: innerCircleSize,
                  height: innerCircleSize,
                  borderRadius: innerCircleSize / 2,
                },
              ]}
            />
            <Image
              source={Icons.vision}
              style={[
                styles.image,
                {
                  width: width * 0.3,
                  height: width * 0.3,
                  tintColor: 'green',
                },
              ]}
              resizeMode="contain"
            />
          </>
        )}
      </View>
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  outerCircle: {
    borderWidth: 1,
    borderColor: Colors.buttonsBorder,
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  innerCircle: {
    backgroundColor: Colors.disabledVision,
    position: 'absolute',
  } as ViewStyle,
  videoView: {
    aspectRatio: 1,
    width: '100%',
    height: '100%',
    overflow: 'hidden',
  } as ViewStyle,
  image: {} as ImageStyle,
  media: {
    width: '100%',
    height: '100%',
    position: 'absolute',
  },
});

export default CameraButtonView;


================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/components/ChatView.tsx
================================================
import React, { useEffect, useRef } from "react";
import {
  View,
  Text,
  StyleSheet,
  FlatList,
  Image,
  ViewStyle,
  ListRenderItemInfo,
} from "react-native";

import { Images } from '../theme/Assets';

import {LiveMessage, MessageType} from "../context/VoiceClientContext";


interface ChatViewProps {
  messages: LiveMessage[];
}

const MessageView: React.FC<{ message: LiveMessage }> = ({ message }) => {
  return (
    <View style={[styles.messageContainer, messagePadding(message.type)]}>
      {message.type === "bot" && (
        <Image
          source={Images.dailyBot}
          style={styles.botIcon}
        />
      )}
      <View
        style={[
          styles.messageBubble,
          { backgroundColor: messageBackgroundColor(message.type) },
        ]}
      >
        <Text style={[styles.messageText, { color: "#fff" }]}>{message.content}</Text>
      </View>
    </View>
  );
};

export const ChatView: React.FC<ChatViewProps> = ({ messages }) => {
  const flatListRef = useRef<FlatList<LiveMessage>>(null);

  // Auto-scroll to the bottom when messages change
  useEffect(() => {
    if (messages.length > 0) {
      flatListRef.current?.scrollToEnd({ animated: true });
    }
  }, [messages]);

  const renderItem = (info: ListRenderItemInfo<LiveMessage>) => {
    const alignment: ViewStyle = {
      alignItems: messageAlignment(info.item.type),
      width: "100%",
    };
    return (
      <View style={alignment}>
        <MessageView message={info.item} />
      </View>
    );
  };

  return (
    <View style={styles.container}>
      <FlatList
        data={messages}
        renderItem={renderItem}
        keyExtractor={item => item.id}
        ref={flatListRef}
        contentContainerStyle={styles.list}
        onContentSizeChange={() => flatListRef.current?.scrollToEnd({ animated: true })}
      />
    </View>
  );
};

// Helper functions
function messageAlignment(type: MessageType): ViewStyle["alignItems"] {
  switch (type) {
    case "bot":
      return "flex-start";
    case "user":
      return "flex-end";
    case "system":
      return "center";
  }
}

function messageBackgroundColor(type: MessageType): string {
  switch (type) {
    case "bot":
      return "#101010";
    case "user":
      return "#606060";
    case "system":
      return "rgba(52, 120, 246, 0.6)";
  }
}

function messagePadding(type: MessageType): ViewStyle {
  switch (type) {
    case "bot":
      return { marginRight: 40, marginVertical: 4 };
    case "user":
      return { marginLeft: 40, marginVertical: 4 };
    case "system":
      return { marginVertical: 4 };
  }
}

// Styles
const styles = StyleSheet.create({
  container: {
    flex: 1,
    paddingBottom: 10,
    backgroundColor: "transparent",
  },
  list: {
    flexGrow: 1,
    justifyContent: "flex-end",
    paddingHorizontal: 10,
  },
  messageContainer: {
    flexDirection: "row",
    alignItems: "center",
    maxWidth: "90%",
  },
  botIcon: {
    width: 24,
    height: 24,
    marginRight: 6,
  },
  messageBubble: {
    borderRadius: 16,
    paddingVertical: 9,
    paddingHorizontal: 13,
    borderWidth: 1,
    borderColor: "rgba(128,128,128,0.5)",
  },
  messageText: {
    fontSize: 16,
  },
});

export default ChatView;


================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/components/MicrophoneView.tsx
================================================
import React, { useState, useMemo } from 'react';
import { View, StyleSheet, LayoutChangeEvent, ViewStyle } from 'react-native';
import { MaterialIcons } from '@expo/vector-icons';
import Colors from '../theme/Colors';
import { useVoiceClient } from '../context/VoiceClientContext';

interface MicrophoneViewProps {
  style?: ViewStyle;
}

const MicrophoneView: React.FC<MicrophoneViewProps> = ({ style }) => {
  const { isMicEnabled, localAudioLevel: audioLevel } = useVoiceClient();
  const [dimensions, setDimensions] = useState({ width: 0, height: 0 });

  const onLayout = (event: LayoutChangeEvent) => {
    const { width, height } = event.nativeEvent.layout;
    setDimensions({ width, height });
  };

  const { width } = dimensions;

  const circleSize = useMemo(() => width * 0.9, [width]);
  const innerCircleSize = useMemo(() => width * 0.82, [width]);
  const audioCircleSize = useMemo(() => audioLevel * width * 0.95, [audioLevel, width]);
  const iconSize = useMemo(() => width * 0.2, [width]);

  return (
    <View style={[styles.container, style]} onLayout={onLayout}>
      {width > 0 && (
        <View
          style={[
            styles.outerCircle,
            { width: circleSize, height: circleSize, borderRadius: circleSize / 2 },
          ]}
        >
          <View
            style={[
              styles.innerCircle,
              {
                backgroundColor: !isMicEnabled ? Colors.disabledMic : Colors.backgroundCircle,
                width: innerCircleSize,
                height: innerCircleSize,
                borderRadius: innerCircleSize / 2,
              },
            ]}
          />

          {isMicEnabled && audioCircleSize > 0 && (
            <View
              style={[
                styles.audioCircle,
                {
                  width: audioCircleSize,
                  height: audioCircleSize,
                  borderRadius: audioCircleSize / 2,
                },
              ]}
            />
          )}

          <MaterialIcons
            name={!isMicEnabled ? "mic-off" : "mic"}
            size={iconSize > 0 ? iconSize : 1}
            color="white"
            style={styles.micIcon}
          />
        </View>
      )}
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  outerCircle: {
    borderWidth: 1,
    borderColor: Colors.buttonsBorder,
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  innerCircle: {
    position: 'absolute',
  } as ViewStyle,
  audioCircle: {
    position: 'absolute',
    backgroundColor: Colors.micVolume,
    opacity: 0.5,
  } as ViewStyle,
  micIcon: {
    position: 'absolute',
  },
});

export default MicrophoneView;


================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/context/VoiceClientContext.tsx
================================================
import React, { createContext, useState, useContext, ReactNode, useCallback, useMemo, useRef, useEffect } from 'react'
import Toast from 'react-native-toast-message'
import {
  RNSmallWebRTCTransport,
  SmallWebRTCTransportConstructorOptions,
} from '@pipecat-ai/react-native-small-webrtc-transport';

import {
  APIRequest,
  PipecatClient,
  TransportState,
  Participant
} from '@pipecat-ai/client-js';

import { DailyMediaManager } from '@pipecat-ai/react-native-daily-media-manager';
import { MediaStreamTrack } from '@daily-co/react-native-webrtc'
import { SettingsManager } from '../settings/SettingsManager';

interface VoiceClientContextProps {
  voiceClient: PipecatClient | null
  inCall: boolean
  currentState: string
  botReady: boolean
  localAudioLevel: number
  isMicEnabled: boolean
  isCamEnabled: boolean
  localVideoTrack?: MediaStreamTrack
  remoteVideoTrack?: MediaStreamTrack
  // methods
  start: (url: string, authorizationToken:string) => Promise<void>
  leave: () => void
  toggleMicInput: () => void
  toggleCamInput: () => void,
  messages: LiveMessage[]
}

export const VoiceClientContext = createContext<VoiceClientContextProps | undefined>(undefined)

interface VoiceClientProviderProps {
  children: ReactNode
}

export type MessageType = 'bot' | 'user' | 'system'

export interface LiveMessage {
  id: string;
  content: string
  type: MessageType
  updatedAt: Date
}

export const VoiceClientProvider: React.FC<VoiceClientProviderProps> = ({ children }) => {

  const [voiceClient, setVoiceClient] = useState<PipecatClient | null>(null)
  const [inCall, setInCall] = useState<boolean>(false)
  const [currentState, setCurrentState] = useState<TransportState>("disconnected")
  const [botReady, setBotReady] = useState<boolean>(false)
  const [isMicEnabled, setIsMicEnabled] = useState<boolean>(false)
  const [isCamEnabled, setIsCamEnabled] = useState<boolean>(false)
  const [localVideoTrack, setLocalVideoTrack] = useState<MediaStreamTrack | undefined>()
  const [remoteVideoTrack, setRemoteVideoTrack] = useState<MediaStreamTrack | undefined>()
  const [localAudioLevel, setLocalAudioLevel] = useState<number>(0)
  const botSpeakingRef = useRef(false)
  // Live messages to the chat
  const [messages, setMessages] = useState<LiveMessage[]>([])

  const handleError = useCallback((error: any) => {
    console.log("Error occurred:", error)
    const errorMessage = error.message || error.data?.error || "An unexpected error occurred"
    Toast.show({
      type: 'error',
      text1: errorMessage,
    })
  }, [])

  const createVoiceClient = useCallback((): PipecatClient => {
    const inCallStates = new Set(["authenticating", "authenticated", "connecting", "connected", "ready"])
    const options: SmallWebRTCTransportConstructorOptions = {
      mediaManager: new DailyMediaManager(),
    };
    const client = new PipecatClient({
      transport: new RNSmallWebRTCTransport(options),
      enableMic: true,
      enableCam: true,
      callbacks: {
        onTransportStateChanged: (state) => {
          setCurrentState(state)
          setInCall(inCallStates.has(state))
          createLiveMessage(state, 'system')
        },
        onError: (error) => {
          handleError(error)
        },
        onBotReady: () => {
          setBotReady(true)
        },
        onDisconnected: () => {
          setBotReady(false)
          setIsMicEnabled(false)
          setIsCamEnabled(false)
        },
        onLocalAudioLevel: (level: number) => {
          setLocalAudioLevel(level)
        },
        onUserStartedSpeaking:() => {
          createLiveMessage("User started speaking", "system")
        },
        onUserStoppedSpeaking:() => {
          createLiveMessage("User stopped speaking", "system")
        },
        onUserTranscript:(data) => {
          createLiveMessage(data.text, "user")
        },
        onBotStartedSpeaking: () => {
          createLiveMessage("Bot started speaking", "system")
          botSpeakingRef.current = true
          createLiveMessage("", "bot")
        },
        onBotStoppedSpeaking: () => {
          createLiveMessage("Bot stopped speaking", "system")
          botSpeakingRef.current = false
        },
        onBotTtsText:(data) => {
          appendTextToLiveMessage(data.text)
        },
        onConnected: () => {
          setIsMicEnabled(client.isMicEnabled)
          setIsCamEnabled(client.isCamEnabled)
          client.updateMic("SPEAKERPHONE")
        },
        onTrackStarted: (track: MediaStreamTrack, p?: Participant) => {
          if (track.kind !== 'video') {
            return
          }
          if (p?.local){
            setLocalVideoTrack(track)
          } else {
            setRemoteVideoTrack(track)
          }
        },
        onTrackStopped: (track: MediaStreamTrack, p?: Participant) => {
          if (track.kind !== 'video') {
            return
          }
          if (p?.local){
            setLocalVideoTrack(undefined)
          } else {
            setRemoteVideoTrack(undefined)
          }
        }
      },
    })
    return client
  }, [handleError])

  const start = useCallback(async (url: string, authorizationToken:string): Promise<void> => {
    resetLiveMessages()
    const client = createVoiceClient()
    setVoiceClient(client)
    try {
      await client?.initDevices()
      const connectParams: APIRequest = {
        endpoint: url + '/start',
        requestData: {
          createDailyRoom: false,
          enableDefaultIceServers: true,
        },
      };
      if (authorizationToken.trim()) {
        const headers = new Headers();
        headers.append('Authorization', `Bearer ${authorizationToken}`);
        connectParams.headers = headers;
      }
      await client?.startBotAndConnect(connectParams);
      // updating the preferences
      const newSettings = await SettingsManager.getSettings();
      newSettings.backendURL = url
      newSettings.authorizationToken = authorizationToken
      await SettingsManager.updateSettings(newSettings)
    } catch (error) {
      handleError(error)
    }
  }, [createVoiceClient, handleError])

  const leave = useCallback(async (): Promise<void> => {
    if (voiceClient) {
      await voiceClient.disconnect()
      setVoiceClient(null)
    }
  }, [voiceClient])

  const toggleMicInput = useCallback(async (): Promise<void> => {
    if (voiceClient) {
      try {
        let enableMic = !isMicEnabled
        voiceClient.enableMic(enableMic)
        setIsMicEnabled(enableMic)
      } catch (e) {
        handleError(e)
      }
    }
  }, [voiceClient, isMicEnabled])

  const toggleCamInput = useCallback(async (): Promise<void> => {
    if (voiceClient) {
      try {
        let enableCam = !isCamEnabled
        voiceClient.enableCam(enableCam)
        setIsCamEnabled(enableCam)
      } catch (e) {
        handleError(e)
      }
    }
  }, [voiceClient, isCamEnabled])

  const createLiveMessage = useCallback((content: string, type: MessageType) => {
    const uniqueId = Date.now().toString(36) + Math.random().toString(36).substring(2, 9);
    const liveMessage: LiveMessage = {
      content,
      type,
      updatedAt: new Date(),
      id: uniqueId
    }
    setMessages(prev => [...prev, liveMessage])
  }, [])

  const appendTextToLiveMessage = useCallback((content: string) => {
    setMessages(prevMessages => {
      if (prevMessages.length) {
        const lastBotIndex = [...prevMessages].reverse().findIndex(msg => msg.type === "bot");
        if (lastBotIndex !== -1) {
          const realIndex = prevMessages.length - 1 - lastBotIndex;
          prevMessages[realIndex]!.content = prevMessages[realIndex]!.content + content
        }
      }
      return [...prevMessages]
    });
  }, []);

  const resetLiveMessages = useCallback(() => {
    setMessages([])
  }, [])

  useEffect(() => {
    return () => {
      if (voiceClient) {
        voiceClient.removeAllListeners() // Cleanup on unmount
        resetLiveMessages()
      }
    }
  }, [voiceClient])

  const contextValue = useMemo(() => ({
    voiceClient,
    inCall,
    currentState,
    botReady,
    isMicEnabled,
    isCamEnabled,
    localAudioLevel,
    localVideoTrack,
    remoteVideoTrack,
    start,
    leave,
    toggleMicInput,
    toggleCamInput,
    messages
  }), [voiceClient, inCall, currentState, botReady, isMicEnabled, isCamEnabled, localAudioLevel, localVideoTrack, remoteVideoTrack, start, leave, toggleMicInput, toggleCamInput, messages])

  return (
    <VoiceClientContext.Provider value={contextValue}>
      {children}
    </VoiceClientContext.Provider>
  )
}

export const useVoiceClient = (): VoiceClientContextProps => {
  const context = useContext(VoiceClientContext)
  if (!context) {
    throw new Error('useVoiceClient must be used within a VoiceClientProvider')
  }
  return context
}


================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/hooks/useVoiceClientNavigation.ts
================================================
import { useEffect } from 'react';
import { useNavigation, NavigationProp } from '@react-navigation/native';
import { useVoiceClient } from '../context/VoiceClientContext';

export type RootStackParamList = {
  Meeting: undefined;
  Prejoin: undefined;
};

export const useVoiceClientNavigation = () => {
  const navigation = useNavigation<NavigationProp<RootStackParamList>>();
  const { inCall } = useVoiceClient();

  useEffect(() => {
    if (inCall) {
      navigation.navigate('Meeting');
    } else {
      navigation.navigate('Prejoin');
    }
  }, [inCall, navigation]);

};



================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/settings/SettingsManager.ts
================================================
import AsyncStorage from '@react-native-async-storage/async-storage';

export interface SettingsManager {
  enableCam: boolean;
  enableMic: boolean;
  backendURL: string;
  authorizationToken: string;
}

// Define the settings object
const defaultSettings: SettingsManager = {
  enableCam: false,
  enableMic: true,
  backendURL: process.env.EXPO_PUBLIC_BASE_URL || "",
  authorizationToken: process.env.EXPO_PUBLIC_AUTHORIZATION_TOKEN || "",
};

export class SettingsManager {
  private static preferencesKey = 'settingsPreference';

  static async getSettings(): Promise<SettingsManager> {
    try {
      const data = await AsyncStorage.getItem(this.preferencesKey);
      if (data !== null) {
        return JSON.parse(data) as SettingsManager;
      } else {
        return defaultSettings;
      }
    } catch (error) {
      console.error("Failed to load settings:", error);
      return defaultSettings;
    }
  }

  static async updateSettings(settings: SettingsManager): Promise<void> {
    try {
      const data = JSON.stringify(settings);
      await AsyncStorage.setItem(this.preferencesKey, data);
    } catch (error) {
      console.error("Failed to save settings:", error);
    }
  }
}




================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/theme/Assets.ts
================================================
export const Images = {
  dailyBot: require('../../assets/images/pipecat.png'),
};

export const Icons = {
  vision: require('../../assets/icons/vision.png'),
};



================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/theme/Colors.ts
================================================
type ColorsType = {
  white: string;
  black: string;
  backgroundCircle: string;
  backgroundCircleNotConnected: string;
  backgroundApp: string;
  buttonsBorder: string;
  micVolume: string;
  disabledMic: string;
  disabledVision: string;
};

const Colors: ColorsType = {
  white: '#ffffff',
  black: '#000000',
  backgroundCircle: '#374151',
  backgroundCircleNotConnected: '#D1D5DB',
  backgroundApp: '#F9FAFB',
  buttonsBorder: '#E5E7EB',
  micVolume: '#86EFAC',
  disabledMic: '#ee6b6e',
  disabledVision: '#BBF7D0',
};

export default Colors;



================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/theme/CustomButton.tsx
================================================
import React from 'react';
import { TouchableOpacity, Text, StyleSheet, ViewStyle, TextStyle, GestureResponderEvent } from 'react-native';
import { MaterialIcons } from '@expo/vector-icons';

interface CustomButtonProps {
  title: string;
  onPress: (event: GestureResponderEvent) => void;
  backgroundColor?: string; // Optional prop for background color
  textColor?: string; // Optional prop for text color
  style?: ViewStyle; // Optional additional styles for the button container
  textStyle?: TextStyle; // Optional additional styles for the text
  iconName?: string; // Optional prop for the icon name from MaterialIcons
  iconPosition?: 'left' | 'right'; // Optional prop to control icon position
  iconSize?: number; // Optional prop for icon size
  iconColor?: string; // Optional prop for icon color
}

const CustomButton: React.FC<CustomButtonProps> = ({
  title,
  onPress,
  backgroundColor = 'black',
  textColor = 'white',
  style,
  textStyle,
  iconName,
  iconPosition = 'left',
  iconSize = 24,
  iconColor = 'white',
}) => {
  return (
    <TouchableOpacity
      onPress={onPress}
      style={[styles.button, { backgroundColor }, style]}>
      {iconName && iconPosition === 'left' && (
        <MaterialIcons name={iconName as keyof typeof MaterialIcons.glyphMap} size={iconSize} color={iconColor} style={styles.icon} />
      )}
      <Text style={[styles.text, { color: textColor }, textStyle]}>{title}</Text>
      {iconName && iconPosition === 'right' && (
        <MaterialIcons name={iconName as keyof typeof MaterialIcons.glyphMap} size={iconSize} color={iconColor} style={styles.icon} />
      )}
    </TouchableOpacity>
  );
};

const styles = StyleSheet.create({
  button: {
    padding: 12,
    borderRadius: 8,
    alignItems: 'center',
    justifyContent: 'center',
    flexDirection: 'row', // Ensures icon and text are aligned in a row
  },
  text: {
    fontSize: 16,
    fontWeight: 'bold',
  },
  icon: {
    marginHorizontal: 5, // Adds space between the icon and text
  },
});

export default CustomButton;



================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/views/MeetingView.tsx
================================================
import {
  View,
  StyleSheet,
  Text,
  Image,
  TouchableOpacity,
} from 'react-native';

import React, {useState} from "react"

import { useVoiceClient } from '../context/VoiceClientContext';

import { Images } from '../theme/Assets';

import MicrophoneView from '../components/MicrophoneView';
import CameraButtonView from '../components/CameraButtonView';
import { SafeAreaView } from 'react-native-safe-area-context';
import Colors from '../theme/Colors';
import CustomButton from '../theme/CustomButton';
import {PipecatClientVideoView} from "@pipecat-ai/react-native-small-webrtc-transport";
import {ChatView} from "../components/ChatView";

const MeetingView: React.FC = () => {

  const { leave, toggleMicInput, toggleCamInput, remoteVideoTrack, messages } = useVoiceClient();

  return (
    <SafeAreaView style={styles.safeArea}>
      <View style={styles.container}>
        <View style={styles.header}>
          <Image source={Images.dailyBot} style={styles.botImage} />
        </View>

        <View style={styles.mainPanel}>
          <PipecatClientVideoView
            videoTrack={remoteVideoTrack || null}
            audioTrack={null}
            style={styles.media}
            objectFit="cover"
          />

          {/* Floating chat overlay (bottom or top, adjust as needed) */}
          <View style={styles.overlay}>
            <ChatView messages={messages} />
            {/* Floating controls at the bottom */}
            <View style={styles.bottomControls}>
              <TouchableOpacity onPress={toggleMicInput}>
                <MicrophoneView style={styles.microphone} />
              </TouchableOpacity>
              <TouchableOpacity onPress={toggleCamInput}>
                <CameraButtonView style={styles.camera} />
              </TouchableOpacity>
            </View>
          </View>
        </View>

        {/* Bottom Panel */}
        <View style={styles.bottomPanel}>
          <CustomButton
            title="End"
            iconName={"exit-to-app"}
            onPress={leave}
            backgroundColor={Colors.black}
          />
        </View>
      </View>
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  safeArea: {
    flex: 1,
    width: "100%",
    backgroundColor: Colors.backgroundApp,
  },
  container: {
    flex: 1,
    padding: 20,
  },
  header: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'space-between',
    paddingBottom: 10,
  },
  botImage: {
    width: 48,
    height: 48,
  },
  mainPanel: {
    flex: 1,
    position: 'relative',       // Needed for all overlays to use as boundary
    width: '100%',
  },
  bottomControls: {
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    width: '100%',
    paddingBottom: 20,
  },
  microphone: {
    width: 160,
    height: 160,
  },
  camera: {
    width: 120,
    height: 120,
  },
  bottomPanel: {
    paddingVertical: 10,
  },
  endButton: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'center',
    backgroundColor: 'black',
    borderRadius: 12,
    padding: 10,
  },
  endText: {
    marginLeft: 5,
    color: 'white',
  },
  media: {
    ...StyleSheet.absoluteFillObject,  // Video covers all
  },
  overlay: {
    ...StyleSheet.absoluteFillObject,
    flex: 1
  },
});

export default MeetingView;


================================================
FILE: p2p-webrtc/video-transform/client/react-native/src/views/PreJoinView.tsx
================================================
import {
  View,
  StyleSheet,
  Text,
  TextInput,
  Image
} from "react-native"

import React, { useEffect, useState } from 'react';

import { useVoiceClient } from '../context/VoiceClientContext';

import Colors from '../theme/Colors';
import { Images } from '../theme/Assets';
import CustomButton from '../theme/CustomButton';
import { SettingsManager } from '../settings/SettingsManager';

const styles = StyleSheet.create({
  container: {
    flex: 1,
    padding: 20,
    backgroundColor: Colors.backgroundApp,
    justifyContent: 'center',
    alignItems: 'center',
  },
  image: {
    width: 64,
    height: 64,
    marginBottom: 20,
  },
  header: {
    fontSize: 18,
    fontWeight: 'bold',
    marginBottom: 20,
  },
  textInput: {
    width: '100%',
    padding: 10,
    borderColor: Colors.buttonsBorder,
    backgroundColor: Colors.white,
    borderWidth: 1,
    borderRadius: 5,
    marginBottom: 10,
  },
  lastTextInput: {
    marginBottom: 20,
  },
});

const PreJoinView: React.FC = () => {
  const { start } = useVoiceClient();

  const [backendURL, setBackendURL] = useState<string>('')
  const [authorizationToken, setAuthorizationToken] = useState<string>('');

  useEffect(() => {
    const loadSettings = async () => {
      const loadedSettings = await SettingsManager.getSettings();
      setBackendURL(loadedSettings.backendURL)
      setAuthorizationToken(loadedSettings.authorizationToken)
    };
    loadSettings();
  }, []);

  return (
    <View style={styles.container}>
      <Image source={Images.dailyBot} style={styles.image} />
      <Text style={styles.header}>Connect to Pipecat.</Text>
      <TextInput
        placeholder="Server URL"
        value={backendURL}
        onChangeText={setBackendURL}
        style={[styles.textInput, styles.lastTextInput]}
      />
      <TextInput
        style={[styles.textInput, styles.lastTextInput]}
        value={authorizationToken}
        onChangeText={(newToken) => {
          setAuthorizationToken(newToken);
        }}
        placeholder="Enter authorization token"
        secureTextEntry={true}
      />
      <CustomButton
        title="Connect"
        onPress={() => start(backendURL, authorizationToken)}
        backgroundColor={Colors.backgroundCircle}
      />
    </View>
  )
};

export default PreJoinView;



================================================
FILE: p2p-webrtc/video-transform/client/typescript/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the bot server. See the [server README](../../README).

2. Navigate to the `client/typescript` directory:

```bash
cd client/typescript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: p2p-webrtc/video-transform/client/typescript/index.html
================================================
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>WebRTC demo</title>
  </head>
  <body>
    <div class="container">
      <!-- Status Bar -->
      <div class="status-bar">
        <div class="status">
          Status: <span id="connection-status">Disconnected</span>
        </div>
        <div class="controls">
          <button id="connect-btn">Connect</button>
          <button id="disconnect-btn" disabled>Disconnect</button>
        </div>
      </div>

      <!-- Main Content -->
      <div class="main-content">
        <div class="bot-container">
          <div id="bot-video-container">
            <video id="bot-video" autoplay="true" playsinline="true"></video>
          </div>
          <audio id="bot-audio" autoplay></audio>
        </div>
        <!-- Debug Panel -->
        <div class="debug-panel">
          <div id="debug-log"></div>
        </div>
      </div>

      <!-- Settings Bar -->
      <div class="status-bar">
        <div class="option">
          <label>Audio</label>
          <select id="audio-input">
            <option value="" selected>Default device</option>
          </select>
          <select id="audio-codec">
            <option value="default" selected>Default codecs</option>
            <option value="opus/48000/2">Opus</option>
            <option value="PCMU/8000">PCMU</option>
            <option value="PCMA/8000">PCMA</option>
          </select>
          <button id="mute-mic">Unmute Mic</button>
        </div>
        <div class="option">
          <label>Video</label>
          <select id="video-input">
            <option value="" selected>Default device</option>
          </select>
          <select id="video-codec">
            <option value="default" selected>Default codecs</option>
            <option value="VP8/90000">VP8</option>
            <option value="H264/90000">H264</option>
          </select>
        </div>
      </div>

      <!-- Local feeds -->
      <div class="main-content">
        <div class="bot-container">
          <div id="bot-video-container">
            <video id="local-cam" autoplay="true" playsinline="true"></video>
            <button id="mute-btn">
              <div id="cam-icon">📷</div>
              <div id="cam-x">❌</div>
            </button>
          </div>
          <audio id="bot-audio" autoplay></audio>
        </div>
        <!-- Debug Panel -->
        <div class="bot-container">
          <div id="bot-video-container">
            <video id="local-screen" autoplay="true" playsinline="true"></video>
            <button id="screen-btn">
              <div id="screen-icon">🖥️</div>
              <div id="screen-x">❌</div>
            </button>
          </div>
        </div>
      </div>
    </div>

    <script type="module" src="/src/app.ts"></script>
    <link rel="stylesheet" href="/src/style.css" />
  </body>
</html>



================================================
FILE: p2p-webrtc/video-transform/client/typescript/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "node_modules/.bin/vite",
    "build": "node_modules/.bin/tsc && vite build",
    "preview": "node_modules/.bin/vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.13.1",
    "@vitejs/plugin-react-swc": "^3.7.2",
    "typescript": "^5.7.3",
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/small-webrtc-transport": "^1.3.0"
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/typescript/tsconfig.json
================================================
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */

    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */

    /* Language and Environment */
    "target": "es2016",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for legacy experimental decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */

    /* Modules */
    "module": "commonjs",                                /* Specify what module code is generated. */
    // "rootDir": "./",                                  /* Specify the root folder within your source files. */
    // "moduleResolution": "node10",                     /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "allowImportingTsExtensions": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */
    // "rewriteRelativeImportExtensions": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */
    // "resolvePackageJsonExports": true,                /* Use the package.json 'exports' field when resolving package imports. */
    // "resolvePackageJsonImports": true,                /* Use the package.json 'imports' field when resolving imports. */
    // "customConditions": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */
    // "noUncheckedSideEffectImports": true,             /* Check side effect imports. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "allowArbitraryExtensions": true,                 /* Enable importing files with any extension, provided a declaration file is present. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */

    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */

    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    // "outDir": "./",                                   /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */

    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "verbatimModuleSyntax": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */
    // "isolatedDeclarations": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true,            /* Ensure that casing is correct in imports. */

    /* Type Checking */
    "strict": true,                                      /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "strictBuiltinIteratorReturn": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */

    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true                                 /* Skip type checking all .d.ts files. */
  }
}



================================================
FILE: p2p-webrtc/video-transform/client/typescript/vite.config.js
================================================
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

export default defineConfig({
    plugins: [react()],
    server: {
        allowedHosts: true, // Allows external connections like ngrok
        proxy: {
            // Proxy /api requests to the backend server
            '/api': {
                target: 'http://0.0.0.0:7860', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: p2p-webrtc/video-transform/client/typescript/src/app.ts
================================================
import { SmallWebRTCTransport } from '@pipecat-ai/small-webrtc-transport';
import {
  BotLLMTextData,
  Participant,
  PipecatClient,
  PipecatClientOptions,
  TranscriptData,
  TransportState,
} from '@pipecat-ai/client-js';

class WebRTCApp {
  private declare connectBtn: HTMLButtonElement;
  private declare disconnectBtn: HTMLButtonElement;
  private declare micBtn: HTMLButtonElement;
  private declare muteBtn: HTMLButtonElement;
  private declare screenBtn: HTMLButtonElement;

  private declare audioInput: HTMLSelectElement;
  private declare videoInput: HTMLSelectElement;
  private declare audioCodec: HTMLSelectElement;
  private declare videoCodec: HTMLSelectElement;

  private declare botVideoElement: HTMLVideoElement;
  private declare botAudioElement: HTMLAudioElement;

  private declare localCamElement: HTMLVideoElement;
  private declare localScreenElement: HTMLVideoElement;

  private debugLog: HTMLElement | null = null;
  private statusSpan: HTMLElement | null = null;

  private declare smallWebRTCTransport: SmallWebRTCTransport;
  private declare pcClient: PipecatClient;

  constructor() {
    this.setupDOMElements();
    this.setupDOMEventListeners();
    this.initializePipecatClient();
    void this.populateDevices();
  }

  private initializePipecatClient(): void {
    const opts: PipecatClientOptions = {
      transport: new SmallWebRTCTransport({ webrtcUrl: '/api/offer' }),
      enableMic: true,
      enableCam: true,
      callbacks: {
        onTransportStateChanged: (state: TransportState) => {
          this.log(`Transport state: ${state}`);
        },
        onConnected: () => {
          this.onConnectedHandler();
        },
        onBotReady: () => {
          this.log('Bot is ready.');
        },
        onDisconnected: () => {
          this.onDisconnectedHandler();
        },
        onUserStartedSpeaking: () => {
          this.log('User started speaking.');
        },
        onUserStoppedSpeaking: () => {
          this.log('User stopped speaking.');
        },
        onBotStartedSpeaking: () => {
          this.log('Bot started speaking.');
        },
        onBotStoppedSpeaking: () => {
          this.log('Bot stopped speaking.');
        },
        onUserTranscript: (transcript: TranscriptData) => {
          if (transcript.final) {
            this.log(`User transcript: ${transcript.text}`);
          }
        },
        onBotTranscript: (data: BotLLMTextData) => {
          this.log(`Bot transcript: ${data.text}`);
        },
        onTrackStarted: (
          track: MediaStreamTrack,
          participant?: Participant
        ) => {
          if (participant?.local) {
            this.onLocalTrackStarted(track);
          } else {
            this.onBotTrackStarted(track);
          }
        },
        onTrackStopped: (
          track: MediaStreamTrack,
          participant?: Participant
        ) => {
          if (participant?.local) {
            this.onLocalTrackStopped(track);
          }
        },
        onServerMessage: (msg: unknown) => {
          this.log(`Server message: ${msg}`);
        },
      },
    };
    this.pcClient = new PipecatClient(opts);
    // @ts-ignore
    window.webapp = this;
    // @ts-ignore
    window.client = this.pcClient; // Expose client for debugging
    this.smallWebRTCTransport = this.pcClient.transport as SmallWebRTCTransport;
  }

  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      'connect-btn'
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      'disconnect-btn'
    ) as HTMLButtonElement;
    this.micBtn = document.getElementById('mute-mic') as HTMLButtonElement;
    this.muteBtn = document.getElementById('mute-btn') as HTMLButtonElement;
    this.screenBtn = document.getElementById('screen-btn') as HTMLButtonElement;

    this.audioInput = document.getElementById(
      'audio-input'
    ) as HTMLSelectElement;
    this.videoInput = document.getElementById(
      'video-input'
    ) as HTMLSelectElement;
    this.audioCodec = document.getElementById(
      'audio-codec'
    ) as HTMLSelectElement;
    this.videoCodec = document.getElementById(
      'video-codec'
    ) as HTMLSelectElement;

    this.botVideoElement = document.getElementById(
      'bot-video'
    ) as HTMLVideoElement;
    this.botAudioElement = document.getElementById(
      'bot-audio'
    ) as HTMLAudioElement;

    this.localCamElement = document.getElementById(
      'local-cam'
    ) as HTMLVideoElement;
    this.localScreenElement = document.getElementById(
      'local-screen'
    ) as HTMLVideoElement;

    this.debugLog = document.getElementById('debug-log');
    this.statusSpan = document.getElementById('connection-status');
  }

  private setupDOMEventListeners(): void {
    this.connectBtn.addEventListener('click', () => this.start());
    this.disconnectBtn.addEventListener('click', () => this.stop());
    this.audioInput.addEventListener('change', (e) => {
      // @ts-ignore
      let audioDevice = e.target?.value;
      this.pcClient.updateMic(audioDevice);
    });
    this.micBtn.addEventListener('click', async () => {
      if (this.pcClient.state === 'disconnected') {
        await this.pcClient.initDevices();
      } else {
        let isMicEnabled = this.pcClient.isMicEnabled;
        this.pcClient.enableMic(!isMicEnabled);
      }
    });
    this.videoInput.addEventListener('change', (e) => {
      // @ts-ignore
      let videoDevice = e.target?.value;
      this.pcClient.updateCam(videoDevice);
    });
    this.muteBtn.addEventListener('click', async () => {
      if (this.pcClient.state === 'disconnected') {
        await this.pcClient.initDevices();
      } else {
        let isCamEnabled = this.pcClient.isCamEnabled;
        this.pcClient.enableCam(!isCamEnabled);
      }
    });
    this.screenBtn.addEventListener('click', async () => {
      if (this.pcClient.state === 'disconnected') {
        await this.pcClient.initDevices();
      }
      let isScreenEnabled = this.pcClient.isSharingScreen;
      this.pcClient.enableScreenShare(!isScreenEnabled);
    });
  }

  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3';
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50';
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
  }

  private clearAllLogs() {
    this.debugLog!.innerText = '';
  }

  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  private onConnectedHandler() {
    this.updateStatus('Connected');
    if (this.connectBtn) this.connectBtn.disabled = true;
    if (this.disconnectBtn) this.disconnectBtn.disabled = false;
  }

  private onDisconnectedHandler() {
    this.updateStatus('Disconnected');
    if (this.connectBtn) this.connectBtn.disabled = false;
    if (this.disconnectBtn) this.disconnectBtn.disabled = true;
  }

  private onLocalTrackStarted(track: MediaStreamTrack) {
    if (track.kind === 'audio') {
      this.micBtn.innerHTML = 'Mute Mic';
      return;
    }

    const settings = track.getSettings();
    // ... Because Firefox 😡
    interface FirefoxConstraints extends MediaTrackConstraints {
      mediaSource?: string;
    }
    const constraints = track.getConstraints() as FirefoxConstraints;
    const screenShareOpts = ['window', 'monitor', 'browser'];
    if (
      screenShareOpts.includes(settings?.displaySurface ?? '') ||
      screenShareOpts.includes(constraints?.mediaSource ?? '')
    ) {
      this.localScreenElement.srcObject = new MediaStream([track]);
      (document.getElementById('screen-x') as HTMLDivElement).hidden = true;
    } else {
      this.localCamElement.srcObject = new MediaStream([track]);
      (document.getElementById('cam-x') as HTMLDivElement).hidden = true;
    }
  }

  private onBotTrackStarted(track: MediaStreamTrack) {
    if (track.kind === 'video') {
      this.botVideoElement.srcObject = new MediaStream([track]);
    } else {
      this.botAudioElement.srcObject = new MediaStream([track]);
    }
  }

  private onLocalTrackStopped(track: MediaStreamTrack) {
    if (track.kind === 'audio') {
      this.micBtn.innerHTML = 'Unmute Mic';
      return;
    }

    const settings = track.getSettings();
    // ... Because Firefox 😡
    interface FirefoxConstraints extends MediaTrackConstraints {
      mediaSource?: string;
    }
    const constraints = track.getConstraints() as FirefoxConstraints;
    const screenShareOpts = ['window', 'monitor', 'browser'];
    if (
      screenShareOpts.includes(settings?.displaySurface ?? '') ||
      screenShareOpts.includes(constraints?.mediaSource ?? '')
    ) {
      this.localScreenElement.srcObject = null;
      (document.getElementById('screen-x') as HTMLDivElement).hidden = false;
    } else {
      this.localCamElement.srcObject = null;
      (document.getElementById('cam-x') as HTMLDivElement).hidden = false;
    }
  }

  private async populateDevices(): Promise<void> {
    const populateSelect = (
      select: HTMLSelectElement,
      devices: MediaDeviceInfo[]
    ): void => {
      let counter = 1;
      devices.forEach((device) => {
        const option = document.createElement('option');
        option.value = device.deviceId;
        option.text = device.label || 'Device #' + counter;
        select.appendChild(option);
        counter += 1;
      });
    };

    try {
      const audioDevices = await this.pcClient.getAllMics();
      populateSelect(this.audioInput, audioDevices);
      const videoDevices = await this.pcClient.getAllCams();
      populateSelect(this.videoInput, videoDevices);
    } catch (e) {
      alert(e);
    }
  }

  private async start(): Promise<void> {
    this.clearAllLogs();

    this.connectBtn.disabled = true;
    this.updateStatus('Connecting');

    this.smallWebRTCTransport.setAudioCodec(this.audioCodec.value);
    this.smallWebRTCTransport.setVideoCodec(this.videoCodec.value);
    try {
      await this.pcClient.connect();
    } catch (e) {
      console.log(`Failed to connect ${e}`);
      this.stop();
    }
  }

  private stop(): void {
    void this.pcClient.disconnect();
  }
}

// Create the WebRTCConnection instance
const webRTCConnection = new WebRTCApp();



================================================
FILE: p2p-webrtc/video-transform/client/typescript/src/style.css
================================================
body {
  margin: 0;
  padding: 20px;
  font-family: Arial, sans-serif;
  background-color: #f0f0f0;
  display: flex;
  flex-direction: row;
  width: 100%;
}

.container {
  margin: 0 auto;
  width: 90%;
}

.option {
  display: flex;
  flex-direction: row;
  align-items: center;
}

label {
  margin: 5px;
}

select {
  padding: 8px;
  margin: 10px;
  border-radius: 4px;
  border: 1px solid #ccc;
}

#mute-mic {
  padding: 8px;
  margin: 10px;
  border-radius: 4px;
  border: 1px solid #ccc;
  background-color: white;
  cursor: pointer;
}

.status-bar {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  align-items: center;
  padding: 10px;
  background-color: #fff;
  border-radius: 8px;
  margin-bottom: 20px;
}

.controls button {
  padding: 8px 16px;
  margin-left: 10px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

#connect-btn {
  background-color: #4caf50;
  color: white;
}

#disconnect-btn {
  background-color: #f44336;
  color: white;
}

button:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.main-content {
  background-color: #fff;
  border-radius: 8px;
  padding: 20px;
  margin-bottom: 20px;
  display: flex;
  flex-wrap: wrap;
}

.bot-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  width: 50%;
}

#bot-video-container {
  width: 90%;
  aspect-ratio: 16 / 9;
  background-color: #e0e0e0;
  border-radius: 8px;
  overflow: hidden;
  display: flex;
  align-items: center;
  justify-content: center;
  position: relative;
}

#bot-video-container video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}

#mute-btn,
#screen-btn {
  position: absolute;
  bottom: 10px;
  right: 10px;
  background-color: rgba(0, 0, 0, 0.6);
  color: white;
  border: none;
  border-radius: 20px;
  padding: 8px 12px;
  cursor: pointer;
  font-size: 16px;
  z-index: 1;
}

#cam-icon,
#screen-icon {
  transform: scale(2) translateX(2px) translateY(-1px);
}

#cam-x,
#screen-x {
  position: absolute;
  bottom: 10px;
  right: 9px;
  color: rgba(0, 0, 0, 0.8);
  transform: scale(2);
}
.debug-panel {
  background-color: #fff;
  border-radius: 8px;
  width: 50%;
}

@media (max-width: 768px) {
  .bot-container {
    width: 100%;
  }
  .debug-panel {
    width: 100%;
  }
}

.debug-panel h3 {
  margin: 0 0 10px 0;
  font-size: 16px;
  font-weight: bold;
}

#debug-log {
  height: 360px;
  overflow-y: auto;
  background-color: #f8f8f8;
  border-radius: 4px;
  font-family: monospace;
  font-size: 12px;
  line-height: 1.4;
}

#local-cam {
  transform: scale(-1, 1);
}



================================================
FILE: p2p-webrtc/video-transform/server/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import os

import cv2
import numpy as np
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import Frame, InputImageRawFrame, LLMRunFrame, OutputImageRawFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)


class EdgeDetectionProcessor(FrameProcessor):
    def __init__(self, video_out_width, video_out_height: int):
        super().__init__()
        self._video_out_width = video_out_width
        self._video_out_height = video_out_height

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        # Send back the user's camera video with edge detection applied
        if isinstance(frame, InputImageRawFrame) and frame.transport_source == "camera":
            # Convert bytes to NumPy array
            img = np.frombuffer(frame.image, dtype=np.uint8).reshape(
                (frame.size[1], frame.size[0], 3)
            )

            # perform edge detection only on camera frames
            img = cv2.cvtColor(cv2.Canny(img, 100, 200), cv2.COLOR_GRAY2BGR)

            # convert the size if needed
            desired_size = (self._video_out_width, self._video_out_height)
            if frame.size != desired_size:
                resized_image = cv2.resize(img, desired_size)
                out_frame = OutputImageRawFrame(resized_image.tobytes(), desired_size, frame.format)
                await self.push_frame(out_frame)
            else:
                out_frame = OutputImageRawFrame(
                    image=img.tobytes(), size=frame.size, format=frame.format
                )
                await self.push_frame(out_frame)
        else:
            await self.push_frame(frame, direction)


SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


async def run_bot(webrtc_connection):
    transport_params = TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        audio_out_10ms_chunks=2,
        video_in_enabled=True,
        video_out_enabled=True,
        video_out_is_live=True,
        vad_analyzer=SileroVADAnalyzer(),
    )

    pipecat_transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection, params=transport_params
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_user_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
    )

    messages = [
        {
            "role": "user",
            "content": "Start by greeting the user warmly and introducing yourself.",
        }
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor()

    pipeline = Pipeline(
        [
            pipecat_transport.input(),
            context_aggregator.user(),
            rtvi,
            llm,  # LLM
            EdgeDetectionProcessor(
                transport_params.video_out_width, transport_params.video_out_height
            ),  # Sending the video back to the user
            pipecat_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.info("Pipecat client ready.")
        await rtvi.set_bot_ready()
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @pipecat_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")
        await pipecat_transport.capture_participant_video("camera")
        await pipecat_transport.capture_participant_video("screenVideo")

    @pipecat_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)



================================================
FILE: p2p-webrtc/video-transform/server/env.example
================================================
GOOGLE_API_KEY=


================================================
FILE: p2p-webrtc/video-transform/server/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
aiortc
opencv-python
pipecat-ai[google,silero,webrtc]>=0.0.82
pipecat-ai-small-webrtc-prebuilt


================================================
FILE: p2p-webrtc/video-transform/server/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import sys
import uuid
from contextlib import asynccontextmanager
from http import HTTPMethod
from typing import Any, Dict, List, Optional, TypedDict, Union

import uvicorn
from bot import run_bot
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, Request, Response
from fastapi.responses import RedirectResponse
from loguru import logger
from pipecat.transports.smallwebrtc.request_handler import (
    IceCandidate,
    SmallWebRTCPatchRequest,
    SmallWebRTCRequest,
    SmallWebRTCRequestHandler,
)
from pipecat_ai_small_webrtc_prebuilt.frontend import SmallWebRTCPrebuiltUI

# Load environment variables
load_dotenv(override=True)

app = FastAPI()

# Mount the frontend at /
app.mount("/prebuilt", SmallWebRTCPrebuiltUI)

# Initialize the SmallWebRTC request handler
small_webrtc_handler: SmallWebRTCRequestHandler = SmallWebRTCRequestHandler()

# In-memory store of active sessions: session_id -> session info
active_sessions: Dict[str, Dict[str, Any]] = {}


@app.get("/", include_in_schema=False)
async def root_redirect():
    return RedirectResponse(url="/prebuilt/")


@app.post("/api/offer")
async def offer(request: SmallWebRTCRequest, background_tasks: BackgroundTasks):
    """Handle WebRTC offer requests via SmallWebRTCRequestHandler."""

    # Prepare runner arguments with the callback to run your bot
    async def webrtc_connection_callback(connection):
        background_tasks.add_task(run_bot, connection)

    # Delegate handling to SmallWebRTCRequestHandler
    answer = await small_webrtc_handler.handle_web_request(
        request=request,
        webrtc_connection_callback=webrtc_connection_callback,
    )
    return answer


@app.patch("/api/offer")
async def ice_candidate(request: SmallWebRTCPatchRequest):
    """Handle WebRTC new ice candidate requests."""
    logger.debug(f"Received patch request: {request}")
    await small_webrtc_handler.handle_patch_request(request)
    return {"status": "success"}


@app.post("/start")
async def rtvi_start(request: Request):
    """Mimic Pipecat Cloud's /start endpoint."""

    class IceServer(TypedDict, total=False):
        urls: Union[str, List[str]]

    class IceConfig(TypedDict):
        iceServers: List[IceServer]

    class StartBotResult(TypedDict, total=False):
        sessionId: str
        iceConfig: Optional[IceConfig]

    # Parse the request body
    try:
        request_data = await request.json()
        logger.debug(f"Received request: {request_data}")
    except Exception as e:
        logger.error(f"Failed to parse request body: {e}")
        request_data = {}

    # Store session info immediately in memory, replicate the behavior expected on Pipecat Cloud
    session_id = str(uuid.uuid4())
    active_sessions[session_id] = request_data

    result: StartBotResult = {"sessionId": session_id}
    if request_data.get("enableDefaultIceServers"):
        result["iceConfig"] = IceConfig(
            iceServers=[IceServer(urls=["stun:stun.l.google.com:19302"])]
        )

    return result


@app.api_route(
    "/sessions/{session_id}/{path:path}",
    methods=["GET", "POST", "PUT", "PATCH", "DELETE"],
)
async def proxy_request(
    session_id: str, path: str, request: Request, background_tasks: BackgroundTasks
):
    """Mimic Pipecat Cloud's proxy."""
    active_session = active_sessions.get(session_id)
    if active_session is None:
        return Response(content="Invalid or not-yet-ready session_id", status_code=404)

    if path.endswith("api/offer"):
        # Parse the request body and convert to SmallWebRTCRequest
        try:
            request_data = await request.json()
            if request.method == HTTPMethod.POST.value:
                webrtc_request = SmallWebRTCRequest(
                    sdp=request_data["sdp"],
                    type=request_data["type"],
                    pc_id=request_data.get("pc_id"),
                    restart_pc=request_data.get("restart_pc"),
                    request_data=request_data,
                )
                return await offer(webrtc_request, background_tasks)
            elif request.method == HTTPMethod.PATCH.value:
                patch_request = SmallWebRTCPatchRequest(
                    pc_id=request_data["pc_id"],
                    candidates=[IceCandidate(**c) for c in request_data.get("candidates", [])],
                )
                return await ice_candidate(patch_request)
        except Exception as e:
            logger.error(f"Failed to parse WebRTC request: {e}")
            return Response(content="Invalid WebRTC request", status_code=400)

    logger.info(f"Received request for path: {path}")
    return Response(status_code=200)


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app
    await small_webrtc_handler.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WebRTC demo")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="TRACE")
    else:
        logger.add(sys.stderr, level="DEBUG")

    uvicorn.run(app, host=args.host, port=args.port)



================================================
FILE: p2p-webrtc/voice-agent/README.md
================================================
# Voice Agent

A Pipecat example demonstrating the simplest way to create a voice agent using `SmallWebRTCTransport`.

## 🚀 Quick Start

### 1️⃣ Start the Bot Server

#### 🔧 Set Up the Environment
1. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure environment variables:
   - Copy `env.example` to `.env`
   ```bash
   cp env.example .env
   ```
   - Add your API keys

#### ▶️ Run the Server
```bash
python server.py
```

### 2️⃣ Connect Using the Client App

Open your browser and visit:
```
http://localhost:7860
```

## 📌 Requirements

- Python **3.10+**
- Node.js **16+** (for JavaScript components)
- Google API Key
- Modern web browser with WebRTC support

---

## WebRTC ICE Servers Configuration

When implementing WebRTC in your project, **STUN** (Session Traversal Utilities for NAT) and **TURN** (Traversal Using Relays around NAT) 
servers are usually needed in cases where users are behind routers or firewalls.

In local networks (e.g., testing within the same home or office network), you usually don’t need to configure STUN or TURN servers. 
In such cases, WebRTC can often directly establish peer-to-peer connections without needing to traverse NAT or firewalls.

### What are STUN and TURN Servers?

- **STUN Server**: Helps clients discover their public IP address and port when they're behind a NAT (Network Address Translation) device (like a router). 
This allows WebRTC to attempt direct peer-to-peer communication by providing the public-facing IP and port.
  
- **TURN Server**: Used as a fallback when direct peer-to-peer communication isn't possible due to strict NATs or firewalls blocking connections. 
The TURN server relays media traffic between peers.

### Why are ICE Servers Important?

**ICE (Interactive Connectivity Establishment)** is a framework used by WebRTC to handle network traversal and NAT issues. 
The `iceServers` configuration provides a list of **STUN** and **TURN** servers that WebRTC uses to find the best way to connect two peers. 

### Example Configuration for ICE Servers

Here’s how you can configure a basic `iceServers` object in WebRTC for testing purposes, using Google's public STUN server:

```javascript
const config = {
  iceServers: [
    {
      urls: ["stun:stun.l.google.com:19302"], // Google's public STUN server
    }
  ],
};
```

> For testing purposes, you can either use public **STUN** servers (like Google's) or set up your own **TURN** server. 
If you're running your own TURN server, make sure to include your server URL, username, and credential in the configuration.

---

### 💡 Notes
- Ensure all dependencies are installed before running the server.
- Check the `.env` file for missing configurations.
- WebRTC requires a secure environment (HTTPS) for full functionality in production.

Happy coding! 🎉


================================================
FILE: p2p-webrtc/voice-agent/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)

SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


async def run_bot(webrtc_connection):
    pipecat_transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            audio_out_10ms_chunks=2,
        ),
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_user_audio=True,
        transcribe_model_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
    )

    context = LLMContext(
        [
            {
                "role": "user",
                "content": "Start by greeting the user warmly and introducing yourself.",
            }
        ],
    )
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            pipecat_transport.input(),
            context_aggregator.user(),
            llm,  # LLM
            pipecat_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @pipecat_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @pipecat_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)



================================================
FILE: p2p-webrtc/voice-agent/env.example
================================================
GOOGLE_API_KEY=


================================================
FILE: p2p-webrtc/voice-agent/index.html
================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>WebRTC Voice Agent</title>
    <style>
        body { font-family: Arial, sans-serif; text-align: center; margin-top: 50px; }
        #status { font-size: 20px; margin: 20px; }
        button { padding: 10px 20px; font-size: 16px; }
    </style>
</head>
<body>
    <h1>WebRTC Voice Agent</h1>
    <p id="status">Disconnected</p>
    <button id="connect-btn">Connect</button>
    <audio id="audio-el" autoplay></audio>

    <script>
        const statusEl = document.getElementById("status")
        const buttonEl = document.getElementById("connect-btn")
        const audioEl = document.getElementById("audio-el")

        let connected = false
        let peerConnection = null

        const sendIceCandidate = async (pc, candidate) => {
            await fetch('/api/offer', {
              method: "PATCH",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                pc_id: pc.pc_id,
                candidates:[{
                    candidate: candidate.candidate,
                    sdp_mid: candidate.sdpMid,
                    sdp_mline_index: candidate.sdpMLineIndex
                }]
              })
            });
        };

        const createSmallWebRTCConnection = async (audioTrack) => {
            const config = {
              iceServers:[
                {
                  urls:"stun:stun.l.google.com:19302",
                }
              ]
            };
            const pc = new RTCPeerConnection(config)
            
            // Queue to store ICE candidates until we have received the answer and have a session in progress
            pc.pendingIceCandidates = []
            pc.canSendIceCandidates = false
            
            addPeerConnectionEventListeners(pc)
            pc.ontrack = e => audioEl.srcObject = e.streams[0]
            // SmallWebRTCTransport expects to receive both transceivers
            pc.addTransceiver(audioTrack, { direction: 'sendrecv' })
            pc.addTransceiver('video', { direction: 'sendrecv' })
            await pc.setLocalDescription(await pc.createOffer())
            const offer = pc.localDescription
            const response = await fetch('/api/offer', {
                body: JSON.stringify({ sdp: offer.sdp, type: offer.type}),
                headers: { 'Content-Type': 'application/json' },
                method: 'POST',
            });
            const answer = await response.json()
            pc.pc_id = answer.pc_id
            await pc.setRemoteDescription(answer)
            
            // Now we can send ICE candidates
            pc.canSendIceCandidates = true
            
            // Send any queued ICE candidates
            for (const candidate of pc.pendingIceCandidates) {
                await sendIceCandidate(pc, candidate)
            }
            pc.pendingIceCandidates = []
            
            return pc
        }

        const connect = async () => {
            _onConnecting()
            const audioStream = await navigator.mediaDevices.getUserMedia({audio: true})
            peerConnection= await createSmallWebRTCConnection(audioStream.getAudioTracks()[0])
        }

        const addPeerConnectionEventListeners = (pc) => {
            pc.oniceconnectionstatechange = () => {
                console.log("oniceconnectionstatechange", pc?.iceConnectionState)
            }
            pc.onconnectionstatechange = () => {
                console.log("onconnectionstatechange", pc?.connectionState)
                let connectionState = pc?.connectionState
                if (connectionState === 'connected') {
                    _onConnected()
                } else if (connectionState === 'disconnected') {
                    _onDisconnected()
                }
            }
            pc.onicecandidate = async (event) => {
                if (event.candidate) {
                    console.log("New ICE candidate:", event.candidate);
                    // Check if we can send ICE candidates (we have received the answer with pc_id)
                    if (pc.canSendIceCandidates && pc.pc_id) {
                        // Send immediately
                        await sendIceCandidate(pc, event.candidate)
                    } else {
                        // Queue the candidate until we have pc_id
                        pc.pendingIceCandidates.push(event.candidate)
                    }
                } else {
                    console.log("All ICE candidates have been sent.");
                }
            };
        }

        const _onConnecting = () => {
            statusEl.textContent = "Connecting"
            buttonEl.textContent = "Disconnect"
            connected = true
        }

        const _onConnected = () => {
            statusEl.textContent = "Connected"
            buttonEl.textContent = "Disconnect"
            connected = true
        }

        const _onDisconnected = () => {
            statusEl.textContent = "Disconnected"
            buttonEl.textContent = "Connect"
            connected = false
        }

        const disconnect = () => {
            if (!peerConnection) {
                return
            }
            peerConnection.close()
            peerConnection = null
            _onDisconnected()
        }

        buttonEl.addEventListener("click", async () => {
            if (!connected) {
                await connect()
            } else {
                disconnect()
            }
        });
    </script>
</body>
</html>


================================================
FILE: p2p-webrtc/voice-agent/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
pipecat-ai[google,silero, webrtc]>=0.0.82


================================================
FILE: p2p-webrtc/voice-agent/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import sys
from contextlib import asynccontextmanager

import uvicorn
from bot import run_bot
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, Request
from fastapi.responses import FileResponse
from loguru import logger
from pipecat.transports.smallwebrtc.request_handler import (
    SmallWebRTCPatchRequest,
    SmallWebRTCRequest,
    SmallWebRTCRequestHandler,
)

# Load environment variables
load_dotenv(override=True)

app = FastAPI()

# Initialize the SmallWebRTC request handler
small_webrtc_handler: SmallWebRTCRequestHandler = SmallWebRTCRequestHandler()


@app.post("/api/offer")
async def offer(request: SmallWebRTCRequest, background_tasks: BackgroundTasks):
    """Handle WebRTC offer requests via SmallWebRTCRequestHandler."""

    # Prepare runner arguments with the callback to run your bot
    async def webrtc_connection_callback(connection):
        background_tasks.add_task(run_bot, connection)

    # Delegate handling to SmallWebRTCRequestHandler
    answer = await small_webrtc_handler.handle_web_request(
        request=request,
        webrtc_connection_callback=webrtc_connection_callback,
    )
    return answer


@app.patch("/api/offer")
async def ice_candidate(request: SmallWebRTCPatchRequest):
    logger.debug(f"Received patch request: {request}")
    await small_webrtc_handler.handle_patch_request(request)
    return {"status": "success"}


@app.get("/")
async def serve_index():
    return FileResponse("index.html")


@asynccontextmanager
async def lifespan(app: FastAPI):
    yield  # Run app
    await small_webrtc_handler.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="WebRTC demo")
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="TRACE")
    else:
        logger.add(sys.stderr, level="DEBUG")

    uvicorn.run(app, host=args.host, port=args.port)



================================================
FILE: phone-chatbot/README.md
================================================
# Pipecat Phone Chatbot Examples

This directory contains examples for building phone chatbots using Pipecat. All examples can be run locally for development or deployed to [Pipecat Cloud](https://pipecat.daily.co) for production.

## Examples

- **[daily-pstn-dial-in](./daily-pstn-dial-in/)** - Basic incoming call handling with Daily PSTN
- **[daily-pstn-dial-out](./daily-pstn-dial-out/)** - Basic outgoing call handling with Daily PSTN
- **[daily-pstn-cold-transfer](./daily-pstn-cold-transfer/)** - Customer support bot with cold transfer to human operators
- **[daily-twilio-sip-dial-in](./daily-twilio-sip-dial-in/)** - Incoming calls using Daily + Twilio SIP
- **[daily-twilio-sip-dial-out](./daily-twilio-sip-dial-out/)** - Outgoing calls using Daily + Twilio SIP

Each example includes its own README with detailed setup instructions, architecture details, and deployment guidance.

## Getting Started

1. Choose an example that matches your use case
2. Follow the setup instructions in that example's README
3. Test locally using ngrok for webhook endpoints
4. Deploy to [Pipecat Cloud](https://pipecat.daily.co) for production

## Architecture

All examples use:

- **Transport**: Daily WebRTC
- **Speech-to-Text**: Deepgram
- **LLM**: OpenAI GPT-4o
- **Text-to-Speech**: Cartesia
- **Phone Numbers**: Daily PSTN or Twilio SIP

## Support

For questions or advanced use cases, join our [Discord community](https://discord.gg/pipecat).



================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/README.md
================================================
# Daily PSTN Cold Call Transfer

A basic example of how to create a bot that handles the initial customer interaction and then performs a cold transfer to a human operator when requested.

## How It Works

1. Daily receives an incoming call to your phone number.
2. Daily calls your webhook server (`/start` endpoint).
3. The server creates a Daily room with dial-in capabilities
4. The server starts the bot process with the room details
5. The caller is put on hold with music
6. The bot joins the Daily room and signals readiness
7. Daily forwards the call to the Daily room
8. The caller and the bot are connected, and the bot handles the conversation
9. When requested, the bot dials out to an operator and performs a cold transfer (bot leaves the call)

## Architecture Overview

This example uses the following components:

- 🔁 **Transport**: Daily WebRTC
- 💬 **Speech-to-Text**: Deepgram
- 🤖 **LLM**: OpenAI GPT-4o
- 🔉 **Text-to-Speech**: Cartesia

## Prerequisites

### Daily

- A Daily account with an API key (or Daily API key from Pipecat Cloud account)

### AI Services

- OpenAI API key for the bot's intelligence
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)
- One phone to dial-in from and another phone to receive calls when escalating to a manager

## Setup

1. Create a virtual environment and install dependencies

   ```bash
   uv sync
   ```

2. Set up environment variables

   Copy the example file and fill in your API keys:

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

   - Note: Please specify an OPERATOR_NUMBER so that the bot can ring a number when escalating to a manager

3. Buy a phone number

   Instructions on how to do that can be found at this [docs link:](https://docs.daily.co/reference/rest-api/phone-numbers/buy-phone-number)

4. Set up the dial-in config

   Instructions on how to do that can be found at this [docs link:](https://docs.daily.co/reference/rest-api/domainDialinConfig).

   Note that the `room_creation_api` is the address and route of your server that will handle the webhook that fires when a call is received. For local testing this will be your `ngrok` tunnel URL and the route should match your server's endpoint. In testing your demo, this will be `https://your-ngrok-url.ngrok.io/start`.

   > Tip: If you're using Pipecat Cloud, you can purchase a number using the Pipecat Cloud dashboard (Settings > Telephony).

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server or ngrok URL for handling the dial-in webhook and starting the bot
- Default configuration for development and testing

### Production (`ENV=production`)

- Bot is deployed to Pipecat Cloud; requires `PIPECAT_API_KEY` and `PIPECAT_AGENT_NAME`
- Set these when deploying to production environments
- Your FastAPI server runs either locally or deployed to your infrastructure

## Run the Bot Locally

1. Start the webhook server:

   ```bash
   python server.py
   ```

2. Start an ngrok tunnel to expose your local server

   ```bash
   ngrok http 7860
   ```

   Important: Make sure that this URL matches the `room_creation_api` URL for your phone number.

   > Tip: Use the `--subdomain` for a reusable ngrok link.

3. Call your bot!

   Call the number you configured to talk to your bot. Ask to speak to a manager and the bot will transfer you to the OPERATOR_NUMBER (cold transfer - the bot will leave the call).

## Production Deployment

You can deploy your bot to Pipecat Cloud and server to your infrastructure to run this bot in a production environment.

#### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

You'll only deploy your `bot.py` file.

#### Deploy the Server

The `server.py` handles inbound call webhooks and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (receives webhooks and starts the bot)

#### Environment Variables for Production

Add these to your production environment:

```bash
ENV=production
PIPECAT_API_KEY=your_pipecat_cloud_api_key
PIPECAT_AGENT_NAME=your-agent-name
```

The server automatically detects the environment and routes bot starting requests accordingly.

## What is a Cold Transfer?

A cold transfer is when the bot transfers the call directly to another party (the operator) and then leaves the call entirely. This means:

1. **Customer** calls and speaks with the **bot**
2. Customer requests to speak with a supervisor
3. **Bot** dials the **operator** and says "I'm transferring you to a supervisor now"
4. When the **operator** answers, the **bot immediately leaves the call**
5. **Customer** and **operator** are now connected directly

This is different from a "warm transfer" where the bot would stay on the call, introduce the customer to the operator, provide a summary, and then leave.

## Daily SIP Configuration

The bot configures Daily rooms with SIP capabilities using these settings:

```python
sip_params = DailyRoomSipParams(
    display_name="phone-user",  # This will show up in the Daily UI; optional display the dialer's number
    video=False,                # Audio-only call
    sip_mode="dial-in",         # For receiving calls (vs. dial-out)
    num_endpoints=1,            # Number of SIP endpoints to create
)

properties = DailyRoomProperties(
        sip=sip_params,
        enable_dialout=True,  # Needed for outbound calls if you expand the bot
        enable_chat=False,  # No need for chat in a voice bot
        start_video_off=True,  # Voice only
)
```

If you're using the Pipecat development runner's Daily util, these args are handled for you when calling `configure()`.

## Troubleshooting

### Call is not being answered

- Check that your dial-in config is correctly configured to point towards your ngrok server and correct endpoint
- Make sure the server.py file is running
- Make sure ngrok is correctly setup and pointing to the correct port

### The bot does not escalate to the manager

- Check that your room has `enable_dialout=True` set
- Check that your meeting token is an owner token (The bot does this for you automatically)
- Check that the phone number you are trying to ring is correct, and is a US or Canadian number.

### Call connects but no bot is heard

- Ensure your Daily API key is correct and has SIP capabilities
- Verify that the Deepgram API key is correct
- Verify that the Cartesia API key and voice ID are correct

### Bot starts but disconnects immediately

- Check the Daily logs for any error messages
- Ensure your server has stable internet connectivity



================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import ToolsSchema
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    EndFrame,
    EndTaskFrame,
    LLMMessagesAppendFrame,
    LLMRunFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.llm_service import FunctionCallParams
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyDialinSettings, DailyParams, DailyTransport

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


async def terminate_call(params: FunctionCallParams):
    """Function the bot can call to terminate the call."""
    await params.llm.queue_frame(EndTaskFrame(), FrameDirection.UPSTREAM)


async def dial_operator(transport: BaseTransport, params: FunctionCallParams):
    """Function the bot can call to dial an operator and transfer the call."""
    operator_number = os.getenv("OPERATOR_NUMBER", None)

    if operator_number:
        logger.info(f"Transferring call to operator: {operator_number}")

        # Inform the user about the transfer
        content = "I'm transferring you to a supervisor now. Please hold while I connect you."
        message = {
            "role": "system",
            "content": content,
        }

        # Queue the message to the context and let it speak
        await params.llm.push_frame(LLMMessagesAppendFrame([message], run_llm=True))

        # Start the dialout to transfer the call
        transfer_params = {"toEndPoint": operator_number}
        logger.debug(f"SIP call transfer parameters: {transfer_params}")
        await transport.sip_call_transfer(transfer_params)

    else:
        # No operator number configured
        content = "I'm sorry, but supervisor transfer is not available at this time. Is there anything else I can help you with?"
        message = {
            "role": "system",
            "content": content,
        }

        # Queue the message to the context
        await params.llm.push_frame(LLMMessagesAppendFrame([message], run_llm=True))
        logger.warning("No operator dialout settings available")


async def run_bot(transport: BaseTransport, handle_sigint: bool) -> None:
    """Run the voice bot with the given parameters."""
    # Operator dialout number

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY", ""),
        voice_id="b7d50908-b17c-442d-ad8d-810c63997ed9",  # Use Helpful Woman voice by default
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    # ------------ LLM AND CONTEXT SETUP ------------

    system_instruction = """You are Hailey, a friendly customer support representative. Your responses will be converted to speech, so use natural, conversational language without special characters or formatting.

Guidelines:
1. Start by greeting callers: "Hello, this is Hailey from customer support. What can I help you with today?"

2. When handling requests:
   - If a caller asks to speak with a supervisor, manager, or human agent, use the `dial_operator` function to transfer them
   - If a caller wants to end the conversation or says goodbye, use the `terminate_call` function

3. Be helpful and professional while assisting with their questions or concerns.

Note: When you transfer a call to a supervisor, you will leave the call and the customer will speak directly with the supervisor.

Available functions:
- `dial_operator`: Call this when the user requests to speak with a supervisor or manager (this will transfer the call)
- `terminate_call`: Call this when the user wants to end the conversation"""

    messages = [
        {
            "role": "system",
            "content": system_instruction,
        }
    ]

    # ------------ FUNCTION DEFINITIONS ------------

    # Define function schemas for tools
    terminate_call_function = FunctionSchema(
        name="terminate_call",
        description="Call this function to terminate the call.",
        properties={},
        required=[],
    )

    dial_operator_function = FunctionSchema(
        name="dial_operator",
        description="Call this function when the user asks to speak with a human",
        properties={},
        required=[],
    )

    # Create tools schema
    tools = ToolsSchema(standard_tools=[terminate_call_function, dial_operator_function])

    # Register functions with the LLM
    llm.register_function("terminate_call", terminate_call)
    llm.register_function("dial_operator", lambda params: dial_operator(transport, params))

    # Initialize LLM context and aggregator
    context = LLMContext(messages, tools)
    context_aggregator = LLMContextAggregatorPair(context)

    # ------------ PIPELINE SETUP ------------

    # Build simple pipeline for cold transfer
    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            stt,
            context_aggregator.user(),  # User responses
            llm,
            tts,
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    # Create pipeline task
    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
        ),
    )

    # ------------ EVENT HANDLERS ------------

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        # Bot answers the phone and greets the user
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_dialout_answered")
    async def on_dialout_answered(transport, data):
        logger.info(f"Operator answered, transferring call: {data}")
        # Cold transfer: bot leaves the call, customer and operator continue
        # await task.cancel()
        await task.queue_frames([EndFrame()])

    @transport.event_handler("on_dialout_error")
    async def on_dialout_error(transport, data):
        logger.error(f"Operator dialout error: {data}")
        # Inform the customer that transfer failed
        content = "I'm sorry, but I'm unable to connect you with a supervisor at this time. Is there anything else I can help you with?"
        message = {"role": "system", "content": content}
        await task.queue_frames([LLMMessagesAppendFrame([message], run_llm=True)])

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        logger.debug(f"Participant left: {participant}, reason: {reason}")
        # If customer leaves, end the call
        await task.cancel()

    # ------------ RUN PIPELINE ------------

    runner = PipelineRunner(handle_sigint=handle_sigint)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    # Body is always a dict (compatible with both local and Pipecat Cloud)
    body_data = runner_args.body
    room_url = body_data.get("room_url")
    token = body_data.get("token")
    call_id = body_data.get("callId")
    call_domain = body_data.get("callDomain")

    if not all([call_id, call_domain]):
        logger.error("Call ID and Call Domain are required in the body.")
        return None

    daily_dialin_settings = DailyDialinSettings(call_id=call_id, call_domain=call_domain)

    transport = DailyTransport(
        room_url,
        token,
        "Call Transfer Bot",
        params=DailyParams(
            dialin_settings=daily_dialin_settings,
            api_key=os.getenv("DAILY_API_KEY"),
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV3(),
        ),
    )

    await run_bot(transport, runner_args.handle_sigint)



================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/env.example
================================================
# Daily credentials
DAILY_API_KEY=

# Service keys
OPENAI_API_KEY=
CARTESIA_API_KEY=
DEEPGRAM_API_KEY=

# Operator number
OPERATOR_NUMBER=

# Environment mode: "local" for development, "production" for cloud deployment
ENV=local

# Local server URL (only needed if running on different port/host)
LOCAL_SERVER_URL=http://localhost:7860

# Pipecat Cloud (only needed for production)
PIPECAT_API_KEY=
PIPECAT_AGENT_NAME=daily-pstn-call-transfer


================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/pcc-deploy.toml
================================================
agent_name = "daily-pstn-call-transfer"
image = "your_username/daily-pstn-call-transfer:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "daily-pstn-secrets"

[scaling]
	min_agents = 1



================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/pyproject.toml
================================================
[project]
name = "daily-pstn-call-transfer"
version = "0.1.0"
description = "Daily PSTN Call Transfer example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,cartesia,openai,silero,local-smart-turn-v3,runner]>=0.0.85",
    "pipecatcloud>=0.2.4",
]

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []


================================================
FILE: phone-chatbot/daily-pstn-cold-transfer/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Webhook server to handle Daily PSTN calls and start the voice bot.

This server provides endpoints for handling Daily PSTN webhooks and starting the bot.
The server automatically detects the environment (local vs production) and routes
bot starting requests accordingly:
- Local: Uses internal /start_bot endpoint
- Production: Calls Pipecat Cloud API

All call data (room_url, token, callId, callDomain) flows through the body parameter
to ensure consistency between local and cloud deployments.
"""

import asyncio
import os
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from loguru import logger
from pipecat.runner.daily import configure
from pipecat.runner.types import DailyRunnerArguments

from bot import bot as bot_function

load_dotenv()

# ----------------- API ----------------- #


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create aiohttp session to be used for Daily API calls
    app.state.session = aiohttp.ClientSession()
    yield
    # Close session when shutting down
    await app.state.session.close()


app = FastAPI(lifespan=lifespan)


@app.post("/start")
async def handle_incoming_daily_webhook(request: Request) -> JSONResponse:
    """Handle incoming Daily PSTN call webhook.

    This endpoint:
    1. Receives Daily webhook data for incoming PSTN calls
    2. Creates a Daily room with dial-in capabilities
    3. Starts the bot (locally or via Pipecat Cloud based on ENV)
    4. Returns room details for the caller

    Returns:
        JSONResponse with room_url and token
    """
    logger.debug("Received webhook from Daily")

    # Get the dial-in properties from the request
    try:
        data = await request.json()

        if not all(key in data for key in ["From", "To", "callId", "callDomain"]):
            raise HTTPException(
                status_code=400, detail="Missing properties 'From', 'To', 'callId', 'callDomain'"
            )

        # Extract the caller's phone number
        caller_phone = str(data.get("From"))
        call_id = data.get("callId")
        logger.debug(f"Processing call with ID: {call_id} from {caller_phone}")

        # Create a Daily room with dial-in capabilities
        try:
            room_details = await configure(request.app.state.session, sip_caller_phone=caller_phone)
        except Exception as e:
            logger.error(f"Error creating Daily room: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create Daily room: {str(e)}")

        # Extract necessary details
        room_url = room_details.room_url
        token = room_details.token
        logger.debug(f"Created Daily room: {room_url} with token: {token}")

        # Start the bot - either locally or via Pipecat Cloud
        try:
            # Check environment mode (local development vs production)
            environment = os.getenv("ENV", "local")  # "local" or "production"

            # Prepare body data with all necessary information
            # This data structure is consistent between local and cloud deployments
            body_data = {
                **data,  # Original webhook data (From, To, callId, callDomain, sipHeaders)
                "room_url": room_url,
                "token": token,
            }

            if environment == "production":
                # Production: Call Pipecat Cloud API to start the bot
                pipecat_api_key = os.getenv("PIPECAT_API_KEY")
                agent_name = os.getenv("PIPECAT_AGENT_NAME")

                if not pipecat_api_key:
                    raise HTTPException(
                        status_code=500, detail="PIPECAT_API_KEY required for production mode"
                    )

                logger.debug(f"Starting bot via Pipecat Cloud for call {call_id}")
                async with request.app.state.session.post(
                    f"https://api.pipecat.daily.co/v1/public/{agent_name}/start",
                    headers={
                        "Authorization": f"Bearer {pipecat_api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "createDailyRoom": False,  # We already created the room
                        "body": body_data,
                    },
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise HTTPException(
                            status_code=500,
                            detail=f"Failed to start bot via Pipecat Cloud: {error_text}",
                        )
                    cloud_data = await response.json()
                    logger.debug(f"Bot started successfully via Pipecat Cloud")
            else:
                # Local development: Call internal /start_bot endpoint to start the bot
                local_server_url = os.getenv("LOCAL_SERVER_URL", "http://localhost:7860")

                logger.debug(f"Starting bot via local /start_bot endpoint for call {call_id}")
                async with request.app.state.session.post(
                    f"{local_server_url}/start_bot",
                    headers={"Content-Type": "application/json"},
                    json={
                        "createDailyRoom": False,  # We already created the room
                        "body": body_data,
                    },
                ) as response:
                    if response.status != 200:
                        error_text = await response.text()
                        raise HTTPException(
                            status_code=500,
                            detail=f"Failed to start bot via local /start_bot endpoint: {error_text}",
                        )
                    local_data = await response.json()
                    logger.debug(f"Bot started successfully via local /start_bot endpoint")

        except Exception as e:
            logger.error(f"Error starting bot: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to start bot: {str(e)}")

    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

    # Return room details for the caller
    return JSONResponse({"room_url": room_url, "token": token})


@app.post("/start_bot")
async def start_bot_endpoint(request: Request):
    """Start bot endpoint for local development.

    This endpoint mimics the Pipecat Cloud API pattern, receiving the same body data
    structure and starting the bot locally. Used only in local development mode.

    Args:
        request: FastAPI request containing body with room_url, token, callId, callDomain

    Returns:
        dict: Success status and call_id
    """
    try:
        # Parse the request body
        request_data = await request.json()
        body = request_data.get("body", {})

        # Extract required data from body
        room_url = body.get("room_url")
        token = body.get("token")
        call_id = body.get("callId")
        call_domain = body.get("callDomain")

        if not all([room_url, token, call_id, call_domain]):
            raise HTTPException(
                status_code=400,
                detail="Missing required parameters in body: room_url, token, callId, callDomain",
            )

        # Create runner arguments with body data
        # Note: room_url and token are passed via body, not as direct arguments
        runner_args = DailyRunnerArguments(
            room_url=None,  # Data comes from body
            token=None,  # Data comes from body
            body=body,
        )
        runner_args.handle_sigint = False

        # Start the bot in the background
        asyncio.create_task(bot_function(runner_args))

        return {"status": "Bot started successfully", "call_id": call_id}

    except Exception as e:
        logger.error(f"Error in /start_bot endpoint: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start bot: {str(e)}")


@app.get("/health")
async def health_check():
    """Health check endpoint.

    Returns:
        dict: Status indicating server health
    """
    return {"status": "healthy"}


# ----------------- Main ----------------- #


if __name__ == "__main__":
    # Run the server
    port = int(os.getenv("PORT", "7860"))
    logger.info(f"Starting server on port {port}")
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)



================================================
FILE: phone-chatbot/daily-pstn-dial-in/README.md
================================================
# Daily PSTN dial-in simple chatbot

This project demonstrates how to create a voice bot that can receive phone calls via Daily's PSTN capabilities to enable voice conversations.

## How It Works

1. Daily receives an incoming call to your phone number
2. Daily calls your webhook server (`/daily-dialin-webhook` endpoint)
3. The webhook creates a Daily room with SIP configuration
4. The webhook starts your bot with the room details and caller information
5. The caller is put on hold with music
6. The bot joins the Daily room and signals readiness
7. Daily forwards the call to the Daily room
8. The caller and bot are connected for the conversation

## Project Structure

This example uses Pipecat's development runner to handle the webhook and bot lifecycle:

- **`bot.py`** - The voice bot implementation
  - Handles the conversation with the caller
  - Uses `DailyDialinRequest` from the runner for type-safe dial-in data
  - Deployed to Pipecat Cloud in production or run locally for development
  - The runner automatically provides webhook handling when using `--dialin` flag

## Prerequisites

### Daily

- A Daily account with an API key (or Daily API key from Pipecat Cloud account)

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Create a virtual environment and install dependencies

   ```bash
   uv sync
   ```

2. Set up environment variables

   Copy the example file and fill in your API keys:

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

   Required environment variables:

   - `DAILY_API_KEY` - Your Daily API key
   - `DEEPGRAM_API_KEY` - For speech-to-text
   - `CARTESIA_API_KEY` - For text-to-speech
   - `OPENAI_API_KEY` - For LLM inference

3. Buy a phone number

   Instructions on how to do that can be found at this [docs link](https://docs.daily.co/reference/rest-api/phone-numbers/buy-phone-number)

4. Set up the dial-in config

   Instructions on how to do that can be found at this [docs link](https://docs.daily.co/reference/rest-api/domainDialinConfig).

   The `room_creation_api` should point to your webhook endpoint. For local testing with ngrok, this will be:

   ```
   https://your-ngrok-url.ngrok.io/daily-dialin-webhook
   ```

   > Tip: If you're using Pipecat Cloud, you can purchase a number using the Pipecat Cloud dashboard (Settings > Telephony).

## Run the Bot Locally

1. **Run your bot with dial-in support**

   ```bash
   uv run bot.py -t daily --dialin
   ```

   This starts a FastAPI server on port 7860 with the `/daily-dialin-webhook` endpoint.

2. **Expose your bot to the internet**

   ```bash
   ngrok http 7860
   ```

   Copy the ngrok URL (e.g., `https://abc123.ngrok.io`).

   > Tip: Use `ngrok http 7860 --subdomain your-subdomain` for a reusable URL.

3. **Configure your Daily phone number**

   Set your phone number's `room_creation_api` webhook to:

   ```
   https://your-ngrok-url.ngrok.io/daily-dialin-webhook
   ```

   Instructions: [Daily Dial-in Config Docs](https://docs.daily.co/reference/rest-api/domainDialinConfig)

4. **Call your bot!**

   Call your configured phone number to talk to your bot.

## Deploy to Pipecat Cloud

1. **Deploy your bot**

   Follow the [Pipecat Cloud quickstart](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your `bot.py` file.

   You only need to deploy `bot.py` - Pipecat Cloud automatically handles webhook endpoints and room creation.

2. **Configure your phone number webhook**

   Using the Pipecat Cloud dashboard, configure your phone number's webhook endpoint to point to your deployed agent.

   This will set the webhook URL to:

   ```
   https://api.pipecat.daily.co/v1/public/webhooks/{organization_id}/{agent_name}/dialin
   ```

   Pipecat Cloud will automatically:

   - Receive the webhook
   - Create a Daily room with SIP configuration
   - Start your bot with the dial-in settings
   - Pass caller information via `DailyDialinRequest`

## Customize Your Bot

You can use the caller's phone number to personalize the conversation:

```python
from pipecat.runner.types import DailyDialinRequest, RunnerArguments

async def bot(runner_args: RunnerArguments):
    # Parse dial-in request
    request = DailyDialinRequest.model_validate(runner_args.body)

    # Get caller's phone number
    caller_phone = request.dialin_settings.From

    # Look up customer information from your database
    customer = await get_customer_by_phone(caller_phone)

    # Customize the system prompt
    messages = [
        {
            "role": "system",
            "content": f"You are a helpful assistant for {customer.name}. "
                      f"Their account status is {customer.status}. "
                      "Keep responses concise and conversational."
        }
    ]

    # Use the customized context in your bot...
```

## Troubleshooting

### Call is not being answered

- Check that your dial-in config's `room_creation_api` points to your ngrok URL + `/daily-dialin-webhook`
- Verify the bot is running with `uv run bot.py -t daily --dialin`
- Make sure ngrok is running and pointing to port 7860
- Check the bot logs for webhook reception
- Ensure your `DAILY_API_KEY` has the phone number associated with it

### Call connects but no bot is heard

- Ensure your `DAILY_API_KEY` environment variable is set and has SIP capabilities
- Verify that the `CARTESIA_API_KEY` and voice ID are correct
- Check that `DEEPGRAM_API_KEY` is set for speech-to-text

### Bot starts but disconnects immediately

- Check the bot logs for error messages
- Verify all required environment variables are set
- Ensure your server has stable internet connectivity

### Webhook test fails

- The runner automatically handles Daily's webhook verification test
- Check that the bot is running and accessible via your ngrok URL

## Daily SIP Configuration

The runner automatically configures Daily rooms with SIP capabilities when using `--dialin`:

```python
# The runner calls this for you:
room_config = await configure(session, sip_caller_phone=data.get("From"))
```

This creates a room with these SIP settings:

- `display_name`: Set to the caller's phone number (From field)
- `video`: False (audio-only call)
- `sip_mode`: "dial-in" (for receiving calls)
- `num_endpoints`: 1 (one SIP endpoint for the incoming caller)

The runner passes the caller's phone number and call details to your bot via `DailyDialinRequest` in `runner_args.body`.



================================================
FILE: phone-chatbot/daily-pstn-dial-in/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Daily PSTN dial-in bot.

This bot demonstrates how to receive inbound phone calls using Daily's PSTN capabilities.
The bot answers incoming calls and conducts voice conversations with callers.
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import DailyDialinRequest, RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyDialinSettings, DailyParams, DailyTransport

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, handle_sigint: bool) -> None:
    """Run the voice bot for an inbound call.

    Sets up the bot pipeline with STT, LLM, and TTS services, then handles
    the conversation when a caller connects.

    Args:
        transport: Daily transport for the call
        handle_sigint: Whether to handle SIGINT signals
    """

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY", ""),
        voice_id="b7d50908-b17c-442d-ad8d-810c63997ed9",  # Use Helpful Woman voice by default
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    # Initialize LLM context with system prompt
    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly phone assistant. Your responses will be read aloud, "
                "so keep them concise and conversational. Avoid special characters or "
                "formatting. Begin by greeting the caller and asking how you can help them today."
            ),
        },
    ]

    # Setup the conversational context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
        ),
    )

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        logger.debug(f"First participant joined: {participant['id']}")
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    @transport.event_handler("on_dialin_error")
    async def on_dialin_error(transport, data):
        logger.error(f"Dial-in error: {data}")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud.

    Parses the runner arguments, configures the Daily transport with dial-in
    settings, and starts the bot to handle the incoming call.

    Args:
        runner_args: Arguments from the Pipecat runner containing room details,
            call ID, and call domain for the inbound call

    Raises:
        Exception: If bot initialization or execution fails
    """

    try:
        # Parse the dial-in request from the runner
        request = DailyDialinRequest.model_validate(runner_args.body)

        # Configure Daily transport with dial-in settings
        daily_dialin_settings = DailyDialinSettings(
            call_id=request.dialin_settings.call_id,
            call_domain=request.dialin_settings.call_domain,
        )

        transport = DailyTransport(
            runner_args.room_url,
            runner_args.token,
            "Daily PSTN Dial-in Bot",
            params=DailyParams(
                api_key=request.daily_api_key,
                api_url=request.daily_api_url,
                dialin_settings=daily_dialin_settings,
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                turn_analyzer=LocalSmartTurnAnalyzerV3(),
            ),
        )

        # Log caller information if available
        # You can use this to look up customer information to personalize the conversation
        if request.dialin_settings.From:
            logger.info(f"Handling call from: {request.dialin_settings.From}")

        await run_bot(transport, runner_args.handle_sigint)

    except Exception as e:
        logger.error(f"Error running bot: {e}")
        raise e


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: phone-chatbot/daily-pstn-dial-in/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: phone-chatbot/daily-pstn-dial-in/env.example
================================================
# Daily credentials
DAILY_API_KEY=

# Service keys
DEEPGRAM_API_KEY=
OPENAI_API_KEY=
CARTESIA_API_KEY=


================================================
FILE: phone-chatbot/daily-pstn-dial-in/pcc-deploy.toml
================================================
agent_name = "daily-pstn-dial-in"
image = "your_username/daily-pstn-dial-in:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "daily-pstn-secrets"
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: phone-chatbot/daily-pstn-dial-in/pyproject.toml
================================================
[project]
name = "daily-pstn-dial-in"
version = "0.1.0"
description = "Daily PSTN Dial-in example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,cartesia,openai,silero,local-smart-turn-v3,runner]>=0.0.98",
    "pipecatcloud>=0.2.15",
]

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []


================================================
FILE: phone-chatbot/daily-pstn-dial-out/README.md
================================================
# Daily PSTN dial-out simple chatbot

This project demonstrates how to create a voice bot that uses Daily's PSTN capabilities to make outbound calls to phone numbers.

## How It Works

1. The server receives a dial-out request with the phone number to call
2. The server creates a Daily room with dial-out capabilities
3. The server starts the bot process (locally or via Pipecat Cloud based on ENV)
4. The bot joins the room and initiates the dial-out to the specified number
5. The bot automatically retries on failure (up to 5 attempts)
6. When the call is answered, the bot conducts the conversation

## Project Structure

This example is organized to be production-ready and easy to customize:

- **`server.py`** - FastAPI server that handles dial-out requests

  - Receives dial-out requests via `/dialout` endpoint
  - Creates Daily rooms with dial-out capabilities
  - Routes to local or production bot deployment
  - Uses shared HTTP session for optimal performance

- **`server_utils.py`** - Utility functions for Daily API interactions

  - Data models for dial-out requests and agent configuration
  - Room creation logic
  - Bot starting logic (production and local modes)
  - Easy to extend with custom business logic

- **`bot.py`** - The voice bot implementation
  - `DialoutManager` class for retry logic
  - Handles the conversation with the person being called
  - Deployed to Pipecat Cloud in production or run locally for development

## Prerequisites

### Daily

- A Daily account with an API key (or Daily API key from Pipecat Cloud account)
- A phone number purchased through Daily
- Dial-out must be enabled on your domain. Find out more by reading this [document and filling in the form](https://docs.daily.co/guides/products/dial-in-dial-out#main)

### AI Services

- Deepgram API key for speech-to-text
- OpenAI API key for the LLM inference
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager (recommended) or pip
- Docker (for production deployment)

## Setup

1. Create a virtual environment and install dependencies

   ```bash
   uv sync
   ```

2. Set up environment variables

   Copy the example file and fill in your API keys:

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. Buy a phone number

   Instructions on how to do that can be found at this [docs link](https://docs.daily.co/reference/rest-api/phone-numbers/buy-phone-number)

4. Request dial-out enablement

   For compliance reasons, to enable dial-out for your Daily account, you must request enablement via the form. You can find out more about dial-out, and the form at the [link here](https://docs.daily.co/guides/products/dial-in-dial-out#main)

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server for handling dial-out requests and starting the bot
- Default configuration for development and testing

### Production (`ENV=production`)

- Bot is deployed to Pipecat Cloud; requires `PIPECAT_API_KEY` and `PIPECAT_AGENT_NAME`
- Set these when deploying to production environments
- Your FastAPI server runs either locally or deployed to your infrastructure

## Run the Bot Locally

You'll need two terminal windows open:

1. **Terminal 1**: Start the webhook server:

   ```bash
   uv run server.py
   ```

   This runs on port 8080 and handles dial-out requests.

2. **Terminal 2**: Start the bot server:

   ```bash
   uv run bot.py -t daily
   ```

   This runs on port 7860 and handles the bot logic.

3. **Test the dial-out functionality**

   With both servers running, send a dial-out request:

   ```bash
   curl -X POST "http://localhost:8080/dialout" \
     -H "Content-Type: application/json" \
     -d '{
       "dialout_settings": {
         "phone_number": "+1234567890"
       }
     }'
   ```

   The server will create a room, start the bot, and the bot will call the specified number. Answer the call to speak with the bot.

## Production Deployment

You can deploy your bot to Pipecat Cloud and server to your infrastructure to run this bot in a production environment.

### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

You'll only deploy your `bot.py` file.

### Deploy the Server

The `server.py` handles dial-out requests and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (receives dial-out requests and starts the bot)

### Environment Variables for Production

Add these to your production environment:

```bash
ENV=production
PIPECAT_API_KEY=your_pipecat_cloud_api_key
PIPECAT_AGENT_NAME=your-agent-name
```

The server automatically detects the environment and routes bot starting requests accordingly.

## Customization

This example is designed to be easily customized for your use case:

### Adding Custom Data to Dial-out Requests

You can extend the `DialoutSettings` model in `server_utils.py` to pass custom data:

```python
class DialoutSettings(BaseModel):
    phone_number: str
    caller_id: str | None = None
    # Add your custom fields here
    customer_name: str | None = None
    account_id: str | None = None
```

Then populate this data in `server.py` before starting the bot:

```python
# Example: Look up customer information
customer_info = await get_customer_by_phone(dialout_request.dialout_settings.phone_number)

agent_request = AgentRequest(
    room_url=daily_room_config.room_url,
    token=daily_room_config.token,
    dialout_settings=dialout_request.dialout_settings,
    # Your custom data
    customer_name=customer_info.name,
    account_id=customer_info.id,
)
```

## Troubleshooting

### I get an error about dial-out not being enabled

- Check that your room has `enable_dialout=True` set
- Check that your meeting token is an owner token (The bot does this for you automatically)
- Check that you have purchased a phone number to ring from
- Check that the phone number you are trying to ring is correct, and is a US or Canadian number.

### Call connects but no bot is heard

- Ensure your Daily API key is correct and has SIP capabilities
- Verify that the Cartesia API key and voice ID are correct

### Bot starts but disconnects immediately

- Check the Daily logs for any error messages
- Ensure your server has stable internet connectivity



================================================
FILE: phone-chatbot/daily-pstn-dial-out/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Daily PSTN dial-out bot.

This bot demonstrates how to make outbound phone calls using Daily's PSTN capabilities.
The bot initiates a call to a specified phone number and conducts a voice conversation.
"""

import os
from typing import Any, Optional

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyParams, DailyTransport

from server_utils import AgentRequest, DialoutSettings

load_dotenv()


class DialoutManager:
    """Manages dialout attempts with retry logic.

    Handles the complexity of initiating outbound calls with automatic retry
    on failure, up to a configurable maximum number of attempts.

    Args:
        transport: The Daily transport instance for making the dialout
        dialout_settings: Settings containing phone number and optional caller ID
        max_retries: Maximum number of dialout attempts (default: 5)
    """

    def __init__(
        self,
        transport: BaseTransport,
        dialout_settings: DialoutSettings,
        max_retries: Optional[int] = 5,
    ):
        self._transport = transport
        self._phone_number = dialout_settings.phone_number
        self._max_retries = max_retries
        self._attempt_count = 0
        self._is_successful = False

    async def attempt_dialout(self) -> bool:
        """Attempt to start a dialout call.

        Initiates an outbound call if retry limit hasn't been reached and
        no successful connection has been made yet.

        Returns:
            True if dialout attempt was initiated, False if max retries reached
            or call already successful
        """
        if self._attempt_count >= self._max_retries:
            logger.error(
                f"Maximum retry attempts ({self._max_retries}) reached. Giving up on dialout."
            )
            return False

        if self._is_successful:
            logger.debug("Dialout already successful, skipping attempt")
            return False

        self._attempt_count += 1
        logger.info(
            f"Attempting dialout (attempt {self._attempt_count}/{self._max_retries}) to: {self._phone_number}"
        )

        await self._transport.start_dialout({"phoneNumber": self._phone_number})
        return True

    def mark_successful(self):
        """Mark the dialout as successful to prevent further retry attempts."""
        self._is_successful = True

    def should_retry(self) -> bool:
        """Check if another dialout attempt should be made.

        Returns:
            True if retry limit not reached and call not yet successful
        """
        return self._attempt_count < self._max_retries and not self._is_successful


async def run_bot(
    transport: BaseTransport, handle_sigint: bool, dialout_settings: DialoutSettings
) -> None:
    """Run the voice bot for an outbound call.

    Sets up the bot pipeline with STT, LLM, and TTS services, then initiates
    the dialout and handles the conversation with retry logic.

    Args:
        transport: Daily transport for the call
        handle_sigint: Whether to handle SIGINT signals
        dialout_settings: Phone number and optional caller ID for the outbound call
    """

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY", ""),
        voice_id="b7d50908-b17c-442d-ad8d-810c63997ed9",  # Use Helpful Woman voice by default
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    # Initialize LLM context with system prompt
    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly phone assistant. Your responses will be read aloud, "
                "so keep them concise and conversational. Avoid special characters or "
                "formatting. Begin by greeting the caller and asking how you can help them today."
            ),
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
        ),
    )

    # Initialize dialout manager
    dialout_manager = DialoutManager(transport, dialout_settings)

    @transport.event_handler("on_joined")
    async def on_joined(transport, data):
        await dialout_manager.attempt_dialout()

    @transport.event_handler("on_dialout_answered")
    async def on_dialout_answered(transport, data):
        logger.debug(f"Dial-out answered: {data}")
        dialout_manager.mark_successful()

    @transport.event_handler("on_dialout_error")
    async def on_dialout_error(transport, data: Any):
        logger.error(f"Dial-out error, retrying: {data}")

        if dialout_manager.should_retry():
            await dialout_manager.attempt_dialout()
        else:
            logger.error(f"No more retries allowed, stopping bot.")
            await task.cancel()

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud.

    Parses the runner arguments, configures the Daily transport with dialout
    capabilities, and starts the bot.

    Args:
        runner_args: Arguments from the Pipecat runner containing room details
            and dialout settings

    Raises:
        Exception: If bot initialization or execution fails
    """
    try:
        request = AgentRequest.model_validate(runner_args.body)

        transport = DailyTransport(
            request.room_url,
            request.token,
            "Daily PSTN Dial-out Bot",
            params=DailyParams(
                api_key=os.getenv("DAILY_API_KEY"),
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                turn_analyzer=LocalSmartTurnAnalyzerV3(),
            ),
        )

        await run_bot(transport, runner_args.handle_sigint, request.dialout_settings)

    except Exception as e:
        logger.error(f"Error running bot: {e}")
        raise e


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: phone-chatbot/daily-pstn-dial-out/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./server_utils.py server_utils.py
COPY ./bot.py bot.py



================================================
FILE: phone-chatbot/daily-pstn-dial-out/env.example
================================================
# Daily credentials
DAILY_API_KEY=

# Service keys
DEEPGRAM_API_KEY=
OPENAI_API_KEY=
CARTESIA_API_KEY=

# Environment mode: "local" for development, "production" for cloud deployment
ENV=local

# Local server URL (only needed if running on different port/host)
LOCAL_SERVER_URL=http://localhost:7860

# Pipecat Cloud (only needed for production)
PIPECAT_API_KEY=
PIPECAT_AGENT_NAME=daily-pstn-dial-out


================================================
FILE: phone-chatbot/daily-pstn-dial-out/pcc-deploy.toml
================================================
agent_name = "daily-pstn-dial-out"
image = "your_username/daily-pstn-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "daily-pstn-secrets"
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: phone-chatbot/daily-pstn-dial-out/pyproject.toml
================================================
[project]
name = "daily-pstn-dial-out"
version = "0.1.0"
description = "Daily PSTN Dial-out example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,cartesia,openai,silero,local-smart-turn-v3,runner]>=0.0.91",
    "pipecatcloud>=0.2.7",
]

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []


================================================
FILE: phone-chatbot/daily-pstn-dial-out/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Webhook server to handle Daily PSTN dial-out requests and start the voice bot.

This server provides endpoints for handling Daily PSTN dial-out requests and starting the bot.
The server automatically detects the environment (local vs production) and routes
bot starting requests accordingly:
- Local: Uses internal /start endpoint
- Production: Calls Pipecat Cloud API

All call data (room_url, token, dialout_settings) flows through the body parameter
to ensure consistency between local and cloud deployments.
"""

import os
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from loguru import logger

from server_utils import (
    AgentRequest,
    create_daily_room,
    dialout_request_from_request,
    start_bot_local,
    start_bot_production,
)

load_dotenv()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle and shared resources.

    Creates a shared aiohttp session for making HTTP requests to bot endpoints.
    The session is reused across requests for better performance through connection pooling.
    """
    # Create shared HTTP session for bot API calls
    app.state.http_session = aiohttp.ClientSession()
    logger.info("Created shared HTTP session")
    yield
    # Clean up: close the session on shutdown
    await app.state.http_session.close()
    logger.info("Closed shared HTTP session")


app = FastAPI(lifespan=lifespan)


@app.post("/dialout")
async def handle_dial_out_request(request: Request) -> JSONResponse:
    """Handle dial-out request.

    This endpoint:
    1. Receives dial-out request with phone number and optional caller ID
    2. Creates a Daily room with dial-out capabilities
    3. Starts the bot (locally or via Pipecat Cloud based on ENV)
    4. Returns room details for monitoring

    Args:
        request: FastAPI request containing dialout_settings

    Returns:
        JSONResponse: Success status with room_url and token

    Raises:
        HTTPException: If request data is invalid or bot fails to start
    """
    logger.debug("Received dial-out request")

    dialout_request = await dialout_request_from_request(request)

    daily_room_config = await create_daily_room(dialout_request, request.app.state.http_session)

    agent_request = AgentRequest(
        room_url=daily_room_config.room_url,
        token=daily_room_config.token,
        dialout_settings=dialout_request.dialout_settings,
    )

    try:
        if os.getenv("ENV") == "production":
            await start_bot_production(agent_request, request.app.state.http_session)
        else:
            await start_bot_local(agent_request, request.app.state.http_session)
    except Exception as e:
        logger.error(f"Error starting bot: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start bot: {str(e)}")

    return JSONResponse(
        {
            "status": "success",
            "room_url": daily_room_config.room_url,
            "token": daily_room_config.token,
            "phone_number": dialout_request.dialout_settings.phone_number,
        }
    )


@app.get("/health")
async def health_check():
    """Health check endpoint.

    Returns:
        dict: Status indicating server health
    """
    return {"status": "healthy"}


# ----------------- Main ----------------- #


if __name__ == "__main__":
    # Run the server
    port = int(os.getenv("PORT", "8080"))
    logger.info(f"Starting server on port {port}")
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)



================================================
FILE: phone-chatbot/daily-pstn-dial-out/server_utils.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Utilities for Daily PSTN dial-out handling and bot management.

This module provides data models and functions for:
- Parsing dial-out request data
- Creating Daily rooms for outgoing calls
- Starting bots in production (Pipecat Cloud) or local development mode
"""

import os

import aiohttp
from fastapi import HTTPException, Request
from loguru import logger
from pipecat.runner.daily import DailyRoomConfig, configure
from pydantic import BaseModel


class DialoutSettings(BaseModel):
    """Settings for outbound call.

    Attributes:
        phone_number: The phone number to dial
        caller_id: Optional caller ID to display (if not provided, uses your Daily number)
    """

    phone_number: str
    caller_id: str | None = None
    # Include any custom data here needed for the call


class DialoutRequest(BaseModel):
    """Request data for initiating a dial-out call.

    Add any custom data here needed for the call. For example,
    you may add customer information, campaign data, or call context.

    Attributes:
        dialout_settings: Settings for the outbound call
    """

    dialout_settings: DialoutSettings


class AgentRequest(BaseModel):
    """Request data sent to bot start endpoint.

    Attributes:
        room_url: Daily room URL for the bot to join
        token: Authentication token for the Daily room
        dialout_settings: Settings for the outbound call
    """

    room_url: str
    token: str
    dialout_settings: DialoutSettings
    # Include any custom data here needed for the agent


async def dialout_request_from_request(request: Request) -> DialoutRequest:
    """Parse and validate dial-out request data.

    Args:
        request: FastAPI request object containing dial-out data

    Returns:
        DialoutRequest: Parsed and validated dial-out request

    Raises:
        HTTPException: If required fields are missing from the request data
    """
    data = await request.json()

    if not data.get("dialout_settings"):
        raise HTTPException(
            status_code=400, detail="Missing 'dialout_settings' in the request body"
        )

    try:
        return DialoutRequest.model_validate(data)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid request data: {str(e)}")


async def create_daily_room(
    dialout_request: DialoutRequest, session: aiohttp.ClientSession
) -> DailyRoomConfig:
    """Create a Daily room configured for PSTN dial-out.

    Args:
        dialout_request: Dial-out request containing phone number and settings
        session: Shared aiohttp session for making HTTP requests

    Returns:
        DailyRoomConfig: Configuration object with room_url and token

    Raises:
        HTTPException: If room creation fails
    """
    try:
        return await configure(
            session, sip_caller_phone=dialout_request.dialout_settings.phone_number
        )
    except Exception as e:
        logger.error(f"Error creating Daily room: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create Daily room: {str(e)}")


async def start_bot_production(agent_request: AgentRequest, session: aiohttp.ClientSession):
    """Start the bot via Pipecat Cloud API for production deployment.

    Args:
        agent_request: Agent configuration with room_url, token, and dialout settings
        session: Shared aiohttp session for making HTTP requests

    Raises:
        HTTPException: If required environment variables are missing or API call fails
    """
    pipecat_api_key = os.getenv("PIPECAT_API_KEY")
    agent_name = os.getenv("PIPECAT_AGENT_NAME")

    if not pipecat_api_key or not agent_name:
        raise HTTPException(
            status_code=500,
            detail="PIPECAT_API_KEY and PIPECAT_AGENT_NAME required for production mode",
        )

    logger.debug(
        f"Starting bot via Pipecat Cloud for dial-out to {agent_request.dialout_settings.phone_number}"
    )

    body_data = agent_request.model_dump(exclude_none=True)

    async with session.post(
        f"https://api.pipecat.daily.co/v1/public/{agent_name}/start",
        headers={
            "Authorization": f"Bearer {pipecat_api_key}",
            "Content-Type": "application/json",
        },
        json={
            "createDailyRoom": False,  # We already created the room
            "body": body_data,
        },
    ) as response:
        if response.status != 200:
            error_text = await response.text()
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start bot via Pipecat Cloud: {error_text}",
            )
        logger.debug("Bot started successfully via Pipecat Cloud")


async def start_bot_local(agent_request: AgentRequest, session: aiohttp.ClientSession):
    """Start the bot via local /start endpoint for development.

    Args:
        agent_request: Agent configuration with room_url, token, and dialout settings
        session: Shared aiohttp session for making HTTP requests

    Raises:
        HTTPException: If LOCAL_SERVER_URL is not set or API call fails
    """
    local_server_url = os.getenv("LOCAL_SERVER_URL", "http://localhost:7860")

    logger.debug(
        f"Starting bot via local /start endpoint for dial-out to {agent_request.dialout_settings.phone_number}"
    )

    body_data = agent_request.model_dump(exclude_none=True)

    async with session.post(
        f"{local_server_url}/start",
        headers={"Content-Type": "application/json"},
        json={
            "createDailyRoom": False,  # We already created the room
            "body": body_data,
        },
    ) as response:
        if response.status != 200:
            error_text = await response.text()
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start bot via local /start endpoint: {error_text}",
            )
        logger.debug("Bot started successfully via local /start endpoint")



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/README.md
================================================
# Daily + Twilio SIP dial-in Voice Bot

This project demonstrates how to create a voice bot that can receive phone calls via Twilio and use Daily's SIP capabilities to enable voice conversations.

## How It Works

1. Twilio receives an incoming call to your phone number
2. Twilio calls your webhook server (`/call` endpoint in `server.py`)
3. The server creates a Daily room with SIP capabilities
4. The server starts the bot process with the room details (locally or via Pipecat Cloud)
5. The caller is put on hold with music (a US ringtone in this example)
6. The bot joins the Daily room and signals readiness
7. Twilio forwards the call to Daily's SIP endpoint
8. The caller and the bot are connected, and the bot handles the conversation

## Project Structure

This example is organized to be production-ready and easy to customize:

- **`server.py`** - FastAPI webhook server that handles incoming calls

  - Receives Twilio call webhooks
  - Creates Daily rooms with SIP capabilities
  - Routes to local or production bot deployment
  - Uses shared HTTP session for optimal performance

- **`server_utils.py`** - Utility functions for Twilio and Daily API interactions

  - Data models for call data and agent requests
  - Room creation logic
  - Bot starting logic (production and local modes)
  - Easy to extend with custom business logic

- **`bot.py`** - The voice bot implementation
  - Handles the conversation with the caller
  - Deployed to Pipecat Cloud in production or run locally for development

## Prerequisites

### Twilio

- A Twilio account with a phone number that supports voice
- Twilio Account SID and Auth Token

### Daily

- A Daily account with an API key (or Daily API key from Pipecat Cloud account)

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Create a virtual environment and install dependencies

   ```bash
   uv sync
   ```

2. Set up environment variables

Copy the example file and fill in your API keys:

    ```bash
    cp .env.example .env
    # Edit .env with your API keys
    ```

3. Configure your Twilio webhook

In the Twilio console:

- Go to your phone number's configuration
- Set the webhook for "A call comes in" to your server's URL + "/call"
- For local testing, you can use ngrok to expose your local server

```bash
ngrok http 8080
# Then use the provided URL (e.g., https://abc123.ngrok.io/call) in Twilio
```

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server or ngrok URL for handling the webhook and starting the bot
- Default configuration for development and testing

### Production (`ENV=production`)

- Bot is deployed to Pipecat Cloud; requires `PIPECAT_API_KEY` and `PIPECAT_AGENT_NAME`
- Set these when deploying to production environments
- Your FastAPI server runs either locally or deployed to your infrastructure

## Run the Bot Locally

You'll need three terminal windows open:

1. Terminal 1: Start the webhook server:

   ```bash
   uv run server.py
   ```

2. Terminal 2: Start an ngrok tunnel to expose the FastAPI server running on server.py

   ```bash
   ngrok http 8080
   ```

   Important: Make sure that this URL matches the webhook URL configured in your Twilio phone number settings.

   > Tip: Use the `--subdomain` flag for a reusable ngrok link.

3. Terminal 3: Run your bot:

   ```bash
   uv run bot.py -t daily
   ```

   > The bot.py file includes a FastAPI server. This emulates the Pipecat Cloud service, and is as if you're running with `min_agents=1`.

4. Call your bot!

   Call the Twilio number you configured to talk to your bot.

## Production Deployment

You can deploy your bot to Pipecat Cloud and server to your infrastructure to run this bot in a production environment.

### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

You'll only deploy your `bot.py` file.

### Deploy the Server

The `server.py` handles inbound call webhooks and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (receives webhooks and starts the bot)

### Environment Variables for Production

Add these to your production environment:

```bash
ENV=production
PIPECAT_API_KEY=your_pipecat_cloud_api_key
PIPECAT_AGENT_NAME=your-agent-name
```

The server automatically detects the environment and routes bot starting requests accordingly.

## Adding Custom Data to Agent Requests

You can extend the `AgentRequest` model in `server_utils.py` to pass custom data to your bot:

```python
class AgentRequest(BaseModel):
    room_url: str
    token: str
    call_sid: str
    sip_uri: str
    # Add your custom fields here
    customer_name: str | None = None
    account_id: str | None = None
```

Then populate this data in `server.py` before starting the bot:

```python
# Example: Look up customer information
customer_info = await get_customer_by_phone(call_data.from_phone)

agent_request = AgentRequest(
    room_url=sip_config.room_url,
    token=sip_config.token,
    call_sid=call_data.call_sid,
    sip_uri=sip_config.sip_endpoint,
    customer_name=customer_info.name,
    account_id=customer_info.id,
)
```

## Troubleshooting

### Call is not being answered

- Check that your Twilio webhook is correctly configured to point to your ngrok server and `/call` endpoint
- Make sure the server.py file is running
- Make sure ngrok is correctly setup and pointing to the correct port

### Call connects but no bot is heard

- Ensure your Daily API key is correct and has SIP capabilities
- Verify that the Cartesia API key and voice ID are correct
- Check that your Twilio credentials (Account SID and Auth Token) are correct

### Bot starts but disconnects immediately

- Check the Daily logs for any error messages
- Ensure your server has stable internet connectivity



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Twilio + Daily voice bot implementation."""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from twilio.rest import Client

from server_utils import AgentRequest

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, request: AgentRequest, handle_sigint: bool) -> None:
    """Run the voice bot with the given parameters.

    Args:
        transport: The Daily transport instance
        request: The agent request containing the call details
        handle_sigint: Whether to handle SIGINT
    """
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    # Initialize LLM context with system prompt
    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly phone assistant. Your responses will be read aloud, "
                "so keep them concise and conversational. Avoid special characters or "
                "formatting. Begin by greeting the caller and asking how you can help them today."
            ),
        },
    ]

    # Setup the conversational context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # Build the pipeline
    pipeline = Pipeline(
        [
            transport.input(),
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    # Create the pipeline task
    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
        ),
    )

    # Handle call ready to forward
    @transport.event_handler("on_dialin_ready")
    async def on_dialin_ready(transport, sip_endpoint):
        logger.info(f"Forwarding call {request.call_sid} to {request.sip_uri}")

        try:
            twilio_client = Client(os.getenv("TWILIO_ACCOUNT_SID"), os.getenv("TWILIO_AUTH_TOKEN"))

            # Update the Twilio call with TwiML to forward to the Daily SIP endpoint
            twilio_client.calls(request.call_sid).update(
                twiml=f"<Response><Dial><Sip>{request.sip_uri}</Sip></Dial></Response>"
            )
            logger.info("Call forwarded successfully")
        except Exception as e:
            logger.error(f"Failed to forward call: {str(e)}")
            await task.cancel()

    @transport.event_handler("on_dialin_error")
    async def on_dialin_error(transport, data):
        logger.error(f"Dial-in error: {data}")
        await task.cancel()

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        await task.queue_frame(LLMRunFrame())

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""
    try:
        request = AgentRequest.model_validate(runner_args.body)

        transport = DailyTransport(
            request.room_url,
            request.token,
            "SIP Dial-in Bot",
            params=DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                turn_analyzer=LocalSmartTurnAnalyzerV3(),
            ),
        )

        await run_bot(transport, request, runner_args.handle_sigint)
    except Exception as e:
        logger.error(f"Invalid request: {e}")
        return


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./server_utils.py server_utils.py
COPY ./bot.py bot.py



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/env.example
================================================
# Daily credentials
DAILY_API_KEY=

# Twilio credentials
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=

# Service keys
DEEPGRAM_API_KEY=
OPENAI_API_KEY=
CARTESIA_API_KEY=

# Environment mode: "local" for development, "production" for cloud deployment
ENV=local

# Local bot server URL (where bot.py runs)
LOCAL_SERVER_URL=http://localhost:7860

# Pipecat Cloud (only needed for production)
PIPECAT_API_KEY=
PIPECAT_AGENT_NAME=daily-twilio-sip-dial-in


================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/pcc-deploy.toml
================================================
agent_name = "daily-twilio-sip-dial-in"
image = "your_username/daily-twilio-sip-dial-in:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "daily-twilio-sip-secrets"
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/pyproject.toml
================================================
[project]
name = "daily-twilio-sip-dial-in"
version = "0.1.0"
description = "Daily SIP + Twilio Dial-in example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,cartesia,openai,silero,local-smart-turn-v3,runner]>=0.0.91",
    "pipecatcloud>=0.2.7",
    "twilio",
]

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []


================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Webhook server to handle Twilio calls and start the voice bot.

This server provides two main endpoints:
- /call: Twilio webhook handler that receives incoming calls

The server automatically detects the environment (local vs production) and routes
bot starting requests accordingly:
- Local: Uses internal /start endpoint
- Production: Calls Pipecat Cloud API

All call data (room_url, token, call_sid, sip_uri) flows through the body parameter
to ensure consistency between local and cloud deployments.
"""

import os
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import PlainTextResponse
from loguru import logger
from twilio.twiml.voice_response import VoiceResponse

from server_utils import (
    AgentRequest,
    create_daily_room,
    start_bot_local,
    start_bot_production,
    twilio_call_data_from_request,
)

load_dotenv()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle and shared resources.

    Creates a shared aiohttp session for making HTTP requests to bot endpoints.
    The session is reused across requests for better performance through connection pooling.
    """
    # Create shared HTTP session for bot API calls
    app.state.http_session = aiohttp.ClientSession()
    logger.info("Created shared HTTP session")
    yield
    # Clean up: close the session on shutdown
    await app.state.http_session.close()
    logger.info("Closed shared HTTP session")


app = FastAPI(lifespan=lifespan)


@app.post("/call", response_class=PlainTextResponse)
async def handle_call(request: Request):
    """
    Handle incoming Twilio call webhook.

    This endpoint:
    1. Receives Twilio webhook data for incoming calls
    2. Creates a Daily room with SIP capabilities
    3. Starts the bot (locally or via Pipecat Cloud based on ENV)
    4. Returns TwiML to put caller on hold while bot connects

    Returns:
        TwiML response with hold music for the caller

    """
    logger.debug("Received call webhook from Twilio")

    call_data = await twilio_call_data_from_request(request)

    sip_config = await create_daily_room(call_data, request.app.state.http_session)

    # Make sure we have a SIP endpoint.
    if not sip_config.sip_endpoint:
        raise HTTPException(status_code=500, detail="No SIP endpoint provided by Daily")

    agent_request = AgentRequest(
        room_url=sip_config.room_url,
        token=sip_config.token,
        call_sid=call_data.call_sid,
        sip_uri=sip_config.sip_endpoint,
    )

    # Start bot locally or in production.
    try:
        if os.getenv("ENV") == "production":
            await start_bot_production(agent_request, request.app.state.http_session)
        else:
            await start_bot_local(agent_request, request.app.state.http_session)
    except Exception as e:
        logger.error(f"Error starting bot: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start bot: {e!s}")

    # Generate TwiML response to put the caller on hold with music
    # The caller hears this while the bot connects to the Daily room
    # You can replace the URL with your own music file or use Twilio's built-in music
    # See: https://www.twilio.com/docs/voice/twiml/play#music-on-hold
    try:
        resp = VoiceResponse()
        resp.play(
            url="https://therapeutic-crayon-2467.twil.io/assets/US_ringback_tone.mp3",
            loop=10,
        )
        return str(resp)
    except Exception as e:
        logger.error(f"Unexpected error: {e!s}")
        raise HTTPException(status_code=500, detail=f"Server error: {e!s}")


@app.get("/health")
async def health_check():
    """Health check endpoint.

    Returns:
        dict: Status indicating server health
    """
    return {"status": "healthy"}


if __name__ == "__main__":
    # Run the server
    port = int(os.getenv("PORT", "8080"))
    print(f"Starting server on port {port}")
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-in/server_utils.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

import aiohttp
from fastapi import HTTPException, Request
from loguru import logger
from pipecat.runner.daily import DailyRoomConfig, configure
from pydantic import BaseModel


class TwilioCallData(BaseModel):
    """Data received from Twilio call webhook.

    Attributes:
        call_sid: Unique identifier for the call
        from_phone: The caller's phone number
        to_phone: The dialed phone number
    """

    call_sid: str
    from_phone: str
    to_phone: str


class AgentRequest(BaseModel):
    """Request data sent to bot start endpoint.

    Attributes:
        room_url: Daily room URL for the bot to join
        token: Authentication token for the Daily room
        call_sid: Unique identifier for the call
        sip_uri: SIP URI for the call
    """

    room_url: str
    token: str
    call_sid: str
    sip_uri: str


async def twilio_call_data_from_request(request: Request):
    # Get form data from Twilio webhook
    form_data = await request.form()
    data = dict(form_data)

    # Extract call ID (required to forward the call later)
    call_sid = data.get("CallSid")
    if not call_sid:
        raise HTTPException(status_code=400, detail="Missing CallSid in request")

    # Extract the caller's phone number
    from_phone = data.get("From")
    if not from_phone:
        raise HTTPException(status_code=400, detail="Missing From in request")

    # Extract the caller's phone number
    to_phone = data.get("To")
    if not to_phone:
        raise HTTPException(status_code=400, detail="Missing To in request")

    return TwilioCallData(call_sid=call_sid, from_phone=from_phone, to_phone=to_phone)


async def create_daily_room(
    call_data: TwilioCallData, session: aiohttp.ClientSession
) -> DailyRoomConfig:
    """Create a Daily room configured for PSTN dial-in.

    Args:
        call_data: Call data containing caller phone number and call details
        session: Shared aiohttp session for making HTTP requests

    Returns:
        DailyRoomConfig: Configuration object with room_url and token

    Raises:
        HTTPException: If room creation fails
    """
    try:
        return await configure(session, sip_caller_phone=call_data.from_phone)
    except Exception as e:
        logger.error(f"Error creating Daily room: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create Daily room: {e!s}")


async def start_bot_production(agent_request: AgentRequest, session: aiohttp.ClientSession):
    """Start the bot via Pipecat Cloud API for production deployment.

    Args:
        agent_request: Agent configuration with room_url, token, and call details
        session: Shared aiohttp session for making HTTP requests

    Raises:
        HTTPException: If required environment variables are missing or API call fails
    """
    pipecat_api_key = os.getenv("PIPECAT_API_KEY")
    agent_name = os.getenv("PIPECAT_AGENT_NAME")

    if not pipecat_api_key or not agent_name:
        raise HTTPException(
            status_code=500,
            detail="PIPECAT_API_KEY and PIPECAT_AGENT_NAME required for production mode",
        )

    logger.debug(f"Starting bot via Pipecat Cloud for call {agent_request.call_sid}")

    body_data = agent_request.model_dump(exclude_none=True)

    async with session.post(
        f"https://api.pipecat.daily.co/v1/public/{agent_name}/start",
        headers={
            "Authorization": f"Bearer {pipecat_api_key}",
            "Content-Type": "application/json",
        },
        json={
            "createDailyRoom": False,  # We already created the room
            "body": body_data,
        },
    ) as response:
        if response.status != 200:
            error_text = await response.text()
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start bot via Pipecat Cloud: {error_text}",
            )
        logger.debug(f"Bot started successfully via Pipecat Cloud")


async def start_bot_local(agent_request: AgentRequest, session: aiohttp.ClientSession):
    """Start the bot via local /start endpoint for development.

    Args:
        agent_request: Agent configuration with room_url, token, and call details
        session: Shared aiohttp session for making HTTP requests

    Raises:
        HTTPException: If LOCAL_SERVER_URL is not set or API call fails
    """

    local_server_url = os.getenv("LOCAL_SERVER_URL", "http://localhost:7860")

    logger.debug(f"Starting bot via local /start endpoint for call {agent_request.call_sid}")

    body_data = agent_request.model_dump(exclude_none=True)

    async with session.post(
        f"{local_server_url}/start",
        headers={"Content-Type": "application/json"},
        json={
            "createDailyRoom": False,  # We already created the room
            "body": body_data,
        },
    ) as response:
        if response.status != 200:
            error_text = await response.text()
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start bot via local /start endpoint: {error_text}",
            )
        logger.debug(f"Bot started successfully via local /start endpoint")



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/README.md
================================================
# Daily + Twilio SIP dial-out Voice Bot

This project demonstrates how to create a voice bot that uses Daily's SIP capabilities with Twilio to make outbound calls to phone numbers.

## How It Works

1. The server receives a dial-out request with the SIP URI to call
2. The server creates a Daily room with SIP capabilities
3. The server starts the bot process (locally or via Pipecat Cloud based on ENV)
4. The bot joins the room and initiates the dial-out to the specified SIP URI
5. Twilio receives the SIP request and processes it via configured TwiML
6. Twilio rings the number found within the SIP URI
7. The bot automatically retries on failure (up to 5 attempts)
8. When the call is answered, the bot conducts the conversation

## Project Structure

This example is organized to be production-ready and easy to customize:

- **`server.py`** - FastAPI server that handles dial-out requests

  - Receives dial-out requests via `/dialout` endpoint
  - Creates Daily rooms with SIP capabilities
  - Routes to local or production bot deployment
  - Uses shared HTTP session for optimal performance

- **`server_utils.py`** - Utility functions for Twilio and Daily API interactions

  - Data models for dial-out requests and agent configuration
  - Room creation logic
  - Bot starting logic (production and local modes)
  - Easy to extend with custom business logic

- **`bot.py`** - The voice bot implementation
  - `DialoutManager` class for retry logic
  - Handles the conversation with the person being called
  - Deployed to Pipecat Cloud in production or run locally for development

## Prerequisites

### Twilio

- A Twilio account with a phone number that supports voice
- A correctly configured SIP domain (see setup instructions below)

### Daily

- A Daily account with an API key (or Daily API key from Pipecat Cloud account)

### AI Services

- Deepgram API key for speech-to-text
- OpenAI API key for the LLM inference
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager (recommended) or pip
- Docker (for production deployment)

## Setup

1. Create a virtual environment and install dependencies

   ```bash
   uv sync
   ```

2. Set up environment variables

   Copy the example file and fill in your API keys:

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

3. Create a TwiML Bin

   Visit this link to create your [TwiML Bin](https://www.twilio.com/docs/serverless/twiml-bins)

   - Login to the account that has your purchased Twilio phone number
   - Press the plus button on the TwiML Bin dashboard to write a new TwiML that Twilio will host for you
   - Give it a friendly name. For example "daily sip uri twiml bin"
   - For the TWIML code, use something like:

   ```xml
   <?xml version="1.0" encoding="UTF-8"?>
   <Response>
     <Dial answerOnBridge="true" callerId="+1234567890">{{#e164}}{{To}}{{/e164}}</Dial>
   </Response>
   ```

   - callerId must be a valid number that you own on [Twilio](https://console.twilio.com/us1/develop/phone-numbers/manage/incoming)
   - answerOnBridge="true|false" based on your use-case
   - Save the file. We will use this when creating the SIP domain

4. Create and configure a SIP domain

   This allows Daily to make outbound calls through Twilio.

   **Create the SIP Domain:**

   - Go to [Twilio Console > Voice > SIP Domains](https://console.twilio.com/us1/develop/voice/manage/sip-domains)
   - Click the **+** button to create a new domain
   - **Domain Name**: Choose something like `daily.sip.twilio.com`
   - **Friendly Name**: `Daily SIP Domain`

   **Configure Authentication (Allow all traffic):**

   - Under "Voice Authentication", click **+** next to "IP Access Control Lists"
   - Create **first ACL**:
     - **Friendly Name**: `Allow All - Part 1`
     - **CIDR**: `0.0.0.0/1` (covers 0.0.0.0 to 127.255.255.255)
   - Create **second ACL**:
     - **Friendly Name**: `Allow All - Part 2`
     - **CIDR**: `128.0.0.0/1` (covers 128.0.0.0 to 255.255.255.255)
   - Make sure both ACLs are selected in the dropdown

   **Configure Call Handling:**

   - Under "Call Control Configuration":
     - **Configure with**: `TwiML Bins`
     - **A call comes in**: Select your TwiML bin from step 3
   - Click **Save**

   > **Why these settings?** The IP ranges allow Daily's servers to connect from anywhere, and the TwiML bin tells Twilio how to handle the calls.

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server for handling dial-out requests and starting the bot
- Default configuration for development and testing

### Production (`ENV=production`)

- Bot is deployed to Pipecat Cloud; requires `PIPECAT_API_KEY` and `PIPECAT_AGENT_NAME`
- Set these when deploying to production environments
- Your FastAPI server runs either locally or deployed to your infrastructure

## Run the Bot Locally

You'll need two terminal windows open:

1. **Terminal 1**: Start the webhook server:

   ```bash
   uv run server.py
   ```

   This runs on port 8080 and handles dial-out requests.

2. **Terminal 2**: Start the bot server:

   ```bash
   uv run bot.py -t daily
   ```

   This runs on port 7860 and handles the bot logic.

3. **Test the dial-out functionality**

   With both servers running, send a dial-out request:

   ```bash
   curl -X POST "http://localhost:8080/dialout" \
     -H "Content-Type: application/json" \
     -d '{
       "dialout_settings": {
         "sip_uri": "sip:+1234567890@daily.sip.twilio.com"
       }
     }'
   ```

   Replace:

   - The phone number (starting with +1) with the phone number you want to call
   - `daily.sip.twilio.com` with the SIP domain you configured in step 4

   The server will create a room, start the bot, and the bot will dial out to the provided SIP URI. Answer the call to speak with the bot.

## Production Deployment

You can deploy your bot to Pipecat Cloud and server to your infrastructure to run this bot in a production environment.

### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

You'll only deploy your `bot.py` file.

### Deploy the Server

The `server.py` handles dial-out requests and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (receives dial-out requests and starts the bot)

### Environment Variables for Production

Add these to your production environment:

```bash
ENV=production
PIPECAT_API_KEY=your_pipecat_cloud_api_key
PIPECAT_AGENT_NAME=your-agent-name
```

The server automatically detects the environment and routes bot starting requests accordingly.

## Customization

This example is designed to be easily customized for your use case:

### Adding Custom Data to Dial-out Requests

You can extend the `DialoutSettings` model in `server_utils.py` to pass custom data:

```python
class DialoutSettings(BaseModel):
    sip_uri: str
    # Add your custom fields here
    customer_name: str | None = None
    account_id: str | None = None
```

Then populate this data in `server.py` before starting the bot:

```python
# Example: Look up customer information
customer_info = await get_customer_by_phone(dialout_request.dialout_settings.sip_uri)

agent_request = AgentRequest(
    room_url=daily_room_config.room_url,
    token=daily_room_config.token,
    dialout_settings=dialout_request.dialout_settings,
    # Your custom data
    customer_name=customer_info.name,
    account_id=customer_info.id,
)
```

## Troubleshooting

### Call is not being initiated

- Check that your server.py is running on port 8080
- Check that your bot.py is running on port 7860
- Verify that the SIP URI format is correct: `sip:+1234567890@your-domain.sip.twilio.com`

### Call connects but no bot is heard

- Ensure your Daily API key is correct and has SIP capabilities
- Verify that the Cartesia API key and voice ID are correct
- Check that your Twilio SIP domain is correctly configured with the TwiML bin

### Bot starts but disconnects immediately

- Check the Daily logs for any error messages
- Ensure your server has stable internet connectivity
- Verify that your Twilio IP Access Control Lists allow all traffic

### Twilio SIP domain issues

- Make sure both IP ACLs (0.0.0.0/1 and 128.0.0.0/1) are created and selected
- Verify that the TwiML bin has a valid caller ID from your Twilio account
- Check that the SIP domain name matches what you're using in the SIP URI



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Daily + Twilio SIP dial-out voice bot implementation."""

import os
from typing import Any, Optional

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

from server_utils import AgentRequest, DialoutSettings

load_dotenv(override=True)


class DialoutManager:
    """Manages dialout attempts with retry logic.

    Handles the complexity of initiating outbound calls with automatic retry
    on failure, up to a configurable maximum number of attempts.

    Args:
        transport: The Daily transport instance for making the dialout
        dialout_settings: Settings containing SIP URI
        max_retries: Maximum number of dialout attempts (default: 5)
    """

    def __init__(
        self,
        transport: DailyTransport,
        dialout_settings: DialoutSettings,
        max_retries: Optional[int] = 5,
    ):
        self._transport = transport
        self._sip_uri = dialout_settings.sip_uri
        self._max_retries = max_retries
        self._attempt_count = 0
        self._is_successful = False

    async def attempt_dialout(self) -> bool:
        """Attempt to start a dialout call.

        Initiates an outbound call if retry limit hasn't been reached and
        no successful connection has been made yet.

        Returns:
            True if dialout attempt was initiated, False if max retries reached
            or call already successful
        """
        if self._attempt_count >= self._max_retries:
            logger.error(
                f"Maximum retry attempts ({self._max_retries}) reached. Giving up on dialout."
            )
            return False

        if self._is_successful:
            logger.debug("Dialout already successful, skipping attempt")
            return False

        self._attempt_count += 1
        logger.info(
            f"Attempting dialout (attempt {self._attempt_count}/{self._max_retries}) to: {self._sip_uri}"
        )

        await self._transport.start_dialout({"sipUri": self._sip_uri})
        return True

    def mark_successful(self):
        """Mark the dialout as successful to prevent further retry attempts."""
        self._is_successful = True

    def should_retry(self) -> bool:
        """Check if another dialout attempt should be made.

        Returns:
            True if retry limit not reached and call not yet successful
        """
        return self._attempt_count < self._max_retries and not self._is_successful


async def run_bot(
    transport: DailyTransport, dialout_settings: DialoutSettings, handle_sigint: bool
) -> None:
    """Run the voice bot with the given parameters.

    Args:
        transport: The Daily transport instance
        dialout_settings: Settings containing SIP URI for dialout
    """
    logger.info(f"Starting dial-out bot, dialing out to: {dialout_settings.sip_uri}")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    # Create system message and initialize messages list
    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly phone assistant. Your responses will be read aloud, "
                "so keep them concise and conversational. Avoid special characters or "
                "formatting. Begin by greeting the caller and asking how you can help them today."
            ),
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # Build pipeline
    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            stt,
            context_aggregator.user(),  # User responses
            llm,  # LLM
            tts,  # TTS
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    # Create pipeline task
    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
        ),
    )

    # Initialize dialout manager
    dialout_manager = DialoutManager(transport, dialout_settings)

    @transport.event_handler("on_joined")
    async def on_joined(transport, data):
        await dialout_manager.attempt_dialout()

    @transport.event_handler("on_dialout_answered")
    async def on_dialout_answered(transport, data):
        logger.debug(f"Dial-out answered: {data}")
        dialout_manager.mark_successful()

    @transport.event_handler("on_dialout_error")
    async def on_dialout_error(transport, data: Any):
        logger.error(f"Dial-out error, retrying: {data}")

        if dialout_manager.should_retry():
            await dialout_manager.attempt_dialout()
        else:
            logger.error(f"No more retries allowed, stopping bot.")
            await task.cancel()

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    try:
        request = AgentRequest.model_validate(runner_args.body)

        transport = DailyTransport(
            request.room_url,
            request.token,
            "SIP Dial-out Bot",
            params=DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                turn_analyzer=LocalSmartTurnAnalyzerV3(),
            ),
        )

        await run_bot(transport, request.dialout_settings, runner_args.handle_sigint)

    except Exception as e:
        logger.error(f"Error running bot: {e}")
        raise e


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./server_utils.py server_utils.py
COPY ./bot.py bot.py



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/env.example
================================================
# Daily credentials
DAILY_API_KEY=

# Service keys
DEEPGRAM_API_KEY=
OPENAI_API_KEY=
CARTESIA_API_KEY=

# Environment mode: "local" for development, "production" for cloud deployment
ENV=local

# Local bot server URL (where bot.py runs)
LOCAL_SERVER_URL=http://localhost:7860

# Pipecat Cloud (only needed for production)
PIPECAT_API_KEY=
PIPECAT_AGENT_NAME=daily-twilio-sip-dial-out


================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/pcc-deploy.toml
================================================
agent_name = "daily-twilio-sip-dial-out"
image = "your_username/daily-twilio-sip-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "daily-twilio-sip-secrets"
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/pyproject.toml
================================================
[project]
name = "daily-twilio-sip-dial-out"
version = "0.1.0"
description = "Daily SIP + Twilio Dial-out example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,cartesia,deepgram,openai,silero,local-smart-turn-v3,runner]>=0.0.91",
    "pipecatcloud>=0.2.7",
]

[dependency-groups]
dev = [
    "pre-commit~=4.2.0",
    "ruff~=0.12.1",
    "python-dotenv>=1.0.1,<2.0.0",
]

[tool.ruff]
exclude = [".git", "*_pb2.py"]
line-length = 100

[tool.ruff.lint]
select = ["I"]
ignore = []


================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Webhook server to handle dial-out requests and start the voice bot.

This server provides endpoints for initiating outbound calls:
- /start: Main endpoint that receives dial-out requests with SIP URI
- /health: Health check endpoint

The server automatically detects the environment (local vs production) and routes
bot starting requests accordingly:
- Local: Uses internal bot starting logic
- Production: Calls Pipecat Cloud API

All call data (room_url, token, dialout_settings) flows through the body parameter
to ensure consistency between local and cloud deployments.
"""

import os
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
from loguru import logger

from server_utils import (
    AgentRequest,
    create_daily_room,
    dialout_request_from_request,
    start_bot_local,
    start_bot_production,
)

load_dotenv()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifecycle and shared resources.

    Creates a shared aiohttp session for making HTTP requests to bot endpoints.
    The session is reused across requests for better performance through connection pooling.
    """
    # Create shared HTTP session for bot API calls
    app.state.http_session = aiohttp.ClientSession()
    logger.info("Created shared HTTP session")
    yield
    # Clean up: close the session on shutdown
    await app.state.http_session.close()
    logger.info("Closed shared HTTP session")


app = FastAPI(lifespan=lifespan)


@app.post("/dialout")
async def handle_dialout_request(request: Request) -> JSONResponse:
    """Handle dial-out request.

    This endpoint:
    1. Receives dial-out request with SIP URI
    2. Creates a Daily room with SIP capabilities
    3. Starts the bot (locally or via Pipecat Cloud based on ENVIRONMENT)
    4. Returns room details for the client

    Returns:
        JSONResponse with room_url and token
    """
    logger.debug("Received dial-out request")

    dialout_request = await dialout_request_from_request(request)

    daily_room_config = await create_daily_room(dialout_request, request.app.state.http_session)

    agent_request = AgentRequest(
        room_url=daily_room_config.room_url,
        token=daily_room_config.token,
        dialout_settings=dialout_request.dialout_settings,
    )

    try:
        if os.getenv("ENV") == "production":
            await start_bot_production(agent_request, request.app.state.http_session)
        else:
            await start_bot_local(agent_request, request.app.state.http_session)
    except Exception as e:
        logger.error(f"Error starting bot: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to start bot: {str(e)}")

    return JSONResponse(
        {
            "status": "success",
            "room_url": daily_room_config.room_url,
            "token": daily_room_config.token,
            "sip_uri": dialout_request.dialout_settings.sip_uri,
        }
    )


@app.get("/health")
async def health_check():
    """Health check endpoint.

    Returns:
        dict: Status indicating server health
    """
    return {"status": "healthy"}


if __name__ == "__main__":
    # Run the server
    port = int(os.getenv("PORT", "8080"))
    logger.info(f"Starting server on port {port}")
    uvicorn.run("server:app", host="0.0.0.0", port=port, reload=True)



================================================
FILE: phone-chatbot/daily-twilio-sip-dial-out/server_utils.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Utilities for Daily PSTN dial-out handling and bot management.

This module provides data models and functions for:
- Parsing dial-out request data
- Creating Daily rooms for outgoing calls
- Starting bots in production (Pipecat Cloud) or local development mode
"""

import os

import aiohttp
from fastapi import HTTPException, Request
from loguru import logger
from pipecat.runner.daily import DailyRoomConfig, configure
from pydantic import BaseModel


class DialoutSettings(BaseModel):
    """Settings for outbound call.

    Attributes:
        sip_uri: The SIP URI to dial
    """

    sip_uri: str
    # Include any custom data here needed for the call


class DialoutRequest(BaseModel):
    """Request data for initiating a dial-out call.

    Add any custom data here needed for the call. For example,
    you may add customer information, campaign data, or call context.

    Attributes:
        dialout_settings: Settings for the outbound call
    """

    dialout_settings: DialoutSettings


class AgentRequest(BaseModel):
    """Request data sent to bot start endpoint.

    Attributes:
        room_url: Daily room URL for the bot to join
        token: Authentication token for the Daily room
        dialout_settings: Settings for the outbound call
    """

    room_url: str
    token: str
    dialout_settings: DialoutSettings
    # Include any custom data here needed for the agent


async def dialout_request_from_request(request: Request) -> DialoutRequest:
    """Parse and validate dial-out request data.

    Args:
        request: FastAPI request object containing dial-out data

    Returns:
        DialoutRequest: Parsed and validated dial-out request

    Raises:
        HTTPException: If required fields are missing from the request data
    """
    data = await request.json()

    if not data.get("dialout_settings"):
        raise HTTPException(
            status_code=400, detail="Missing 'dialout_settings' in the request body"
        )

    try:
        return DialoutRequest.model_validate(data)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid request data: {str(e)}")


async def create_daily_room(
    dialout_request: DialoutRequest, session: aiohttp.ClientSession
) -> DailyRoomConfig:
    """Create a Daily room configured for SIP dial-out.

    Args:
        dialout_request: Dial-out request containing SIP URI
        session: Shared aiohttp session for making HTTP requests

    Returns:
        DailyRoomConfig: Configuration object with room_url and token

    Raises:
        HTTPException: If room creation fails
    """
    sip_uri = dialout_request.dialout_settings.sip_uri

    if sip_uri.startswith("sip:") and "@" in sip_uri:
        phone_part = sip_uri[4:]  # Remove 'sip:' prefix
        sip_caller_phone = phone_part.split("@")[0]  # Get everything before '@'
    else:
        raise HTTPException(status_code=400, detail="Invalid SIP URI")

    try:
        return await configure(session, sip_caller_phone=sip_caller_phone)
    except Exception as e:
        logger.error(f"Error creating Daily room: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create Daily room: {str(e)}")


async def start_bot_production(agent_request: AgentRequest, session: aiohttp.ClientSession):
    """Start the bot via Pipecat Cloud API for production deployment.

    Args:
        agent_request: Agent configuration with room_url, token, and dialout settings
        session: Shared aiohttp session for making HTTP requests

    Raises:
        HTTPException: If required environment variables are missing or API call fails
    """
    pipecat_api_key = os.getenv("PIPECAT_API_KEY")
    agent_name = os.getenv("PIPECAT_AGENT_NAME")

    if not pipecat_api_key or not agent_name:
        raise HTTPException(
            status_code=500,
            detail="PIPECAT_API_KEY and PIPECAT_AGENT_NAME required for production mode",
        )

    logger.debug(
        f"Starting bot via Pipecat Cloud for dial-out to {agent_request.dialout_settings.sip_uri}"
    )

    body_data = agent_request.model_dump(exclude_none=True)

    async with session.post(
        f"https://api.pipecat.daily.co/v1/public/{agent_name}/start",
        headers={
            "Authorization": f"Bearer {pipecat_api_key}",
            "Content-Type": "application/json",
        },
        json={
            "createDailyRoom": False,  # We already created the room
            "body": body_data,
        },
    ) as response:
        if response.status != 200:
            error_text = await response.text()
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start bot via Pipecat Cloud: {error_text}",
            )
        logger.debug("Bot started successfully via Pipecat Cloud")


async def start_bot_local(agent_request: AgentRequest, session: aiohttp.ClientSession):
    """Start the bot via local /start endpoint for development.

    Args:
        agent_request: Agent configuration with room_url, token, and dialout settings
        session: Shared aiohttp session for making HTTP requests

    Raises:
        HTTPException: If LOCAL_SERVER_URL is not set or API call fails
    """
    local_server_url = os.getenv("LOCAL_SERVER_URL", "http://localhost:7860")

    logger.debug(
        f"Starting bot via local /start endpoint for dial-out to {agent_request.dialout_settings.sip_uri}"
    )

    body_data = agent_request.model_dump(exclude_none=True)

    async with session.post(
        f"{local_server_url}/start",
        headers={"Content-Type": "application/json"},
        json={
            "createDailyRoom": False,  # We already created the room
            "body": body_data,
        },
    ) as response:
        if response.status != 200:
            error_text = await response.text()
            raise HTTPException(
                status_code=500,
                detail=f"Failed to start bot via local /start endpoint: {error_text}",
            )
        logger.debug("Bot started successfully via local /start endpoint")



================================================
FILE: plivo-chatbot/README.md
================================================
# Plivo Voice Bot Examples

This repository contains examples of voice bots that integrate with Plivo's Voice API using Pipecat. The examples demonstrate both inbound and outbound calling scenarios using Plivo's WebSocket streaming for real-time audio processing.

## Examples

### 🔽 [Inbound Calling](./inbound/)

Demonstrates how to handle incoming phone calls where users call your Plivo number and interact with a voice bot.

### 🔼 [Outbound Calling](./outbound/)

Shows how to initiate outbound phone calls programmatically where your bot calls users.

## Architecture

Both examples use the same core architecture:

```
Phone Call ↔ Plivo ↔ WebSocket Stream ↔ Pipecat ↔ AI Services
```

**Components:**

- **Plivo**: Handles phone call routing and audio transport
- **WebSocket Stream**: Real-time bidirectional audio streaming
- **Pipecat**: Audio processing pipeline and AI service orchestration
- **AI Services**: OpenAI (LLM), Deepgram (STT), Cartesia (TTS)

## Getting Help

- **Detailed Setup**: See individual README files in `inbound/` and `outbound/` directories
- **Pipecat Documentation**: [docs.pipecat.ai](https://docs.pipecat.ai)
- **Plivo Documentation**: [plivo.com/docs](https://www.plivo.com/docs)



================================================
FILE: plivo-chatbot/inbound/README.md
================================================
# Plivo Chatbot

This project is a Pipecat-based chatbot that integrates with Plivo to handle WebSocket connections and provide real-time communication. The project includes FastAPI endpoints for starting a call and handling WebSocket connections.

## Table of Contents

- [Features](#features)
- [Requirements](#requirements)
- [Installation](#installation)
- [Configure Plivo URLs](#configure-plivo-urls)
- [Running the Application](#running-the-application)
- [Usage](#usage)

## How It Works

When someone calls your Plivo number:

1. **Plivo calls your webhook**: `GET https://your-server.com/` with call info (From, To, CallUUID)
2. **Server returns XML**: Tells Plivo to start a WebSocket stream to your bot
3. **WebSocket connection**: Audio streams between caller and your bot
4. **Call information**: Phone numbers are passed via query parameters in the WebSocket URL to your bot

## Prerequisites

### Plivo

- A Plivo account with:
  - API Key
  - A purchased phone number that supports voice calls

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

   ```sh
   cd inbound
   uv sync
   ```

2. Create an .env file and add API keys:

   ```sh
   cp env.example .env
   ```

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server or ngrok URL for WebSocket connections
- Default configuration for development and testing
- WebSocket connections go directly to your running server

### Production (`ENV=production`)

- Uses Pipecat Cloud WebSocket URLs automatically
- Requires `AGENT_NAME` and `ORGANIZATION_NAME` from your Pipecat Cloud deployment
- Set these when deploying to production environments
- WebSocket connections route through Pipecat Cloud infrastructure

## Local Development

### Configure Plivo URLs

1. Start ngrok:
   In a new terminal, start ngrok to tunnel the local server:

   ```sh
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

2. Update the Plivo Application:

   - Go to your Plivo console and navigate to Voice > Applications > XML
   - Select "Add New Application" or edit an existing one
   - Set the Primary Answer URL to your ngrok URL: `https://your-subdomain.ngrok.io/`
   - Ensure the Answer Method is set to GET (not POST)
   - Save the application
   - Configure your number to use the newly created (or updated) application:
     - Go to Phone Numbers > Your Numbers
     - Edit your Plivo number
     - Select Application Type: XML Application
     - Plivo Application: Your application
     - Click "Update" to save

The bot automatically receives the caller's and called phone numbers for personalized responses via the body parameter.

### Run the Local Server

`server.py` runs a FastAPI server, which Plivo uses to coordinate the inbound call. Run the server using:

```bash
uv run server.py
```

### Call your Bot

Place a call to the number associated with your bot. The bot will answer and start the conversation.

## Production Deployment

To deploy your plivo-chatbot for inbound calling, we'll use [Pipecat Cloud](https://pipecat.daily.co/).

### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

### Configure Production Environment

Update your production `.env` file with the Pipecat Cloud details:

```bash
# Set to production mode
ENV=production

# Your Pipecat Cloud deployment details
AGENT_NAME=your-agent-name
ORGANIZATION_NAME=your-org-name

# Keep your existing Plivo and AI service keys
```

### Deploy the Server

The `server.py` handles inbound call webhooks and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (receives webhooks, serves XML responses)

When `ENV=production`, the server automatically routes WebSocket connections to your Pipecat Cloud bot.

### Update Plivo Webhook URL

Update your Plivo application's Primary Answer URL to point to your production server instead of ngrok:

- Change from: `https://your-subdomain.ngrok.io/`
- To: `https://your-production-domain.com/`

> Alternatively, you can test your Pipecat Cloud deployment by running your server locally.

### Call your Bot

Place a call to the number associated with your bot. The bot will answer and start the conversation.

## Accessing Call Information in Your Bot

Your bot automatically receives caller information through query parameters in the WebSocket URL. The server extracts the `From` and `To` phone numbers and passes them as `body` data to your bot via the WebsocketRunnerArguments (coming soon!).

### Adding Custom Data

You can include custom data by adding query parameters to your webhook URL in the Plivo console:

**Basic webhook URL:**

```
https://your-server.com/
```

**Webhook URL with custom data:**

```
https://your-server.com/?user_type=premium&campaign=summer2024
```

Your bot will receive:

```json
{
  "from": "+1234567890",
  "to": "+0987654321",
  "user_type": "premium",
  "campaign": "summer2024"
}
```

This allows your bot to provide personalized responses based on who's calling and any custom context you provide.



================================================
FILE: plivo-chatbot/inbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
from asyncio.transports import BaseTransport

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.plivo import PlivoFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv()


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": "You are an elementary teacher in an audio call. Your output will be converted to audio so don't include special characters in your answers. Respond to what the student said in a short short sentence.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
    logger.info(f"Auto-detected transport: {transport_type}")

    serializer = PlivoFrameSerializer(
        stream_id=call_data["stream_id"],
        call_id=call_data["call_id"],
        auth_id=os.getenv("PLIVO_AUTH_ID", ""),
        auth_token=os.getenv("PLIVO_AUTH_TOKEN", ""),
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)



================================================
FILE: plivo-chatbot/inbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: plivo-chatbot/inbound/env.example
================================================
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=
PLIVO_AUTH_ID=
PLIVO_AUTH_TOKEN=

# Environment configuration
ENV=local
AGENT_NAME=plivo-chatbot-dial-in
ORGANIZATION_NAME=


================================================
FILE: plivo-chatbot/inbound/pcc-deploy.toml
================================================
agent_name = "plivo-chatbot-dial-in"
image = "your_username/plivo-chatbot-dial-in:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "plivo-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: plivo-chatbot/inbound/pyproject.toml
================================================
[project]
name = "plivo-chatbot-dial-in"
version = "0.1.0"
description = "Plivo dial-in example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,openai,silero,deepgram,runner]>=0.0.86",
  "pipecatcloud>=0.2.4"
]



================================================
FILE: plivo-chatbot/inbound/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""An example server for Plivo to start WebSocket streaming to Pipecat Cloud."""

import base64
import json
import os

import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query, Request, WebSocket
from starlette.responses import Response

# Load environment variables from .env file
load_dotenv()

app = FastAPI(title="Plivo XML Server", description="Serves XML for Plivo WebSocket streaming")


def get_websocket_url(host: str, body_data: dict = None):
    """Construct WebSocket URL based on environment variables with query parameters."""
    env = os.getenv("ENV", "local").lower()

    # Build query parameters
    query_params = []

    if env == "production":
        agent_name = os.getenv("AGENT_NAME")
        org_name = os.getenv("ORGANIZATION_NAME")

        if not agent_name or not org_name:
            raise ValueError(
                "AGENT_NAME and ORGANIZATION_NAME must be set in environment variables for production"
            )

        service_host = f"{agent_name}.{org_name}"
        query_params.append(f"serviceHost={service_host}")
        print("If deployed in a region other than us-west (default), update websocket url!")
        base_url = "wss://api.pipecat.daily.co/ws/plivo"
        # uncomment appropriate region url:
        # base_url = wss://us-east.api.pipecat.daily.co/ws/plivo
        # base_url = wss://eu-central.api.pipecat.daily.co/ws/plivo
        # base_url = wss://ap-south.api.pipecat.daily.co/ws/plivo
    else:
        base_url = f"wss://{host}/ws"

    # Add body data as query parameter
    if body_data:
        body_json = json.dumps(body_data)
        body_encoded = base64.b64encode(body_json.encode("utf-8")).decode("utf-8")
        query_params.append(f"body={body_encoded}")

    # Construct final URL
    if query_params:
        return f"{base_url}?{'&amp;'.join(query_params)}"
    else:
        return base_url


@app.get("/")
async def start_call(
    request: Request,
    # Optional Plivo parameters that are automatically passed by Plivo
    CallUUID: str = Query(None, description="Plivo call UUID"),
    From: str = Query(None, description="Caller's phone number"),
    To: str = Query(None, description="Called phone number"),
):
    """
    Returns XML for Plivo to start WebSocket streaming with call information

    Agent and organization names are configured via environment variables:
    - AGENT_NAME: Your deployed agent name
    - ORGANIZATION_NAME: Your Pipecat Cloud organization

    For local development, set ENV=local in your .env file.
    For production, set ENV=production with AGENT_NAME and ORGANIZATION_NAME.

    Optional parameters (automatically passed by Plivo):
    - CallUUID, From, To

    Example webhook URL: https://your-domain.com/
    """
    print("GET Plivo XML")

    # Create body data with phone numbers only
    body_data = {}

    # Always include phone numbers if available
    if From:
        body_data["from"] = From
    if To:
        body_data["to"] = To

    # Log call details
    if CallUUID:
        print(f"Plivo inbound call: {From} → {To}, UUID: {CallUUID}")
        if body_data:
            print(f"Body data: {body_data}")

    # Validate environment configuration
    env = os.getenv("ENV", "local").lower()
    if env == "production":
        if not os.getenv("AGENT_NAME") or not os.getenv("ORGANIZATION_NAME"):
            raise HTTPException(
                status_code=500,
                detail="AGENT_NAME and ORGANIZATION_NAME must be set for production deployment",
            )

    # Get request host and construct WebSocket URL with body data
    host = request.headers.get("host")
    if not host:
        raise HTTPException(status_code=400, detail="Unable to determine server host")

    websocket_url = get_websocket_url(host, body_data if body_data else None)

    # Build XML without extraHeaders (using query parameters instead)
    xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Stream bidirectional="true" keepCallAlive="true" contentType="audio/x-mulaw;rate=8000">
    {websocket_url}
  </Stream>
</Response>"""
    print(f"Generated XML: {xml}")
    return Response(content=xml, media_type="application/xml")


@app.websocket("/ws")
async def websocket_endpoint(
    websocket: WebSocket,
    body: str = Query(None),
    serviceHost: str = Query(None),
):
    """Handle WebSocket connections for inbound calls."""
    await websocket.accept()
    print("WebSocket connection accepted for inbound call")

    print(f"Received query params - body: {body}, serviceHost: {serviceHost}")

    # Decode body parameter if provided
    body_data = {}
    if body:
        try:
            # Base64 decode the JSON (it was base64-encoded in the webhook handler)
            decoded_json = base64.b64decode(body).decode("utf-8")
            body_data = json.loads(decoded_json)
            print(f"Decoded body data: {body_data}")
        except Exception as e:
            print(f"Error decoding body parameter: {e}")
    else:
        print("No body parameter received")

    try:
        # Import the bot function from the bot module
        from bot import bot
        from pipecat.runner.types import WebSocketRunnerArguments

        # Create runner arguments and run the bot
        runner_args = WebSocketRunnerArguments(websocket=websocket)
        runner_args.handle_sigint = False

        # TODO: When WebSocketRunnerArguments supports body, add it here:
        # runner_args = WebSocketRunnerArguments(websocket=websocket, body=body_data)

        await bot(runner_args)

    except Exception as e:
        print(f"Error in WebSocket endpoint: {e}")
        await websocket.close()


if __name__ == "__main__":
    # Run the server on port 7860
    # Use with ngrok: ngrok http 7860
    uvicorn.run(app, host="0.0.0.0", port=7860)



================================================
FILE: plivo-chatbot/outbound/README.md
================================================
# Plivo Chatbot: Outbound

This project is a Pipecat-based chatbot that integrates with Plivo to make outbound calls with personalized custom data. The project includes FastAPI endpoints for initiating outbound calls and handling WebSocket connections with custom data.

## How It Works

When you want to make an outbound call:

1. **Send POST request**: `POST /start` with a phone number to call
2. **Server initiates call**: Uses Plivo's REST API to make the outbound call
3. **Call answered**: When answered, Plivo fetches XML from your server's `/answer` endpoint
4. **Server returns XML**: Tells Plivo to start a WebSocket stream to your bot
5. **WebSocket connection**: Audio streams between the called person and your bot
6. **Call information**: Phone numbers and custom data are passed via query parameters in the WebSocket URL to your bot

## Architecture

```
curl request → /start endpoint → Plivo REST API → Call initiated →
Answer XML fetched → WebSocket connection → Bot conversation
```

## Prerequisites

### Plivo

- A Plivo account with:
  - Auth ID and Auth Token
  - A purchased phone number that supports voice calls

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

```bash
cd outbound
uv sync
```

2. Get your Plivo credentials:

- **Auth ID and Auth Token**: Found in your [Plivo Console](https://console.plivo.com/dashboard/)
- **Phone Number**: [Purchase a phone number](https://console.plivo.com/phone-numbers/search/) that supports voice calls

3. Set up environment variables:

```bash
cp env.example .env
# Edit .env with your API keys
```

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server or ngrok URL for WebSocket connections
- Default configuration for development and testing
- WebSocket connections go directly to your running server

### Production (`ENV=production`)

- Uses Pipecat Cloud WebSocket URLs automatically
- Requires `AGENT_NAME` and `ORGANIZATION_NAME` from your Pipecat Cloud deployment
- Set these when deploying to production environments
- WebSocket connections route through Pipecat Cloud infrastructure

## Local Development

1. Start the outbound bot server:

   ```bash
   uv run server.py
   ```

The server will start on port 7860.

2. Using a new terminal, expose your server to the internet (for development)

   ```bash
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

   Copy the ngrok URL (e.g., `https://abc123.ngrok.io`)

3. No additional Plivo configuration needed

   Unlike inbound calling, outbound calls don't require webhook configuration in the Plivo console. The server will make direct API calls to Plivo to initiate calls.

## Making an Outbound Call

With the server running and exposed via ngrok, you can initiate outbound calls:

### Basic Call

```bash
curl -X POST https://your-ngrok-url.ngrok.io/start \
  -H "Content-Type: application/json" \
  -d '{
    "phone_number": "+1234567890"
  }'
```

### Call with Custom Data

You can include custom data that will be available to your bot:

```bash
curl -X POST https://your-ngrok-url.ngrok.io/start \
  -H "Content-Type: application/json" \
  -d '{
    "phone_number": "+1234567890",
    "body": {
      "user": {
        "id": "user123",
        "firstName": "John",
        "lastName": "Doe",
        "accountType": "premium"
      }
    }
  }'
```

The body data can be any JSON structure - nested objects, arrays, etc. Your bot will receive this data automatically.

Replace:

- `your-ngrok-url.ngrok.io` with your actual ngrok URL
- `+1234567890` with the phone number you want to call

## Production Deployment

### 1. Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to Pipecat Cloud.

### 2. Configure Production Environment

Update your production `.env` file with the Pipecat Cloud details:

```bash
# Set to production mode
ENV=production

# Your Pipecat Cloud deployment details
AGENT_NAME=your-agent-name
ORGANIZATION_NAME=your-org-name

# Keep your existing Plivo and AI service keys
```

### 3. Deploy the Server

The `server.py` handles outbound call initiation and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (initiates calls, serves XML responses)

When `ENV=production`, the server automatically routes WebSocket connections to your Pipecat Cloud bot.

> Alternatively, you can test your Pipecat Cloud deployment by running your server locally.

### Call your Bot

As you did before, initiate a call via `curl` command to trigger your bot to dial a number.

## Accessing Custom Data in Your Bot

Your bot receives custom data through `runner_args.body`:

```python
async def bot(runner_args: RunnerArguments):
    body_data = runner_args.body or {}
    first_name = body_data.get("user", {}).get("firstName", "there")
    # Use first_name to personalize greetings, prompts, etc.
```

This approach works consistently in both local development and Pipecat Cloud production deployments.



================================================
FILE: plivo-chatbot/outbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.plivo import PlivoFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly assistant. "
                "Your responses will be read aloud, so keep them concise and conversational. "
                "Avoid special characters or formatting. "
                "Begin by saying: 'Hello! This is an automated call from our Plivo chatbot demo.' "
            ),
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Kick off the outbound conversation, waiting for the user to speak first
        logger.info("Starting outbound call conversation")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Outbound call ended")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
    logger.info(f"Auto-detected transport: {transport_type}")

    serializer = PlivoFrameSerializer(
        stream_id=call_data["stream_id"],
        call_id=call_data["call_id"],
        auth_id=os.getenv("PLIVO_AUTH_ID", ""),
        auth_token=os.getenv("PLIVO_AUTH_TOKEN", ""),
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)



================================================
FILE: plivo-chatbot/outbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: plivo-chatbot/outbound/env.example
================================================
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=
PLIVO_AUTH_ID=
PLIVO_AUTH_TOKEN=

# Your Plivo phone number for outbound calls
PLIVO_PHONE_NUMBER=

# Environment configuration
ENV=local
AGENT_NAME=plivo-chatbot-dial-out
ORGANIZATION_NAME=


================================================
FILE: plivo-chatbot/outbound/pcc-deploy.toml
================================================
agent_name = "plivo-chatbot-dial-out"
image = "your_username/plivo-chatbot-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "plivo-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: plivo-chatbot/outbound/pyproject.toml
================================================
[project]
name = "plivo-chatbot-dial-out"
version = "0.1.0"
description = "Plivo dial-out example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,openai,silero,deepgram,runner]>=0.0.86",
  "pipecatcloud>=0.2.4"
]



================================================
FILE: plivo-chatbot/outbound/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""server.py

Webhook server to handle outbound call requests, initiate calls via Plivo API,
and handle subsequent WebSocket connections for Media Streams.
"""

import base64
import json
import os
import urllib.parse
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query, Request, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse

load_dotenv(override=True)


# ----------------- HELPERS ----------------- #


async def make_plivo_call(
    session: aiohttp.ClientSession, to_number: str, from_number: str, answer_url: str
):
    """Make an outbound call using Plivo's REST API."""
    auth_id = os.getenv("PLIVO_AUTH_ID")
    auth_token = os.getenv("PLIVO_AUTH_TOKEN")

    if not auth_id:
        raise ValueError("Missing Plivo Auth ID (PLIVO_AUTH_ID)")

    if not auth_token:
        raise ValueError("Missing Plivo Auth Token (PLIVO_AUTH_TOKEN)")

    headers = {
        "Content-Type": "application/json",
    }

    data = {
        "to": to_number,
        "from": from_number,
        "answer_url": answer_url,
        "answer_method": "GET",
    }

    url = f"https://api.plivo.com/v1/Account/{auth_id}/Call/"

    # Use HTTP Basic Auth
    auth = aiohttp.BasicAuth(auth_id, auth_token)

    async with session.post(url, headers=headers, json=data, auth=auth) as response:
        if response.status != 201:
            error_text = await response.text()
            raise Exception(f"Plivo API error ({response.status}): {error_text}")

        result = await response.json()
        return result


def get_websocket_url(host: str):
    """Construct WebSocket URL based on environment variables."""
    env = os.getenv("ENV", "local").lower()

    if env == "production":
        print("If deployed in a region other than us-west (default), update websocket url!")
        ws_url = "wss://api.pipecat.daily.co/ws/plivo"
        # uncomment appropriate region url:
        # ws_url = wss://us-east.api.pipecat.daily.co/ws/plivo
        # ws_url = wss://eu-central.api.pipecat.daily.co/ws/plivo
        # ws_url = wss://ap-south.api.pipecat.daily.co/ws/plivo
        return ws_url
    else:
        return f"wss://{host}/ws"


# ----------------- API ----------------- #


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create aiohttp session for Plivo API calls
    app.state.session = aiohttp.ClientSession()
    yield
    # Close session when shutting down
    await app.state.session.close()


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for testing
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/start")
async def initiate_outbound_call(request: Request) -> JSONResponse:
    """Handle outbound call request and initiate call via Plivo."""
    print("Received outbound call request")

    try:
        data = await request.json()

        # Validate request data
        if not data.get("phone_number"):
            raise HTTPException(
                status_code=400, detail="Missing 'phone_number' in the request body"
            )

        # Extract the phone number to dial
        phone_number = str(data["phone_number"])

        # Extract body data if provided
        body_data = data.get("body", {})
        print(f"Processing outbound call to {phone_number}")

        # Get server URL for answer URL
        host = request.headers.get("host")
        if not host:
            raise HTTPException(status_code=400, detail="Unable to determine server host")

        # Use https for production, http for localhost
        protocol = (
            "https"
            if not host.startswith("localhost") and not host.startswith("127.0.0.1")
            else "http"
        )

        # Add body data as query parameters to answer URL
        answer_url = f"{protocol}://{host}/answer"
        if body_data:
            body_json = json.dumps(body_data)
            body_encoded = urllib.parse.quote(body_json)
            answer_url = f"{answer_url}?body_data={body_encoded}"

        # Initiate outbound call via Plivo
        try:
            call_result = await make_plivo_call(
                session=request.app.state.session,
                to_number=phone_number,
                from_number=os.getenv("PLIVO_PHONE_NUMBER"),
                answer_url=answer_url,
            )

            # Extract call UUID from Plivo response
            call_uuid = (
                call_result.get("request_uuid") or call_result.get("message_uuid") or "unknown"
            )

        except Exception as e:
            print(f"Error initiating Plivo call: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to initiate call: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

    return JSONResponse(
        {
            "call_uuid": call_uuid,
            "status": "call_initiated",
            "phone_number": phone_number,
        }
    )


@app.get("/answer")
async def get_answer_xml(
    request: Request,
    CallUUID: str = Query(None, description="Plivo call UUID"),
    body_data: str = Query(None, description="JSON encoded body data"),
) -> HTMLResponse:
    """Return XML instructions for connecting call to WebSocket."""
    print("Serving answer XML for outbound call")

    # Parse body data from query parameter
    parsed_body_data = {}
    if body_data:
        try:
            parsed_body_data = json.loads(body_data)
        except json.JSONDecodeError:
            print(f"Failed to parse body data: {body_data}")

    # Log call details
    if CallUUID:
        print(f"Plivo outbound call UUID: {CallUUID}")
        if parsed_body_data:
            print(f"Body data: {parsed_body_data}")

    try:
        # Get the server host to construct WebSocket URL
        host = request.headers.get("host")
        if not host:
            raise HTTPException(status_code=400, detail="Unable to determine server host")

        # Get base WebSocket URL
        base_ws_url = get_websocket_url(host)

        # Add query parameters to WebSocket URL
        query_params = []

        # Add serviceHost for production
        env = os.getenv("ENV", "local").lower()
        if env == "production":
            agent_name = os.getenv("AGENT_NAME")
            org_name = os.getenv("ORGANIZATION_NAME")
            service_host = f"{agent_name}.{org_name}"
            query_params.append(f"serviceHost={service_host}")

        # Add body data if available
        if parsed_body_data:
            body_json = json.dumps(parsed_body_data)
            body_encoded = base64.b64encode(body_json.encode("utf-8")).decode("utf-8")
            query_params.append(f"body={body_encoded}")

        # Construct final WebSocket URL
        if query_params:
            ws_url = f"{base_ws_url}?{'&amp;'.join(query_params)}"
        else:
            ws_url = base_ws_url

        # Generate XML response for Plivo
        xml_content = f"""<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Stream bidirectional="true" keepCallAlive="true" contentType="audio/x-mulaw;rate=8000">
        {ws_url}
    </Stream>
</Response>"""

        return HTMLResponse(content=xml_content, media_type="application/xml")

    except Exception as e:
        print(f"Error generating answer XML: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to generate XML: {str(e)}")


@app.websocket("/ws")
async def websocket_endpoint(
    websocket: WebSocket,
    body: str = Query(None),
    serviceHost: str = Query(None),
):
    """Handle WebSocket connection from Plivo Media Streams."""
    await websocket.accept()
    print("WebSocket connection accepted for outbound call")

    print(f"Received query params - body: {body}, serviceHost: {serviceHost}")

    # Decode body parameter if provided
    body_data = {}
    if body:
        try:
            # Base64 decode the JSON (it was base64-encoded in the answer endpoint)
            decoded_json = base64.b64decode(body).decode("utf-8")
            body_data = json.loads(decoded_json)
            print(f"Decoded body data: {body_data}")
        except Exception as e:
            print(f"Error decoding body parameter: {e}")
    else:
        print("No body parameter received")

    try:
        # Import the bot function from the bot module
        from bot import bot
        from pipecat.runner.types import WebSocketRunnerArguments

        # Create runner arguments with body data
        runner_args = WebSocketRunnerArguments(websocket=websocket, body=body_data)

        await bot(runner_args)

    except Exception as e:
        print(f"Error in WebSocket endpoint: {e}")
        await websocket.close()


# ----------------- Main ----------------- #


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)



================================================
FILE: push-to-talk/README.md
================================================
# Push-to-Talk Voice AI Agent

A voice AI agent with push-to-talk functionality. Users hold a button to speak, and release it to send their message to the AI.

## How it Works

- **Client**: Hold the "Hold to Talk" button to speak, release to send
- **Server**: Audio input is gated, only flowing to the STT processor when the button is pressed
- **Real-time**: Uses WebRTC for low-latency audio communication

## Quick Start

To run this demo, you'll need two terminal windows.

### Terminal 1: Server Setup

1. Create virtual environment and install dependencies:

```bash
uv sync
```

2. Configure environment:

```bash
cp env.example .env
```

Edit `.env` and add your API keys:

- `CARTESIA_API_KEY`: For text-to-speech
- `OPENAI_API_KEY`: For the LLM
- `DEEPGRAM_API_KEY`: For speech-to-text
- `DAILY_API_KEY`: For WebRTC transport

3. Run the server:

```bash
uv run bot.py -t daily
```

### Terminal 2: Client Setup

1. Install dependencies:

```bash
npm i
```

2. Configure environment:

```bash
cp env.example .env.local
```

3. Start the client:

```bash
npm run dev
```

4. Open [http://localhost:3000](http://localhost:3000)

## Usage

1. Click "Connect" to join the session
2. Hold "Hold to Talk" button and speak
3. Release the button to send your message
4. The AI will respond with audio
5. Click "Disconnect" to end the session

## Architecture

The push-to-talk functionality uses client-server message passing:

- Client sends `{type: "push_to_talk", data: {state: "start"}}` when button is pressed
- Server opens audio input gate, allowing frames to flow to STT
- Client sends `{type: "push_to_talk", data: {state: "stop"}}` when button is released
- Server closes audio input gate, triggering transcript processing

## Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to Pipecat Cloud.



================================================
FILE: push-to-talk/client/env.example
================================================
# The localhost address will allow you to run locally.
# You can deploy to Pipecat Cloud by setting the BOT_START_URL
# to your agent and then also provide your Pipecat Cloud API key.
BOT_START_URL="http://localhost:7860/start"
BOT_START_PUBLIC_API_KEY=""


================================================
FILE: push-to-talk/client/eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;



================================================
FILE: push-to-talk/client/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {};

export default nextConfig;



================================================
FILE: push-to-talk/client/package.json
================================================
{
  "name": "02-components",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@pipecat-ai/daily-transport": "^1.4.1",
    "@pipecat-ai/voice-ui-kit": "^0.4.0",
    "lucide-react": "^0.511.0",
    "next": "15.3.4",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "three": "^0.180.0",
    "tw-animate-css": "^1.3.5"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.3.4",
    "tailwindcss": "^4",
    "typescript": "^5"
  }
}



================================================
FILE: push-to-talk/client/postcss.config.mjs
================================================
const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;



================================================
FILE: push-to-talk/client/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
FILE: push-to-talk/client/src/app/globals.css
================================================
@import 'tailwindcss';
@import 'tw-animate-css';

@import '@pipecat-ai/voice-ui-kit/styles';



================================================
FILE: push-to-talk/client/src/app/layout.tsx
================================================
import type { Metadata } from 'next';
import { Geist, Geist_Mono } from 'next/font/google';

import './globals.css';

const geistSans = Geist({
  variable: '--font-geist-sans',
  subsets: ['latin'],
});

const geistMono = Geist_Mono({
  variable: '--font-geist-mono',
  subsets: ['latin'],
});

export const metadata: Metadata = {
  title: 'Push-to-Talk Voice AI Agent',
  icons: {
    icon: '/favicon.svg',
  },
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={`${geistSans.variable} ${geistMono.variable} antialiased`}>
        {children}
      </body>
    </html>
  );
}



================================================
FILE: push-to-talk/client/src/app/page.tsx
================================================
'use client';

import {
  FullScreenContainer,
  ThemeProvider,
  PipecatAppBase,
  SpinLoader,
  type PipecatBaseChildProps,
} from '@pipecat-ai/voice-ui-kit';
import { App } from './components/App';

export default function Home() {
  return (
    <ThemeProvider>
      <FullScreenContainer>
        <PipecatAppBase
          transportType="daily"
          connectParams={{
            endpoint: '/api/connect',
          }}>
          {({
            client,
            handleConnect,
            handleDisconnect,
            error,
          }: PipecatBaseChildProps) =>
            !client ? (
              <SpinLoader />
            ) : (
              <App
                handleConnect={handleConnect}
                handleDisconnect={handleDisconnect}
                error={error}
              />
            )
          }
        </PipecatAppBase>
      </FullScreenContainer>
    </ThemeProvider>
  );
}



================================================
FILE: push-to-talk/client/src/app/api/connect/route.ts
================================================
import { NextResponse } from 'next/server';

export async function POST() {
  // Use BOT_START_URL from environment or fallback to localhost
  const botStartUrl =
    process.env.BOT_START_URL || 'http://localhost:7860/start';

  try {
    // Prepare headers - make API key optional
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
    };

    // Only add Authorization header if API key is provided
    if (process.env.BOT_START_PUBLIC_API_KEY) {
      headers.Authorization = `Bearer ${process.env.BOT_START_PUBLIC_API_KEY}`;
    }

    const response = await fetch(botStartUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        createDailyRoom: true,
        dailyRoomProperties: { start_video_off: true },
      }),
    });

    if (!response.ok) {
      throw new Error(`Failed to connect to Pipecat: ${response.statusText}`);
    }

    const data = await response.json();

    if (data.error) {
      throw new Error(data.error);
    }

    return NextResponse.json(data);
  } catch (error) {
    return NextResponse.json(
      { error: `Failed to process connection request: ${error}` },
      { status: 500 }
    );
  }
}



================================================
FILE: push-to-talk/client/src/app/components/App.tsx
================================================
import {
  Button,
  ConnectButton,
  ControlBar,
  ErrorCard,
  PipecatLogo,
  TranscriptOverlay,
  UserAudioControl,
  usePipecatConnectionState,
} from '@pipecat-ai/voice-ui-kit';
import { PlasmaVisualizer } from '@pipecat-ai/voice-ui-kit/webgl';
import { LogOutIcon, XIcon, MicIcon } from 'lucide-react';
import { usePipecatClient } from '@pipecat-ai/client-react';
import { useCallback, useState } from 'react';

export interface AppProps {
  handleConnect?: () => void | Promise<void>;
  handleDisconnect?: () => void | Promise<void>;
  error?: string | null;
}

export type PushToTalkState = 'idle' | 'talking';

const PushToTalkButton = () => {
  const client = usePipecatClient();
  const [pushToTalkState, setPushToTalkState] =
    useState<PushToTalkState>('idle');

  const handlePushToTalk = useCallback(() => {
    if (!client || client.state !== 'ready') {
      return;
    }

    if (pushToTalkState === 'idle') {
      // Start talking
      setPushToTalkState('talking');
      client.sendClientMessage('push_to_talk', { state: 'start' });
    } else {
      // Stop talking
      setPushToTalkState('idle');
      client.sendClientMessage('push_to_talk', { state: 'stop' });
    }
  }, [client, pushToTalkState]);

  const isReady = client && client.state === 'ready';

  return (
    <Button
      size="xl"
      variant={pushToTalkState === 'talking' ? 'destructive' : 'primary'}
      disabled={!isReady}
      onMouseDown={handlePushToTalk}
      onMouseUp={handlePushToTalk}
      onTouchStart={handlePushToTalk}
      onTouchEnd={handlePushToTalk}
      className={`transition-all duration-200 select-none ${
        pushToTalkState === 'talking' ? 'scale-105' : ''
      } flex items-center gap-2`}>
      <MicIcon size={20} />
      {pushToTalkState === 'talking' ? 'Release to Send' : 'Hold to Talk'}
    </Button>
  );
};

export const App = ({ handleConnect, handleDisconnect, error }: AppProps) => {
  const { isConnected } = usePipecatConnectionState();

  if (error) {
    return (
      <ErrorCard error={error} title="An error occured connecting to agent." />
    );
  }

  return (
    <div className="w-full h-screen">
      <div className="flex flex-col h-full">
        <div className="relative bg-background overflow-hidden flex-1 shadow-long/[0.02]">
          <main className="flex flex-col gap-0 h-full relative justify-end items-center">
            <PlasmaVisualizer />
            <div className="absolute w-full h-full flex items-center justify-center">
              <ConnectButton
                size="xl"
                onConnect={handleConnect}
                onDisconnect={handleDisconnect}
              />
            </div>
            <div className="absolute w-full h-full flex items-center justify-center pointer-events-none">
              <TranscriptOverlay participant="remote" className="max-w-md" />
            </div>
            {isConnected && (
              <>
                <div className="absolute bottom-32 left-1/2 transform -translate-x-1/2 z-20">
                  <PushToTalkButton />
                </div>
                <ControlBar>
                  <UserAudioControl />
                  <Button
                    size="xl"
                    isIcon={true}
                    variant="outline"
                    onClick={handleDisconnect}>
                    <LogOutIcon />
                  </Button>
                </ControlBar>
              </>
            )}
          </main>
        </div>
        <footer className="p-5 md:p-7 text-center flex flex-row gap-4 items-center justify-center">
          <PipecatLogo className="h-[24px] w-auto text-gray-500" />
          <div className="flex flex-row gap-2 items-center justify-center opacity-60">
            <p className="text-sm text-muted-foreground font-medium">
              Pipecat AI
            </p>
            <XIcon size={16} className="text-gray-400" />
            <p className="text-sm text-muted-foreground font-medium">
              Voice UI Kit
            </p>
          </div>
        </footer>
      </div>
    </div>
  );
};

export default App;



================================================
FILE: push-to-talk/server/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import (
    Frame,
    InputAudioRawFrame,
    InterruptionFrame,
    LLMRunFrame,
    StartFrame,
    UserStartedSpeakingFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import (
    RTVIClientMessageFrame,
    RTVIConfig,
    RTVIObserver,
    RTVIProcessor,
)
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams

load_dotenv(override=True)

# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


class PushToTalkGate(FrameProcessor):
    def __init__(self):
        super().__init__()
        self._gate_opened = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, StartFrame):
            await self.push_frame(frame, direction)

        elif isinstance(frame, RTVIClientMessageFrame):
            self._handle_rtvi_frame(frame)
            await self.push_frame(frame, direction)

        # If the gate is closed, suppress all audio frames until the user releases the button
        # We don't include the UserStoppedSpeakingFrame because it's an important signal to tell
        # the UserContextAggregator that the user is done speaking and to push the aggregation.
        if not self._gate_opened and isinstance(
            frame,
            (
                InputAudioRawFrame,
                UserStartedSpeakingFrame,
                InterruptionFrame,
            ),
        ):
            logger.trace(f"{frame.__class__.__name__} suppressed - Button not pressed")
        else:
            await self.push_frame(frame, direction)

    def _handle_rtvi_frame(self, frame: RTVIClientMessageFrame):
        if frame.type == "push_to_talk" and frame.data:
            data = frame.data
            if data.get("state") == "start":
                self._gate_opened = True
                logger.info("Input gate opened - user started talking")
            elif data.get("state") == "stop":
                self._gate_opened = False
                logger.info("Input gate closed - user stopped talking")


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    push_to_talk_gate = PushToTalkGate()

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            rtvi,
            push_to_talk_gate,
            stt,
            context_aggregator.user(),  # User responses
            llm,  # LLM
            tts,  # TTS
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: push-to-talk/server/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: push-to-talk/server/env.example
================================================
CARTESIA_API_KEY=your_api_key
DEEPGRAM_API_KEY=your_api_key
OPENAI_API_KEY=your_api_key
DAILY_API_KEY=your_api_key


================================================
FILE: push-to-talk/server/pcc-deploy.toml
================================================
agent_name = "push-to-talk"
image = "your_username/push-to-talk:0.1"
secret_set = "push-to-talk-secrets"

[scaling]
	min_agents = 1



================================================
FILE: push-to-talk/server/pyproject.toml
================================================
[project]
name = "push-to-talk"
version = "0.1.0"
description = "Push to talk example"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,openai,silero,cartesia,runner]>=0.0.89",
    "pipecatcloud>=0.2.6"
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: runner-examples/README.md
================================================
# Pipecat Development Runner Examples

Examples demonstrating the unified development runner for building voice AI bots across multiple transport types.

## Prerequisites

1. Set up venv and install dependencies:

```bash
uv sync
```

2. Set up your API keys in `.env`:

```bash
# Required for all examples
DEEPGRAM_API_KEY=your_deepgram_key
CARTESIA_API_KEY=your_cartesia_key
OPENAI_API_KEY=your_openai_key

# For Daily transport
DAILY_API_KEY=your_daily_key

# For telephony transports
TWILIO_ACCOUNT_SID=your_twilio_sid
TWILIO_AUTH_TOKEN=your_twilio_token
TELNYX_API_KEY=your_telnyx_key
PLIVO_AUTH_ID=your_plivo_id
PLIVO_AUTH_TOKEN=your_plivo_token
```

## Examples

### 01-single-transport-bot.py

Basic WebRTC bot demonstrating the runner fundamentals.

```bash
python 01-single-transport-bot.py
# Opens http://localhost:7860/client
```

### 02-two-transport-bot.py

Bot supporting Daily and WebRTC with manual transport detection.

```bash
# WebRTC (default)
python 02-two-transport-bot.py

# Daily
python 02-two-transport-bot.py -t daily
```

### 03-all-transport-bot.py

Comprehensive bot supporting all five transport types with manual setup and telephony auto-detection.

```bash
# WebRTC
python 03-all-transport-bot.py

# Daily
python 03-all-transport-bot.py -t daily

# Telephony (requires public proxy)
python 03-all-transport-bot.py -t twilio -x yourproxy.ngrok.io
python 03-all-transport-bot.py -t telnyx -x yourproxy.ngrok.io
python 03-all-transport-bot.py -t plivo -x yourproxy.ngrok.io
```

### 04-all-transport-factory-bot.py

Clean implementation using the `create_transport` utility with factory functions.

```bash
# Same usage as 03, but with simplified code
python 04-all-transport-factory-bot.py -t daily
```

## Key Concepts

- **Runner**: HTTP service that spawns bots on-demand and handles transport infrastructure
- **Runner Arguments**: Transport-specific data passed to your bot (room URLs, WebSocket connections, etc.)
- **Transport Factory**: Clean pattern for configuring multiple transports with lazy instantiation
- **Environment Detection**: `ENV=local` enables conditional features like Krisp audio filtering

## Transport Types

| Transport | Usage                                       | Access                       |
| --------- | ------------------------------------------- | ---------------------------- |
| WebRTC    | `python bot.py`                             | http://localhost:7860/client |
| Daily     | `python bot.py -t daily`                    | http://localhost:7860        |
| Twilio    | `python bot.py -t twilio -x proxy.ngrok.io` | Phone calls                  |
| Telnyx    | `python bot.py -t telnyx -x proxy.ngrok.io` | Phone calls                  |
| Plivo     | `python bot.py -t plivo -x proxy.ngrok.io`  | Phone calls                  |

For detailed documentation, see the [Development Runner Guide](https://docs.pipecat.ai/server/utilities/runner/guide).



================================================
FILE: runner-examples/01-single-transport-bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Pipecat development runner example.

This example has a single transport—SmallWebRTCTransport.

Run it with::

    python 01-single-transport-bot.py
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)


async def run_bot(transport: BaseTransport):
    """Main bot logic that works with any transport."""
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        messages.append({"role": "system", "content": "Say hello and briefly introduce yourself."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport = SmallWebRTCTransport(
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
        ),
        webrtc_connection=runner_args.webrtc_connection,
    )

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: runner-examples/02-two-transport-bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Pipecat Cloud-compatible bot example.

Transports are Daily or SmallWebRTC.

Run it with:

- WebRTC transport::

    python 02-two-transport-bot.py

- Daily transport::

    python 02-two-transport-bot.py --transport daily
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import DailyRunnerArguments, RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport

load_dotenv(override=True)


async def run_bot(transport: BaseTransport):
    """Main bot logic that works with any transport."""
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        messages.append({"role": "system", "content": "Say hello and briefly introduce yourself."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport = None

    if isinstance(runner_args, DailyRunnerArguments):
        from pipecat.transports.daily.transport import DailyParams, DailyTransport

        if os.environ.get("ENV") != "local":
            from pipecat.audio.filters.krisp_filter import KrispFilter

            krisp_filter = KrispFilter()
        else:
            krisp_filter = None

        transport = DailyTransport(
            runner_args.room_url,
            runner_args.token,
            "Pipecat Bot",
            params=DailyParams(
                audio_in_enabled=True,
                audio_in_filter=krisp_filter,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

    elif isinstance(runner_args, SmallWebRTCRunnerArguments):
        from pipecat.transports.base_transport import TransportParams
        from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

        transport = SmallWebRTCTransport(
            params=TransportParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
            webrtc_connection=runner_args.webrtc_connection,
        )
    else:
        logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
        return

    if transport is None:
        logger.error("Failed to create transport")
        return

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: runner-examples/03-all-transport-bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Pipecat Cloud-compatible bot example.

Transports are:

- Daily
- SmallWebRTC
- Twilio
- Telnyx
- Plivo
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import (
    DailyRunnerArguments,
    RunnerArguments,
    SmallWebRTCRunnerArguments,
    WebSocketRunnerArguments,
)
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport

load_dotenv(override=True)


async def run_bot(transport: BaseTransport):
    """Main bot logic that works with any transport."""
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        messages.append({"role": "system", "content": "Say hello and briefly introduce yourself."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport = None

    if isinstance(runner_args, DailyRunnerArguments):
        from pipecat.transports.daily.transport import DailyParams, DailyTransport

        if os.environ.get("ENV") != "local":
            from pipecat.audio.filters.krisp_filter import KrispFilter

            krisp_filter = KrispFilter()
        else:
            krisp_filter = None

        transport = DailyTransport(
            runner_args.room_url,
            runner_args.token,
            "Pipecat Bot",
            params=DailyParams(
                audio_in_enabled=True,
                audio_in_filter=krisp_filter,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

    elif isinstance(runner_args, SmallWebRTCRunnerArguments):
        from pipecat.transports.base_transport import TransportParams
        from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

        transport = SmallWebRTCTransport(
            params=TransportParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
            webrtc_connection=runner_args.webrtc_connection,
        )

    elif isinstance(runner_args, WebSocketRunnerArguments):
        # Use the utility to parse WebSocket data
        from pipecat.runner.utils import parse_telephony_websocket

        transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
        logger.info(f"Auto-detected transport: {transport_type}")

        # Create transport based on detected type
        if transport_type == "twilio":
            from pipecat.serializers.twilio import TwilioFrameSerializer

            serializer = TwilioFrameSerializer(
                stream_sid=call_data["stream_id"],
                call_sid=call_data["call_id"],
                account_sid=os.getenv("TWILIO_ACCOUNT_SID", ""),
                auth_token=os.getenv("TWILIO_AUTH_TOKEN", ""),
            )

        elif transport_type == "telnyx":
            from pipecat.serializers.telnyx import TelnyxFrameSerializer

            serializer = TelnyxFrameSerializer(
                stream_id=call_data["stream_id"],
                call_control_id=call_data["call_control_id"],
                outbound_encoding=call_data["outbound_encoding"],
                inbound_encoding="PCMU",  # Set manually
                api_key=os.getenv("TELNYX_API_KEY", ""),
            )

        elif transport_type == "plivo":
            from pipecat.serializers.plivo import PlivoFrameSerializer

            serializer = PlivoFrameSerializer(
                stream_id=call_data["stream_id"],
                call_id=call_data["call_id"],
                auth_id=os.getenv("PLIVO_AUTH_ID", ""),
                auth_token=os.getenv("PLIVO_AUTH_TOKEN", ""),
            )

        elif transport_type == "exotel":
            from pipecat.serializers.exotel import ExotelFrameSerializer

            serializer = ExotelFrameSerializer(
                stream_sid=call_data["stream_id"],
                call_sid=call_data["call_id"],
            )

        else:
            # Generic fallback
            serializer = None

        # Create the transport
        from pipecat.transports.websocket.fastapi import (
            FastAPIWebsocketParams,
            FastAPIWebsocketTransport,
        )

        transport = FastAPIWebsocketTransport(
            websocket=runner_args.websocket,
            params=FastAPIWebsocketParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                add_wav_header=False,
                vad_analyzer=SileroVADAnalyzer(),
                serializer=serializer,
            ),
        )
    else:
        logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
        return

    if transport is None:
        logger.error("Failed to create transport")
        return

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: runner-examples/04-all-transport-factory-bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Pipecat Cloud-compatible bot example.

Transports are:

- Daily
- SmallWebRTC
- Twilio
- Telnyx
- Plivo
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams

load_dotenv(override=True)

# Define transport configurations using factory functions
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        # add_wav_header and serializer will be set automatically
    ),
    "telnyx": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        # add_wav_header and serializer will be set automatically
    ),
    "plivo": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        # add_wav_header and serializer will be set automatically
    ),
    "exotel": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
        # add_wav_header and serializer will be set automatically
    ),
}


async def run_bot(transport: BaseTransport):
    """Main bot logic that works with any transport."""
    logger.info("Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are a friendly AI assistant. Respond naturally and keep your answers conversational.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")
        messages.append({"role": "system", "content": "Say hello and briefly introduce yourself."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)
    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: runner-examples/env.example
================================================
# Required for all examples
DEEPGRAM_API_KEY=your_deepgram_key
CARTESIA_API_KEY=your_cartesia_key
OPENAI_API_KEY=your_openai_key

# For Daily transport
DAILY_API_KEY=your_daily_key

# For telephony transports
TWILIO_ACCOUNT_SID=your_twilio_sid
TWILIO_AUTH_TOKEN=your_twilio_token
TELNYX_API_KEY=your_telnyx_key
PLIVO_AUTH_ID=your_plivo_id
PLIVO_AUTH_TOKEN=your_plivo_token


================================================
FILE: runner-examples/pyproject.toml
================================================
[project]
name = "runner-examples"
version = "0.1.0"
description = "Configuration for runner examples"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,cartesia,openai,runner,silero,webrtc,websocket]",
    "pipecatcloud",
]

[dependency-groups]
dev = [
    "pyright>=1.1.404,<2",
    "ruff>=0.12.11,<1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: scripts/fix-ruff.sh
================================================

#!/bin/bash

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

echo "Running ruff format..."
ruff format "$PROJECT_ROOT"
echo "Running ruff check..."
ruff check --fix "$PROJECT_ROOT"



================================================
FILE: scripts/pre-commit.sh
================================================
#!/bin/bash

# Color codes for output
RED='\033[0;31m'
GREEN='\033[0;32m'
NC='\033[0m' # No Color

echo "🔍 Running pre-commit checks..."

# Change to project root (one level up from scripts/)
cd "$(dirname "$0")/.."

# Format check
echo "📝 Checking code formatting..."
if ! NO_COLOR=1 ruff format --diff --check; then
    echo -e "${RED}❌ Code formatting issues found. Run 'ruff format' to fix.${NC}"
    exit 1
fi

# Lint check
echo "🔍 Running linter..."
if ! ruff check; then
    echo -e "${RED}❌ Linting issues found.${NC}"
    exit 1
fi

echo -e "${GREEN}✅ All pre-commit checks passed!${NC}"


================================================
FILE: simple-chatbot/README.md
================================================
# Simple Chatbot

<img src="image.png" width="420px">

This repository demonstrates a simple AI chatbot with real-time audio/video interaction, implemented using different client and server options. The bot server supports multiple AI backends, and you can connect to it using five different client approaches.

## Two Bot Options

1. **OpenAI Bot**

   - Uses gpt-4o for conversation
   - Requires OpenAI API key

2. **Gemini Bot**
   - Uses Google's Gemini Live model
   - Requires Gemini API key

## Six Client Options

1. **Daily Prebuilt** (Simplest)

   - Direct connection through a Daily Prebuilt room
   - For demo purposes only; handy for quick testing

2. **JavaScript**

   - Basic implementation using [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction)
   - No framework dependencies
   - Good for learning the fundamentals

3. **React**

   - Basic impelmentation using [Pipecat React SDK](https://docs.pipecat.ai/client/react/introduction)
   - Demonstrates the basic client principles with Pipecat React

4. **React Native**

   - Basic impelmentation using [Pipecat React Native SDK](https://docs.pipecat.ai/client/react-native/introduction)
   - Demonstrates the basic client principles with Pipecat React Native

5. **Android**

   - Basic implementation using [Pipecat Android SDK](https://docs.pipecat.ai/client/android/introduction)
   - Demonstrates the basic client principles with Pipecat Android

6. **iOS**
   - Basic implementation using [Pipecat iOS SDK](https://docs.pipecat.ai/client/ios/introduction)
   - Demonstrates the basic client principles with Pipecat iOS

## Quick Start

### First, start the bot server:

Follow the instructions in the [server directory](server/).

### Next, connect using your preferred client app:

- [Android Guide](client/android/README.md)
- [iOS Guide](client/ios/README.md)
- [Daily Prebuilt](client/prebuilt/README.md)
- [JavaScript Guide](client/javascript/README.md)
- [React Guide](client/react/README.md)

## Important Note

The bot server must be running for any of the client implementations to work. Start the server first before trying any of the client apps.

## Requirements

- Python 3.10+
- Node.js 16+ (for JavaScript and React implementations)
- Daily API key
- OpenAI API key (for OpenAI bot)
- Gemini API key (for Gemini bot)
- ElevenLabs API key
- Modern web browser with WebRTC support

## Project Structure

```
simple-chatbot/
├── server/              # Bot server implementation
│   ├── assets           # Directory of sprite images
│   ├── bot-openai.py    # OpenAI bot implementation
│   ├── bot-gemini.py    # Gemini bot implementation
│   ├── env.example      # Env variable example
│   ├── Dockerfile       # Dockerfile for building your image
│   ├── pcc-deploy.toml  # Pipecat Cloud: Deployment specification
│   ├── pyproject.toml   # Project specification
│   ├── README.md        # More specific server setup instructions
│   └── requirements.txt
└── client/              # Client implementations
    ├── android/         # Daily Android connection
    ├── ios/             # Daily iOS connection
    ├── javascript/      # Daily JavaScript connection
    ├── prebuilt/        # Pipecat Prebuilt client
    ├── react/           # Pipecat React client
    └── react-native/    # Pipecat React Native client
```



================================================
FILE: simple-chatbot/client/android/README.md
================================================
# Pipecat Simple Chatbot Client for Android

Demo app which connects to the `simple-chatbot` backend over RTVI.

## Screenshot

<img alt="screenshot" src="files/screenshot.jpg" width="400px" />

## How to run

```bash
# Build and install the app
./gradlew installDebug

# Launch the app
adb shell am start -n ai.pipecat.simple_chatbot_client/.MainActivity
```

Ensure that the `simple-chatbot` server is running as described in the parent README.



================================================
FILE: simple-chatbot/client/android/build.gradle.kts
================================================
plugins {
    alias(libs.plugins.jetbrains.kotlin.android) apply false
    alias(libs.plugins.android.application) apply false
    alias(libs.plugins.compose.compiler) apply false
}



================================================
FILE: simple-chatbot/client/android/gradle.properties
================================================
# Project-wide Gradle settings.
# IDE (e.g. Android Studio) users:
# Gradle settings configured through the IDE *will override*
# any settings specified in this file.
# For more details on how to configure your build environment visit
# http://www.gradle.org/docs/current/userguide/build_environment.html
# Specifies the JVM arguments used for the daemon process.
# The setting is particularly useful for tweaking memory settings.
org.gradle.jvmargs=-Xmx2048m -Dfile.encoding=UTF-8
# When configured, Gradle will run in incubating parallel mode.
# This option should only be used with decoupled projects. For more details, visit
# https://developer.android.com/r/tools/gradle-multi-project-decoupled-projects
# org.gradle.parallel=true
# AndroidX package structure to make it clearer which packages are bundled with the
# Android operating system, and which are packaged with your app's APK
# https://developer.android.com/topic/libraries/support-library/androidx-rn
android.useAndroidX=true
# Kotlin code style for this project: "official" or "obsolete":
kotlin.code.style=official
# Enables namespacing of each library's R class so that its R class includes only the
# resources declared in the library itself and none from the library's dependencies,
# thereby reducing the size of the R class for that library
android.nonTransitiveRClass=true


================================================
FILE: simple-chatbot/client/android/gradlew
================================================
#!/usr/bin/env sh

#
# Copyright 2015 the original author or authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

##############################################################################
##
##  Gradle start up script for UN*X
##
##############################################################################

# Attempt to set APP_HOME
# Resolve links: $0 may be a link
PRG="$0"
# Need this for relative symlinks.
while [ -h "$PRG" ] ; do
    ls=`ls -ld "$PRG"`
    link=`expr "$ls" : '.*-> \(.*\)$'`
    if expr "$link" : '/.*' > /dev/null; then
        PRG="$link"
    else
        PRG=`dirname "$PRG"`"/$link"
    fi
done
SAVED="`pwd`"
cd "`dirname \"$PRG\"`/" >/dev/null
APP_HOME="`pwd -P`"
cd "$SAVED" >/dev/null

APP_NAME="Gradle"
APP_BASE_NAME=`basename "$0"`

# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
DEFAULT_JVM_OPTS='"-Xmx64m" "-Xms64m"'

# Use the maximum available, or set MAX_FD != -1 to use that value.
MAX_FD="maximum"

warn () {
    echo "$*"
}

die () {
    echo
    echo "$*"
    echo
    exit 1
}

# OS specific support (must be 'true' or 'false').
cygwin=false
msys=false
darwin=false
nonstop=false
case "`uname`" in
  CYGWIN* )
    cygwin=true
    ;;
  Darwin* )
    darwin=true
    ;;
  MINGW* )
    msys=true
    ;;
  NONSTOP* )
    nonstop=true
    ;;
esac

CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar


# Determine the Java command to use to start the JVM.
if [ -n "$JAVA_HOME" ] ; then
    if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
        # IBM's JDK on AIX uses strange locations for the executables
        JAVACMD="$JAVA_HOME/jre/sh/java"
    else
        JAVACMD="$JAVA_HOME/bin/java"
    fi
    if [ ! -x "$JAVACMD" ] ; then
        die "ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
    fi
else
    JAVACMD="java"
    which java >/dev/null 2>&1 || die "ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.

Please set the JAVA_HOME variable in your environment to match the
location of your Java installation."
fi

# Increase the maximum file descriptors if we can.
if [ "$cygwin" = "false" -a "$darwin" = "false" -a "$nonstop" = "false" ] ; then
    MAX_FD_LIMIT=`ulimit -H -n`
    if [ $? -eq 0 ] ; then
        if [ "$MAX_FD" = "maximum" -o "$MAX_FD" = "max" ] ; then
            MAX_FD="$MAX_FD_LIMIT"
        fi
        ulimit -n $MAX_FD
        if [ $? -ne 0 ] ; then
            warn "Could not set maximum file descriptor limit: $MAX_FD"
        fi
    else
        warn "Could not query maximum file descriptor limit: $MAX_FD_LIMIT"
    fi
fi

# For Darwin, add options to specify how the application appears in the dock
if $darwin; then
    GRADLE_OPTS="$GRADLE_OPTS \"-Xdock:name=$APP_NAME\" \"-Xdock:icon=$APP_HOME/media/gradle.icns\""
fi

# For Cygwin or MSYS, switch paths to Windows format before running java
if [ "$cygwin" = "true" -o "$msys" = "true" ] ; then
    APP_HOME=`cygpath --path --mixed "$APP_HOME"`
    CLASSPATH=`cygpath --path --mixed "$CLASSPATH"`

    JAVACMD=`cygpath --unix "$JAVACMD"`

    # We build the pattern for arguments to be converted via cygpath
    ROOTDIRSRAW=`find -L / -maxdepth 1 -mindepth 1 -type d 2>/dev/null`
    SEP=""
    for dir in $ROOTDIRSRAW ; do
        ROOTDIRS="$ROOTDIRS$SEP$dir"
        SEP="|"
    done
    OURCYGPATTERN="(^($ROOTDIRS))"
    # Add a user-defined pattern to the cygpath arguments
    if [ "$GRADLE_CYGPATTERN" != "" ] ; then
        OURCYGPATTERN="$OURCYGPATTERN|($GRADLE_CYGPATTERN)"
    fi
    # Now convert the arguments - kludge to limit ourselves to /bin/sh
    i=0
    for arg in "$@" ; do
        CHECK=`echo "$arg"|egrep -c "$OURCYGPATTERN" -`
        CHECK2=`echo "$arg"|egrep -c "^-"`                                 ### Determine if an option

        if [ $CHECK -ne 0 ] && [ $CHECK2 -eq 0 ] ; then                    ### Added a condition
            eval `echo args$i`=`cygpath --path --ignore --mixed "$arg"`
        else
            eval `echo args$i`="\"$arg\""
        fi
        i=`expr $i + 1`
    done
    case $i in
        0) set -- ;;
        1) set -- "$args0" ;;
        2) set -- "$args0" "$args1" ;;
        3) set -- "$args0" "$args1" "$args2" ;;
        4) set -- "$args0" "$args1" "$args2" "$args3" ;;
        5) set -- "$args0" "$args1" "$args2" "$args3" "$args4" ;;
        6) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" ;;
        7) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" ;;
        8) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" ;;
        9) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" "$args8" ;;
    esac
fi

# Escape application args
save () {
    for i do printf %s\\n "$i" | sed "s/'/'\\\\''/g;1s/^/'/;\$s/\$/' \\\\/" ; done
    echo " "
}
APP_ARGS=`save "$@"`

# Collect all arguments for the java command, following the shell quoting and substitution rules
eval set -- $DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS "\"-Dorg.gradle.appname=$APP_BASE_NAME\"" -classpath "\"$CLASSPATH\"" org.gradle.wrapper.GradleWrapperMain "$APP_ARGS"

exec "$JAVACMD" "$@"



================================================
FILE: simple-chatbot/client/android/gradlew.bat
================================================
@rem
@rem Copyright 2015 the original author or authors.
@rem
@rem Licensed under the Apache License, Version 2.0 (the "License");
@rem you may not use this file except in compliance with the License.
@rem You may obtain a copy of the License at
@rem
@rem      https://www.apache.org/licenses/LICENSE-2.0
@rem
@rem Unless required by applicable law or agreed to in writing, software
@rem distributed under the License is distributed on an "AS IS" BASIS,
@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
@rem See the License for the specific language governing permissions and
@rem limitations under the License.
@rem

@if "%DEBUG%" == "" @echo off
@rem ##########################################################################
@rem
@rem  Gradle startup script for Windows
@rem
@rem ##########################################################################

@rem Set local scope for the variables with windows NT shell
if "%OS%"=="Windows_NT" setlocal

set DIRNAME=%~dp0
if "%DIRNAME%" == "" set DIRNAME=.
set APP_BASE_NAME=%~n0
set APP_HOME=%DIRNAME%

@rem Resolve any "." and ".." in APP_HOME to make it shorter.
for %%i in ("%APP_HOME%") do set APP_HOME=%%~fi

@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
set DEFAULT_JVM_OPTS="-Xmx64m" "-Xms64m"

@rem Find java.exe
if defined JAVA_HOME goto findJavaFromJavaHome

set JAVA_EXE=java.exe
%JAVA_EXE% -version >NUL 2>&1
if "%ERRORLEVEL%" == "0" goto execute

echo.
echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:findJavaFromJavaHome
set JAVA_HOME=%JAVA_HOME:"=%
set JAVA_EXE=%JAVA_HOME%/bin/java.exe

if exist "%JAVA_EXE%" goto execute

echo.
echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
echo.
echo Please set the JAVA_HOME variable in your environment to match the
echo location of your Java installation.

goto fail

:execute
@rem Setup the command line

set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar


@rem Execute Gradle
"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %*

:end
@rem End local scope for the variables with windows NT shell
if "%ERRORLEVEL%"=="0" goto mainEnd

:fail
rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
rem the _cmd.exe /c_ return code!
if  not "" == "%GRADLE_EXIT_CONSOLE%" exit 1
exit /b 1

:mainEnd
if "%OS%"=="Windows_NT" endlocal

:omega



================================================
FILE: simple-chatbot/client/android/LICENSE
================================================
BSD 2-Clause License

Copyright (c) 2024–2025, Daily

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice, this
   list of conditions and the following disclaimer.

2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



================================================
FILE: simple-chatbot/client/android/settings.gradle.kts
================================================
pluginManagement {
    repositories {
        google {
            content {
                includeGroupByRegex("com\\.android.*")
                includeGroupByRegex("com\\.google.*")
                includeGroupByRegex("androidx.*")
            }
        }
        mavenCentral()
        gradlePluginPortal()
    }
}
dependencyResolutionManagement {
    repositoriesMode.set(RepositoriesMode.FAIL_ON_PROJECT_REPOS)
    repositories {
        google()
        mavenCentral()
        mavenLocal()
    }
}

rootProject.name = "Pipecat Simple Chatbot Client"
include(":simple-chatbot-client")



================================================
FILE: simple-chatbot/client/android/gradle/libs.versions.toml
================================================
[versions]
accompanistPermissions = "0.34.0"
agp = "8.7.3"
constraintlayoutCompose = "1.1.0"
pipecatClientDaily = "1.0.3"
kotlin = "2.0.20"
coreKtx = "1.15.0"
lifecycleRuntimeKtx = "2.8.7"
activityCompose = "1.9.3"
composeBom = "2024.12.01"
kotlinxSerializationJson = "1.7.1"
kotlinxSerializationPlugin = "2.0.20"

[libraries]
accompanist-permissions = { module = "com.google.accompanist:accompanist-permissions", version.ref = "accompanistPermissions" }
androidx-constraintlayout-compose = { module = "androidx.constraintlayout:constraintlayout-compose", version.ref = "constraintlayoutCompose" }
androidx-core-ktx = { group = "androidx.core", name = "core-ktx", version.ref = "coreKtx" }
pipecat-client-daily = { module = "ai.pipecat:daily-transport", version.ref = "pipecatClientDaily" }
androidx-lifecycle-runtime-ktx = { group = "androidx.lifecycle", name = "lifecycle-runtime-ktx", version.ref = "lifecycleRuntimeKtx" }
androidx-activity-compose = { group = "androidx.activity", name = "activity-compose", version.ref = "activityCompose" }
androidx-compose-bom = { group = "androidx.compose", name = "compose-bom", version.ref = "composeBom" }
androidx-ui = { group = "androidx.compose.ui", name = "ui" }
androidx-ui-graphics = { group = "androidx.compose.ui", name = "ui-graphics" }
androidx-ui-tooling = { group = "androidx.compose.ui", name = "ui-tooling" }
androidx-ui-tooling-preview = { group = "androidx.compose.ui", name = "ui-tooling-preview" }
androidx-ui-test-manifest = { group = "androidx.compose.ui", name = "ui-test-manifest" }
androidx-material3 = { group = "androidx.compose.material3", name = "material3" }
kotlinx-serialization-json = { module = "org.jetbrains.kotlinx:kotlinx-serialization-json", version.ref = "kotlinxSerializationJson" }

[plugins]
jetbrains-kotlin-android = { id = "org.jetbrains.kotlin.android", version.ref = "kotlin" }
android-application = { id = "com.android.application", version.ref = "agp" }
compose-compiler = { id = "org.jetbrains.kotlin.plugin.compose", version.ref = "kotlin" }
jetbrains-kotlin-serialization = { id = "org.jetbrains.kotlin.plugin.serialization", version.ref = "kotlinxSerializationPlugin" }



================================================
FILE: simple-chatbot/client/android/gradle/wrapper/gradle-wrapper.properties
================================================
#Mon Aug 05 13:01:27 BST 2024
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.10.2-bin.zip
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/build.gradle.kts
================================================
plugins {
    alias(libs.plugins.android.application)
    alias(libs.plugins.jetbrains.kotlin.android)
    alias(libs.plugins.jetbrains.kotlin.serialization)
    alias(libs.plugins.compose.compiler)
}

android {
    namespace = "ai.pipecat.simple_chatbot_client"
    compileSdk = 35

    defaultConfig {
        applicationId = "ai.pipecat.simple_chatbot_client"
        minSdk = 26
        targetSdk = 35
        versionCode = 1
        versionName = "1.0"

        vectorDrawables {
            useSupportLibrary = true
        }
    }

    buildTypes {
        release {
            isMinifyEnabled = false
            proguardFiles(
                getDefaultProguardFile("proguard-android-optimize.txt"),
                "proguard-rules.pro"
            )
        }
    }

    compileOptions {
        sourceCompatibility = JavaVersion.VERSION_11
        targetCompatibility = JavaVersion.VERSION_11
    }

    kotlinOptions {
        jvmTarget = "11"
    }

    buildFeatures {
        compose = true
        buildConfig = true
    }

    composeOptions {
        kotlinCompilerExtensionVersion = "1.5.1"
    }

    packaging {
        resources {
            excludes += "/META-INF/{AL2.0,LGPL2.1}"
        }
    }
}

dependencies {
    implementation(libs.pipecat.client.daily)
    implementation(libs.androidx.core.ktx)
    implementation(libs.androidx.lifecycle.runtime.ktx)
    implementation(libs.androidx.activity.compose)
    implementation(platform(libs.androidx.compose.bom))
    implementation(libs.androidx.ui)
    implementation(libs.androidx.ui.graphics)
    implementation(libs.androidx.ui.tooling.preview)
    implementation(libs.androidx.material3)
    implementation(libs.accompanist.permissions)
    implementation(libs.androidx.constraintlayout.compose)
    implementation(libs.kotlinx.serialization.json)
    androidTestImplementation(platform(libs.androidx.compose.bom))
    debugImplementation(libs.androidx.ui.tooling)
    debugImplementation(libs.androidx.ui.test.manifest)
}



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/proguard-rules.pro
================================================
# Add project specific ProGuard rules here.
# You can control the set of applied configuration files using the
# proguardFiles setting in build.gradle.
#
# For more details, see
#   http://developer.android.com/guide/developing/tools/proguard.html

# If your project uses WebView with JS, uncomment the following
# and specify the fully qualified class name to the JavaScript interface
# class:
#-keepclassmembers class fqcn.of.javascript.interface.for.webview {
#   public *;
#}

# Uncomment this to preserve the line number information for
# debugging stack traces.
#-keepattributes SourceFile,LineNumberTable

# If you keep the line number information, uncomment this to
# hide the original source file name.
#-renamesourcefileattribute SourceFile


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/AndroidManifest.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<manifest xmlns:android="http://schemas.android.com/apk/res/android">

    <uses-feature
        android:name="android.hardware.camera"
        android:required="false" />

    <uses-permission android:name="android.permission.INTERNET" />
    <uses-permission android:name="android.permission.CAMERA" />
    <uses-permission android:name="android.permission.RECORD_AUDIO" />
    <uses-permission android:name="android.permission.MODIFY_AUDIO_SETTINGS" />
    <uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" />

    <application
        android:name=".RTVIApplication"
        android:allowBackup="true"
        android:icon="@mipmap/ic_launcher"
        android:label="@string/app_name"
        android:roundIcon="@mipmap/ic_launcher_round"
        android:supportsRtl="true"
        android:usesCleartextTraffic="true"
        android:theme="@style/Theme.RTVIClient">
        <activity
            android:name=".MainActivity"
            android:exported="true"
            android:windowSoftInputMode="adjustResize"
            android:configChanges="keyboard|keyboardHidden|orientation|screenSize"
            android:theme="@style/Theme.RTVIClient">
            <intent-filter>
                <action android:name="android.intent.action.MAIN" />

                <category android:name="android.intent.category.LAUNCHER" />
            </intent-filter>
        </activity>
    </application>

</manifest>



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ChatHistoryElement.kt
================================================
package ai.pipecat.simple_chatbot_client

import androidx.compose.runtime.snapshots.SnapshotStateList

data class ChatHistoryElement(
    val type: Type,
    val text: String,
) {
    enum class Type {
        Bot,
        User,
        Log
    }
}

fun SnapshotStateList<ChatHistoryElement>.append(
    type: ChatHistoryElement.Type,
    text: String
) {
    add(ChatHistoryElement(type, text.trim()))
}

fun SnapshotStateList<ChatHistoryElement>.appendOrUpdate(
    type: ChatHistoryElement.Type,
    text: String
) {
    val last = lastOrNull()
    if (last != null && last.type == type) {
        removeAt(lastIndex)
        add(ChatHistoryElement(type, last.text + " " + text.trim()))

    } else {
        add(ChatHistoryElement(type, text.trim()))
    }
}

fun SnapshotStateList<ChatHistoryElement>.appendLog(text: String) {
    append(ChatHistoryElement.Type.Log, text)
}

fun SnapshotStateList<ChatHistoryElement>.appendOrUpdateUser(text: String) {
    appendOrUpdate(ChatHistoryElement.Type.User, text)
}

fun SnapshotStateList<ChatHistoryElement>.appendOrUpdateBot(text: String) {
    appendOrUpdate(ChatHistoryElement.Type.Bot, text)
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/MainActivity.kt
================================================
package ai.pipecat.simple_chatbot_client

import ai.pipecat.simple_chatbot_client.ui.InCallLayout
import ai.pipecat.simple_chatbot_client.ui.PermissionScreen
import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import ai.pipecat.simple_chatbot_client.ui.theme.RTVIClientTheme
import ai.pipecat.simple_chatbot_client.ui.theme.TextStyles
import ai.pipecat.simple_chatbot_client.ui.theme.textFieldColors
import android.os.Bundle
import androidx.activity.ComponentActivity
import androidx.activity.compose.setContent
import androidx.activity.enableEdgeToEdge
import androidx.annotation.DrawableRes
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.height
import androidx.compose.foundation.layout.imePadding
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.rememberScrollState
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.foundation.text.KeyboardOptions
import androidx.compose.foundation.verticalScroll
import androidx.compose.material3.AlertDialog
import androidx.compose.material3.Button
import androidx.compose.material3.ExperimentalMaterial3Api
import androidx.compose.material3.Icon
import androidx.compose.material3.Scaffold
import androidx.compose.material3.Text
import androidx.compose.material3.TextField
import androidx.compose.runtime.Composable
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.text.input.ImeAction
import androidx.compose.ui.text.input.KeyboardType
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp


class MainActivity : ComponentActivity() {

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        enableEdgeToEdge()

        val voiceClientManager = VoiceClientManager(this)

        setContent {
            RTVIClientTheme {
                Scaffold(modifier = Modifier.fillMaxSize()) { innerPadding ->
                    Box(
                        Modifier
                            .fillMaxSize()
                            .padding(innerPadding)
                    ) {
                        PermissionScreen()

                        val vcState = voiceClientManager.state.value

                        if (vcState != null) {
                            InCallLayout(voiceClientManager)

                        } else {
                            ConnectSettings(voiceClientManager)
                        }

                        voiceClientManager.errors.firstOrNull()?.let { errorText ->

                            val dismiss: () -> Unit = { voiceClientManager.errors.removeAt(0) }

                            AlertDialog(
                                onDismissRequest = dismiss,
                                confirmButton = {
                                    Button(onClick = dismiss) {
                                        Text(
                                            text = "OK",
                                            fontSize = 14.sp,
                                            fontWeight = FontWeight.W700,
                                            color = Color.White,
                                            style = TextStyles.base
                                        )
                                    }
                                },
                                containerColor = Color.White,
                                title = {
                                    Text(
                                        text = "Error",
                                        fontSize = 22.sp,
                                        fontWeight = FontWeight.W600,
                                        color = Color.Black,
                                        style = TextStyles.base
                                    )
                                },
                                text = {
                                    Text(
                                        text = errorText.message,
                                        fontSize = 16.sp,
                                        fontWeight = FontWeight.W400,
                                        color = Color.Black,
                                        style = TextStyles.base
                                    )
                                }
                            )
                        }
                    }
                }
            }
        }
    }
}

@OptIn(ExperimentalMaterial3Api::class)
@Composable
fun ConnectSettings(
    voiceClientManager: VoiceClientManager,
) {
    val scrollState = rememberScrollState()

    val start = {
        val backendUrl = Preferences.backendUrl.value

        voiceClientManager.start(baseUrl = backendUrl!!)
    }

    Box(
        modifier = Modifier
            .fillMaxSize()
            .verticalScroll(scrollState)
            .imePadding()
            .padding(20.dp),
        contentAlignment = Alignment.Center
    ) {
        Box(
            Modifier
                .fillMaxWidth()
                .shadow(2.dp, RoundedCornerShape(16.dp))
                .clip(RoundedCornerShape(16.dp))
                .background(Colors.mainSurfaceBackground)
        ) {
            Column(
                Modifier
                    .fillMaxWidth()
                    .padding(
                        vertical = 24.dp,
                        horizontal = 28.dp
                    )
            ) {
                Spacer(modifier = Modifier.height(12.dp))

                Text(
                    modifier = Modifier.align(Alignment.CenterHorizontally),
                    text = "Connect to an RTVI server",
                    fontSize = 22.sp,
                    fontWeight = FontWeight.W700,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(36.dp))

                Text(
                    text = "Backend URL",
                    fontSize = 16.sp,
                    fontWeight = FontWeight.W400,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(12.dp))

                TextField(
                    modifier = Modifier
                        .fillMaxWidth()
                        .border(1.dp, Colors.textFieldBorder, RoundedCornerShape(12.dp)),
                    value = Preferences.backendUrl.value ?: "",
                    onValueChange = { Preferences.backendUrl.value = it },
                    keyboardOptions = KeyboardOptions(
                        keyboardType = KeyboardType.Uri,
                        imeAction = ImeAction.Next
                    ),
                    colors = textFieldColors(),
                    shape = RoundedCornerShape(12.dp)
                )

                Spacer(modifier = Modifier.height(36.dp))

                Row(
                    modifier = Modifier.fillMaxWidth(),
                    horizontalArrangement = Arrangement.spacedBy(16.dp)
                ) {
                    ConnectDialogButton(
                        modifier = Modifier.weight(1f),
                        onClick = start,
                        text = "Connect",
                        foreground = Color.White,
                        background = Colors.buttonNormal,
                        border = Colors.buttonNormal
                    )
                }
            }
        }
    }
}

@Composable
private fun ConnectDialogButton(
    onClick: () -> Unit,
    text: String,
    foreground: Color,
    background: Color,
    border: Color,
    modifier: Modifier = Modifier,
    @DrawableRes icon: Int? = null,
) {
    val shape = RoundedCornerShape(8.dp)

    Row(
        modifier
            .border(1.dp, border, shape)
            .clip(shape)
            .background(background)
            .clickable(onClick = onClick)
            .padding(vertical = 10.dp, horizontal = 24.dp),
        verticalAlignment = Alignment.CenterVertically,
        horizontalArrangement = Arrangement.Center
    ) {
        if (icon != null) {
            Icon(
                modifier = Modifier.size(24.dp),
                painter = painterResource(icon),
                tint = foreground,
                contentDescription = null
            )

            Spacer(modifier = Modifier.width(8.dp))
        }

        Text(
            text = text,
            fontSize = 16.sp,
            fontWeight = FontWeight.W500,
            color = foreground
        )
    }
}



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/Preferences.kt
================================================
package ai.pipecat.simple_chatbot_client

import android.content.Context
import android.content.SharedPreferences
import androidx.compose.runtime.mutableStateOf
import kotlinx.serialization.KSerializer
import kotlinx.serialization.json.Json

private val JSON_INSTANCE = Json { ignoreUnknownKeys = true }

object Preferences {

    private const val PREF_BACKEND_URL = "backend_url"

    private lateinit var prefs: SharedPreferences

    fun initAppStart(context: Context) {
        prefs = context.applicationContext.getSharedPreferences("prefs", Context.MODE_PRIVATE)

        listOf(backendUrl).forEach { it.init() }
    }

    private fun getString(key: String): String? = prefs.getString(key, null)

    interface BasePref {
        fun init()
    }

    class StringPref(private val key: String): BasePref {
        private val cachedValue = mutableStateOf<String?>(null)

        override fun init() {
            cachedValue.value = getString(key)
            prefs.registerOnSharedPreferenceChangeListener { _, changedKey ->
                if (key == changedKey) {
                    cachedValue.value = getString(key)
                }
            }
        }

        var value: String?
            get() = cachedValue.value
            set(newValue) {
                cachedValue.value = newValue
                prefs.edit().putString(key, newValue).apply()
            }
    }

    class JsonPref<E>(private val key: String, private var serializer: KSerializer<E>): BasePref {
        private val cachedValue = mutableStateOf<E?>(null)

        private fun lookupValue(): E? =
            getString(key)?.let { JSON_INSTANCE.decodeFromString(serializer, it) }

        override fun init() {
            cachedValue.value = lookupValue()
            prefs.registerOnSharedPreferenceChangeListener { _, changedKey ->
                if (key == changedKey) {
                    cachedValue.value = lookupValue()
                }
            }
        }

        var value: E?
            get() = cachedValue.value
            set(newValue) {
                cachedValue.value = newValue
                prefs.edit()
                    .putString(key, newValue?.let { JSON_INSTANCE.encodeToString(serializer, it) })
                    .apply()
            }
    }

    val backendUrl = StringPref(PREF_BACKEND_URL)
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/RTVIApplication.kt
================================================
package ai.pipecat.simple_chatbot_client

import android.app.Application

class RTVIApplication : Application() {
    override fun onCreate() {
        super.onCreate()
        Preferences.initAppStart(this)
    }
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/VoiceClientManager.kt
================================================
package ai.pipecat.simple_chatbot_client

import ai.pipecat.client.PipecatClient
import ai.pipecat.client.PipecatClientOptions
import ai.pipecat.client.PipecatEventCallbacks
import ai.pipecat.client.daily.DailyTransport
import ai.pipecat.client.daily.DailyTransportConnectParams
import ai.pipecat.client.daily.PipecatClientDaily
import ai.pipecat.client.result.Future
import ai.pipecat.client.result.RTVIError
import ai.pipecat.client.types.APIRequest
import ai.pipecat.client.types.BotReadyData
import ai.pipecat.client.types.Participant
import ai.pipecat.client.types.PipecatMetrics
import ai.pipecat.client.types.SendTextOptions
import ai.pipecat.client.types.Tracks
import ai.pipecat.client.types.Transcript
import ai.pipecat.client.types.TransportState
import ai.pipecat.client.types.Value
import android.content.Context
import android.util.Log
import androidx.compose.runtime.Immutable
import androidx.compose.runtime.Stable
import androidx.compose.runtime.mutableFloatStateOf
import androidx.compose.runtime.mutableStateListOf
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.snapshots.SnapshotStateList

@Immutable
data class Error(val message: String)

@Stable
class VoiceClientManager(private val context: Context) {

    companion object {
        private const val TAG = "VoiceClientManager"
    }

    private val client = mutableStateOf<PipecatClientDaily?>(null)

    val state = mutableStateOf<TransportState?>(null)

    val errors = mutableStateListOf<Error>()

    val botReady = mutableStateOf(false)
    val botIsTalking = mutableStateOf(false)
    val userIsTalking = mutableStateOf(false)
    val botAudioLevel = mutableFloatStateOf(0f)
    val userAudioLevel = mutableFloatStateOf(0f)

    val mic = mutableStateOf(false)
    val camera = mutableStateOf(false)
    val tracks = mutableStateOf<Tracks?>(null)

    val chatHistory = SnapshotStateList<ChatHistoryElement>()

    private fun <E> Future<E, RTVIError>.displayErrors() = withErrorCallback {
        Log.e(TAG, "Future resolved with error: ${it.description}", it.exception)
        errors.add(Error(it.description))
    }

    fun start(baseUrl: String) {

        if (client.value != null) {
            return
        }

        chatHistory.clear()

        val url = if (baseUrl.endsWith("/")) {
            baseUrl
        } else {
            "$baseUrl/"
        } + "start"

        state.value = TransportState.Disconnected

        val callbacks = object : PipecatEventCallbacks() {
            override fun onTransportStateChanged(state: TransportState) {
                this@VoiceClientManager.state.value = state
            }

            override fun onBackendError(message: String) {
                "Error from backend: $message".let {
                    Log.e(TAG, it)
                    errors.add(Error(it))
                    chatHistory.appendLog(it)
                }
            }

            override fun onServerMessage(data: Value) {
                Log.i(TAG, "onServerMessage: $data")
            }

            override fun onBotReady(data: BotReadyData) {

                Log.i(TAG, "Bot ready. Version ${data.version}")

                botReady.value = true
                chatHistory.appendLog("Bot is ready")
            }

            override fun onMetrics(data: PipecatMetrics) {
                Log.i(TAG, "Bot metrics: $data")
            }

            override fun onUserTranscript(data: Transcript) {
                Log.i(TAG, "User transcript: $data")
                if (data.final) {
                    chatHistory.appendOrUpdateUser(data.text)
                }
            }

            override fun onBotTranscript(text: String) {
                Log.i(TAG, "Bot transcript: $text")
                chatHistory.appendOrUpdateBot(text)
            }

            override fun onBotStartedSpeaking() {
                Log.i(TAG, "Bot started speaking")
                botIsTalking.value = true
            }

            override fun onBotStoppedSpeaking() {
                Log.i(TAG, "Bot stopped speaking")
                botIsTalking.value = false
            }

            override fun onUserStartedSpeaking() {
                Log.i(TAG, "User started speaking")
                userIsTalking.value = true
            }

            override fun onUserStoppedSpeaking() {
                Log.i(TAG, "User stopped speaking")
                userIsTalking.value = false
            }

            override fun onTracksUpdated(tracks: Tracks) {
                this@VoiceClientManager.tracks.value = tracks
            }

            override fun onInputsUpdated(camera: Boolean, mic: Boolean) {
                this@VoiceClientManager.camera.value = camera
                this@VoiceClientManager.mic.value = mic
            }

            override fun onDisconnected() {
                chatHistory.appendLog("Disconnected")

                botIsTalking.value = false
                userIsTalking.value = false
                state.value = null
                botReady.value = false
                tracks.value = null

                client.value?.release()
                client.value = null
            }

            override fun onUserAudioLevel(level: Float) {
                userAudioLevel.floatValue = level
            }

            override fun onRemoteAudioLevel(level: Float, participant: Participant) {
                botAudioLevel.floatValue = level
            }

            override fun onConnected() {
                Log.i(TAG, "Connected")
            }

            override fun onBotConnected(participant: Participant) {
                Log.i(TAG, "Bot connected: $participant")
            }

            override fun onBotDisconnected(participant: Participant) {
                Log.i(TAG, "Bot disconnected: $participant")
            }

            override fun onParticipantJoined(participant: Participant) {
                Log.i(TAG, "Participant joined: $participant")
            }

            override fun onParticipantLeft(participant: Participant) {
                Log.i(TAG, "Participant left: $participant")
            }
        }

        val options = PipecatClientOptions(
            callbacks = callbacks
        )

        val client = PipecatClient(DailyTransport(context), options)

        client.startBotAndConnect(
            APIRequest(
                endpoint = url,
                requestData = Value.Object()
            )
        ).displayErrors().withErrorCallback {
            callbacks.onDisconnected()
        }

        this.client.value = client
    }

    fun enableCamera(enabled: Boolean) {
        client.value?.enableCam(enabled)?.displayErrors()
    }

    fun enableMic(enabled: Boolean) {
        client.value?.enableMic(enabled)?.displayErrors()
    }

    fun toggleCamera() = enableCamera(!camera.value)
    fun toggleMic() = enableMic(!mic.value)

    fun flipCamera() {
        client.value?.let { pipecatClient ->
            val currentCam = pipecatClient.selectedCam?.id
            pipecatClient.getAllCams().withCallback { result ->
                val newCam = result.valueOrNull?.filterNot { it.id == currentCam }?.firstOrNull()
                newCam?.let { pipecatClient.updateCam(it.id) }
            }
        }
    }

    fun stop() {
        client.value?.disconnect()?.displayErrors()
    }

    fun sendText(text: String, options: SendTextOptions = SendTextOptions()) {
        chatHistory.appendOrUpdateUser(text)
        client.value?.sendText(text, options)
    }
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/AudioIndicator.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import androidx.compose.animation.core.LinearEasing
import androidx.compose.animation.core.animateFloat
import androidx.compose.animation.core.animateFloatAsState
import androidx.compose.animation.core.infiniteRepeatable
import androidx.compose.animation.core.rememberInfiniteTransition
import androidx.compose.animation.core.tween
import androidx.compose.foundation.Canvas
import androidx.compose.runtime.Composable
import androidx.compose.runtime.getValue
import androidx.compose.ui.Modifier
import androidx.compose.ui.geometry.Offset
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.graphics.StrokeCap
import androidx.compose.ui.semantics.clearAndSetSemantics

@Composable
fun ListeningAnimation(
    modifier: Modifier,
    active: Boolean,
    level: Float,
    color: Color,
) {
    val infiniteTransition = rememberInfiniteTransition("listeningAnimation")

    val loopState by infiniteTransition.animateFloat(
        initialValue = 0f,
        targetValue = Math.PI.toFloat() * 2f,
        animationSpec = infiniteRepeatable(tween(durationMillis = 1000, easing = LinearEasing)),
        label = "listeningAnimationLoopState"
    )

    val activeFraction by animateFloatAsState(
        if (active) {
            Math.pow(level.toDouble(), 0.3).toFloat()
        } else {
            0f
        }
    )

    Canvas(modifier.clearAndSetSemantics { }) {

        val strokeWidthPx = size.width / 12

        val lineCount = 5

        for (i in 1..lineCount) {

            val sine = Math.sin(loopState + 0.9 * i)
            val fraction = activeFraction * ((sine + 1) / 2).toFloat()

            val x = (size.width / (lineCount + 1)) * i

            val yMax = size.height * 0.25f
            val yMin = size.height * 0.5f

            val y = yMin + (yMax - yMin) * fraction
            val yEnd = size.height - y

            this.drawLine(
                start = Offset(x, y),
                end = Offset(x, yEnd),
                color = color,
                strokeWidth = strokeWidthPx,
                cap = StrokeCap.Round
            )
        }
    }
}



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/BotIndicator.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import androidx.compose.animation.AnimatedContent
import androidx.compose.animation.animateColorAsState
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.aspectRatio
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.material3.CircularProgressIndicator
import androidx.compose.runtime.Composable
import androidx.compose.runtime.FloatState
import androidx.compose.runtime.State
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableFloatStateOf
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.graphics.StrokeCap
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp

@Composable
fun BotIndicator(
    modifier: Modifier,
    isReady: Boolean,
    isTalking: State<Boolean>,
    audioLevel: FloatState,
) {
    Box(
        modifier = modifier.padding(15.dp),
        contentAlignment = Alignment.Center
    ) {
        val color by animateColorAsState(if (isTalking.value || !isReady) {
            Color.Black
        } else {
            Colors.botIndicatorBackground
        })

        Box(
            Modifier
                .aspectRatio(1f)
                .fillMaxSize()
                .shadow(20.dp, CircleShape)
                .border(12.dp, Color.White, CircleShape)
                .border(1.dp, Colors.lightGrey, CircleShape)
                .clip(CircleShape)
                .background(color)
                .padding(50.dp),
            contentAlignment = Alignment.Center,
        ) {
            AnimatedContent(
                targetState = isReady
            ) { isReadyVal ->
                if (isReadyVal) {
                    ListeningAnimation(
                        modifier = Modifier.fillMaxSize(),
                        active = isTalking.value,
                        level = audioLevel.floatValue,
                        color = Color.White
                    )
                } else {
                    CircularProgressIndicator(
                        modifier = Modifier.size(180.dp),
                        color = Color.White,
                        strokeWidth = 12.dp,
                        strokeCap = StrokeCap.Round,
                        trackColor = color
                    )
                }
            }
        }
    }
}

@Composable
@Preview
fun PreviewBotIndicator() {
    BotIndicator(
        modifier = Modifier,
        isReady = false,
        isTalking = remember { mutableStateOf(true) },
        audioLevel = remember { mutableFloatStateOf(1.0f) }
    )
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/InCallFooter.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.client.types.SendTextOptions
import ai.pipecat.simple_chatbot_client.ChatHistoryElement
import ai.pipecat.simple_chatbot_client.R
import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import ai.pipecat.simple_chatbot_client.ui.theme.TextStyles
import ai.pipecat.simple_chatbot_client.ui.theme.textFieldColors
import androidx.annotation.DrawableRes
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.ColumnScope
import androidx.compose.foundation.layout.PaddingValues
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.height
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.lazy.LazyColumn
import androidx.compose.foundation.lazy.items
import androidx.compose.foundation.lazy.rememberLazyListState
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.foundation.text.KeyboardActions
import androidx.compose.foundation.text.KeyboardOptions
import androidx.compose.material3.Checkbox
import androidx.compose.material3.DropdownMenu
import androidx.compose.material3.DropdownMenuItem
import androidx.compose.material3.Icon
import androidx.compose.material3.Text
import androidx.compose.material3.TextField
import androidx.compose.runtime.Composable
import androidx.compose.runtime.LaunchedEffect
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableStateListOf
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.runtime.setValue
import androidx.compose.runtime.snapshots.SnapshotStateList
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.text.input.ImeAction
import androidx.compose.ui.text.input.KeyboardType
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp

@Composable
private fun FooterButton(
    modifier: Modifier,
    onClick: () -> Unit,
    @DrawableRes icon: Int,
    text: String? = null,
    foreground: Color,
    background: Color,
    border: Color,
) {
    val shape = RoundedCornerShape(12.dp)

    Row(
        modifier
            .border(1.dp, border, shape)
            .clip(shape)
            .background(background)
            .clickable(onClick = onClick)
            .padding(vertical = 10.dp, horizontal = 18.dp),
        verticalAlignment = Alignment.CenterVertically,
        horizontalArrangement = Arrangement.Center
    ) {
        Icon(
            modifier = Modifier.size(24.dp),
            painter = painterResource(icon),
            tint = foreground,
            contentDescription = null
        )

        if (text != null) {
            Spacer(modifier = Modifier.width(8.dp))

            Text(
                text = text,
                style = TextStyles.base,
                fontSize = 14.sp,
                fontWeight = FontWeight.W600,
                color = foreground
            )
        }
    }
}


@Composable
fun ColumnScope.InCallFooter(
    onClickEnd: () -> Unit,
    onSubmitChatText: (String, SendTextOptions) -> Unit,
    chatHistory: SnapshotStateList<ChatHistoryElement>
) {
    var showOptionsPopup by remember { mutableStateOf(false) }
    var sendTextOptions by remember { mutableStateOf(SendTextOptions()) }

    // Chat history

    Box(
        modifier = Modifier
            .fillMaxWidth()
            .height(90.dp)
            .padding(horizontal = 15.dp),
    ) {
        val listState = rememberLazyListState()

        LaunchedEffect(chatHistory.size, chatHistory.lastOrNull()) {
            listState.animateScrollToItem(listState.layoutInfo.totalItemsCount)
        }

        LazyColumn(
            modifier = Modifier
                .fillMaxSize()
                .shadow(4.dp, RoundedCornerShape(12.dp))
                .clip(RoundedCornerShape(12.dp))
                .background(Colors.botIndicatorBackground)
                .border(5.dp, Color.White, RoundedCornerShape(12.dp)),
            state = listState
        ) {
            item {
                Spacer(Modifier.height(8.dp))
            }

            items(chatHistory) { item ->
                Text(
                    modifier = Modifier.padding(horizontal = 10.dp, vertical = 1.dp),
                    text = item.text,
                    style = TextStyles.mono,
                    color = when (item.type) {
                        ChatHistoryElement.Type.Bot -> Colors.activityBackground
                        ChatHistoryElement.Type.User -> Colors.lightGrey
                        ChatHistoryElement.Type.Log -> Colors.logTextColor
                    },
                    fontSize = 12.sp
                )
            }

            item {
                Spacer(Modifier.height(8.dp))
            }
        }
    }

    Spacer(Modifier.height(15.dp))

    // Text input field

    Row(
        modifier = Modifier
            .fillMaxWidth()
            .padding(horizontal = 15.dp),
        verticalAlignment = Alignment.CenterVertically
    ) {
        var chatText by remember { mutableStateOf("") }

        val submitChatText = {
            onSubmitChatText(chatText, sendTextOptions)
            chatText = ""
        }

        TextField(
            modifier = Modifier
                .weight(1f)
                .border(1.dp, Colors.textFieldBorder, RoundedCornerShape(12.dp)),
            value = chatText,
            textStyle = TextStyles.base,
            onValueChange = { chatText = it },
            keyboardOptions = KeyboardOptions(
                keyboardType = KeyboardType.Text,
                imeAction = ImeAction.Go
            ),
            keyboardActions = KeyboardActions(
                onGo = { submitChatText() }
            ),
            placeholder = {
                Text("Send text message...")
            },
            colors = textFieldColors(),
            shape = RoundedCornerShape(12.dp),
        )

        Spacer(Modifier.width(8.dp))

        Box {
            FooterButton(
                modifier = Modifier,
                onClick = { showOptionsPopup = !showOptionsPopup },
                icon = R.drawable.three_dots,
                text = null,
                foreground = Color.White,
                background = Colors.endButton,
                border = Colors.endButton
            )
            
            DropdownMenu(
                expanded = showOptionsPopup,
                onDismissRequest = { showOptionsPopup = false },
                modifier = Modifier.padding(PaddingValues(end = 16.dp)),
                containerColor = Color.White,
            ) {
                DropdownMenuItem(
                    text = { 
                        Row(
                            verticalAlignment = Alignment.CenterVertically
                        ) {
                            Checkbox(
                                checked = sendTextOptions.runImmediately ?: true,
                                onCheckedChange = { 
                                    sendTextOptions = sendTextOptions.copy(runImmediately = it)
                                }
                            )
                            Spacer(modifier = Modifier.width(8.dp))
                            Text(
                                text = "Run Immediately",
                                style = TextStyles.base,
                                fontSize = 14.sp,
                                color = Color.Black
                            )
                        }
                    },
                    onClick = { 
                        sendTextOptions = sendTextOptions.copy(
                            runImmediately = !(sendTextOptions.runImmediately ?: true)
                        )
                    }
                )
                
                DropdownMenuItem(
                    text = { 
                        Row(
                            verticalAlignment = Alignment.CenterVertically
                        ) {
                            Checkbox(
                                checked = sendTextOptions.audioResponse ?: true,
                                onCheckedChange = { 
                                    sendTextOptions = sendTextOptions.copy(audioResponse = it)
                                }
                            )
                            Spacer(modifier = Modifier.width(8.dp))
                            Text(
                                text = "Audio Response",
                                style = TextStyles.base,
                                fontSize = 14.sp,
                                color = Color.Black
                            )
                        }
                    },
                    onClick = { 
                        sendTextOptions = sendTextOptions.copy(
                            audioResponse = !(sendTextOptions.audioResponse ?: true)
                        )
                    }
                )
            }
        }

        Spacer(Modifier.width(8.dp))

        FooterButton(
            modifier = Modifier,
            onClick = submitChatText,
            icon = R.drawable.send,
            text = null,
            foreground = Color.White,
            background = Colors.endButton,
            border = Colors.endButton
        )
    }


    // End button

    Row(Modifier
        .fillMaxWidth(0.5f)
        .padding(15.dp)
        .align(Alignment.CenterHorizontally)
    ) {
        FooterButton(
            modifier = Modifier.weight(1f),
            onClick = onClickEnd,
            icon = R.drawable.phone_hangup,
            text = "End",
            foreground = Color.White,
            background = Colors.endButton,
            border = Colors.endButton
        )
    }
}

@Composable
@Preview
private fun InCallFooterPreview() {
    Column(
        Modifier
            .fillMaxWidth()
            .background(Colors.activityBackground)
    ) {
        InCallFooter({}, { _, _ -> }, remember { mutableStateListOf(
            ChatHistoryElement(ChatHistoryElement.Type.Bot, "Bot"),
            ChatHistoryElement(ChatHistoryElement.Type.User, "User"),
            ChatHistoryElement(ChatHistoryElement.Type.Log, "Log")
        ) })
    }
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/InCallHeader.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.simple_chatbot_client.utils.Timestamp
import androidx.compose.animation.AnimatedContent
import androidx.compose.animation.fadeIn
import androidx.compose.animation.fadeOut
import androidx.compose.animation.togetherWith
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.padding
import androidx.compose.runtime.Composable
import androidx.compose.ui.Modifier
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.constraintlayout.compose.ConstraintLayout

@Composable
fun InCallHeader(
    expiryTime: Timestamp?
) {
    ConstraintLayout(
        Modifier
            .fillMaxWidth()
            .padding(vertical = 15.dp)
    ) {
        val refTimer = createRef()

        AnimatedContent(
            modifier = Modifier.constrainAs(refTimer) {
                top.linkTo(parent.top)
                bottom.linkTo(parent.bottom)
                end.linkTo(parent.end)
            },
            targetState = expiryTime,
            transitionSpec = { fadeIn() togetherWith fadeOut() }
        ) { expiryTimeVal ->
            if (expiryTimeVal != null) {
                Timer(expiryTime = expiryTimeVal, modifier = Modifier)
            }
        }
    }
}

@Composable
@Preview
fun PreviewInCallHeader() {
    InCallHeader(
        Timestamp.now() + java.time.Duration.ofMinutes(3)
    )
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/InCallLayout.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.simple_chatbot_client.VoiceClientManager
import androidx.compose.foundation.layout.Arrangement
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.fillMaxWidth
import androidx.compose.foundation.layout.imePadding
import androidx.compose.runtime.Composable
import androidx.compose.runtime.derivedStateOf
import androidx.compose.runtime.getValue
import androidx.compose.runtime.remember
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.unit.dp

@Composable
fun InCallLayout(voiceClientManager: VoiceClientManager) {

    val localCam by remember { derivedStateOf { voiceClientManager.tracks.value?.local?.video } }

    Column(Modifier.fillMaxSize().imePadding()) {

        InCallHeader(expiryTime = null)

        Box(
            modifier = Modifier
                .weight(1f)
                .fillMaxWidth(),
            contentAlignment = Alignment.Center
        ) {
            Column(
                modifier = Modifier.fillMaxWidth(),
                horizontalAlignment = Alignment.CenterHorizontally,
                verticalArrangement = Arrangement.spacedBy(12.dp, Alignment.CenterVertically)
            ) {
                BotIndicator(
                    modifier = Modifier,
                    isReady = voiceClientManager.botReady.value,
                    isTalking = voiceClientManager.botIsTalking,
                    audioLevel = voiceClientManager.botAudioLevel
                )

                Row(
                    verticalAlignment = Alignment.CenterVertically
                ) {
                    UserMicButton(
                        onClick = voiceClientManager::toggleMic,
                        micEnabled = voiceClientManager.mic.value,
                        modifier = Modifier,
                        isTalking = voiceClientManager.userIsTalking,
                        audioLevel = voiceClientManager.userAudioLevel
                    )

                    UserCamButton(
                        onClick = voiceClientManager::toggleCamera,
                        onLongClick = voiceClientManager::flipCamera,
                        camEnabled = voiceClientManager.camera.value,
                        camTrackId = localCam,
                        modifier = Modifier
                    )
                }
            }
        }

        InCallFooter(
            onClickEnd = voiceClientManager::stop,
            onSubmitChatText = voiceClientManager::sendText,
            chatHistory = voiceClientManager.chatHistory,
        )
    }
}



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/PermissionScreen.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import ai.pipecat.simple_chatbot_client.ui.theme.TextStyles
import android.Manifest
import android.util.Log
import androidx.activity.compose.rememberLauncherForActivityResult
import androidx.activity.result.contract.ActivityResultContracts
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.layout.Column
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.height
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material3.Button
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import androidx.compose.ui.window.Dialog
import com.google.accompanist.permissions.ExperimentalPermissionsApi
import com.google.accompanist.permissions.isGranted
import com.google.accompanist.permissions.rememberPermissionState

@OptIn(ExperimentalPermissionsApi::class)
@Composable
fun PermissionScreen() {
    val cameraPermission = rememberPermissionState(Manifest.permission.CAMERA)
    val micPermission = rememberPermissionState(Manifest.permission.RECORD_AUDIO)

    val requestPermissionLauncher = rememberLauncherForActivityResult(
        ActivityResultContracts.RequestMultiplePermissions()
    ) { isGranted ->
        Log.i("MainActivity", "Permissions granted: $isGranted")
    }

    if (!cameraPermission.status.isGranted || !micPermission.status.isGranted) {

        Dialog(
            onDismissRequest = {},
        ) {
            val dialogShape = RoundedCornerShape(16.dp)

            Column(
                Modifier
                    .shadow(6.dp, dialogShape)
                    .border(1.dp, Colors.logoBorder, dialogShape)
                    .clip(dialogShape)
                    .background(Color.White)
                    .padding(28.dp)
            ) {
                Text(
                    text = "Permissions",
                    fontSize = 24.sp,
                    fontWeight = FontWeight.W700,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(8.dp))

                Text(
                    text = "Please grant camera and mic permissions to continue",
                    fontSize = 18.sp,
                    fontWeight = FontWeight.W400,
                    style = TextStyles.base
                )

                Spacer(modifier = Modifier.height(36.dp))

                Button(
                    modifier = Modifier.align(Alignment.End),
                    shape = RoundedCornerShape(12.dp),
                    onClick = {
                        requestPermissionLauncher.launch(
                            arrayOf(
                                Manifest.permission.CAMERA,
                                Manifest.permission.RECORD_AUDIO
                            )
                        )
                    }
                ) {
                    Text(
                        text = "Grant permissions",
                        fontSize = 16.sp,
                        fontWeight = FontWeight.W700,
                        style = TextStyles.base
                    )
                }
            }
        }
    }
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/Timer.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.simple_chatbot_client.R
import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import ai.pipecat.simple_chatbot_client.utils.Timestamp
import ai.pipecat.simple_chatbot_client.utils.formatTimer
import ai.pipecat.simple_chatbot_client.utils.rtcStateSecs
import androidx.compose.foundation.background
import androidx.compose.foundation.layout.Row
import androidx.compose.foundation.layout.Spacer
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.layout.width
import androidx.compose.foundation.layout.widthIn
import androidx.compose.foundation.shape.RoundedCornerShape
import androidx.compose.material3.Icon
import androidx.compose.material3.Text
import androidx.compose.runtime.Composable
import androidx.compose.runtime.getValue
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.compose.ui.unit.sp
import java.time.Duration

@Composable
fun Timer(
    expiryTime: Timestamp,
    modifier: Modifier,
) {
    val now by rtcStateSecs()

    val shape = RoundedCornerShape(
        topStart = 12.dp,
        bottomStart = 12.dp,
    )

    Row(
        modifier = modifier
            .widthIn(min = 100.dp)
            .clip(shape)
            .background(Colors.lightGrey)
            .padding(top = 12.dp, bottom = 12.dp, start = 12.dp, end = 16.dp),
        verticalAlignment = Alignment.CenterVertically
    ) {
        Icon(
            painter = painterResource(id = R.drawable.timer_outline),
            contentDescription = null,
            modifier = Modifier.size(20.dp),
            tint = Colors.expiryTimerForeground
        )

        Spacer(Modifier.width(8.dp))

        Text(
            text = formatTimer(duration = expiryTime - now),
            fontSize = 16.sp,
            fontWeight = FontWeight.W600,
            color = Colors.expiryTimerForeground
        )
    }
}

@Composable
@Preview
fun PreviewExpiryTimer() {
    Timer(Timestamp.now() + Duration.ofMinutes(5), Modifier)
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/UserCamButton.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.client.daily.VoiceClientVideoView
import ai.pipecat.client.types.MediaTrackId
import ai.pipecat.simple_chatbot_client.R
import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import androidx.compose.animation.animateColorAsState
import androidx.compose.foundation.ExperimentalFoundationApi
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.combinedClickable
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.fillMaxSize
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.material3.Icon
import androidx.compose.runtime.Composable
import androidx.compose.runtime.getValue
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp
import androidx.compose.ui.viewinterop.AndroidView

@OptIn(ExperimentalFoundationApi::class)
@Composable
fun UserCamButton(
    onClick: () -> Unit,
    onLongClick: () -> Unit,
    camEnabled: Boolean,
    camTrackId: MediaTrackId?,
    modifier: Modifier,
) {
    Box(
        modifier = modifier.padding(15.dp).size(96.dp),
        contentAlignment = Alignment.Center
    ) {
        val color by animateColorAsState(
            if (camEnabled) {
                Colors.unmutedMicBackground
            } else {
                Colors.mutedMicBackground
            }
        )

        Box(
            Modifier
                .fillMaxSize()
                .shadow(3.dp, CircleShape)
                .border(6.dp, Color.White, CircleShape)
                .border(1.dp, Colors.lightGrey, CircleShape)
                .clip(CircleShape)
                .background(color)
                .combinedClickable(
                    onClick = onClick,
                    onLongClick = onLongClick
                ),
            contentAlignment = Alignment.Center,
        ) {
            if (camTrackId != null) {
                AndroidView(
                    factory = { context ->
                        VoiceClientVideoView(context)
                    },
                    update = { view ->
                        view.voiceClientTrack = camTrackId
                    }
                )
            } else {
                Icon(
                    modifier = Modifier.size(30.dp),
                    painter = painterResource(
                        if (camEnabled) {
                            R.drawable.video
                        } else {
                            R.drawable.video_off
                        }
                    ),
                    tint = Color.White,
                    contentDescription = if (camEnabled) {
                        "Disable camera"
                    } else {
                        "Enable camera"
                    },
                )
            }
        }
    }
}

@Composable
@Preview
fun PreviewUserCamButton() {
    UserCamButton(
        onClick = {},
        onLongClick = {},
        camTrackId = null,
        camEnabled = true,
        modifier = Modifier,
    )
}

@Composable
@Preview
fun PreviewUserCamButtonMuted() {
    UserCamButton(
        onClick = {},
        onLongClick = {},
        camTrackId = null,
        camEnabled = false,
        modifier = Modifier,
    )
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/UserMicButton.kt
================================================
package ai.pipecat.simple_chatbot_client.ui

import ai.pipecat.simple_chatbot_client.R
import ai.pipecat.simple_chatbot_client.ui.theme.Colors
import androidx.compose.animation.animateColorAsState
import androidx.compose.animation.core.animateDpAsState
import androidx.compose.foundation.background
import androidx.compose.foundation.border
import androidx.compose.foundation.clickable
import androidx.compose.foundation.layout.Box
import androidx.compose.foundation.layout.padding
import androidx.compose.foundation.layout.size
import androidx.compose.foundation.shape.CircleShape
import androidx.compose.material3.Icon
import androidx.compose.runtime.Composable
import androidx.compose.runtime.FloatState
import androidx.compose.runtime.State
import androidx.compose.runtime.getValue
import androidx.compose.runtime.mutableFloatStateOf
import androidx.compose.runtime.mutableStateOf
import androidx.compose.runtime.remember
import androidx.compose.ui.Alignment
import androidx.compose.ui.Modifier
import androidx.compose.ui.draw.clip
import androidx.compose.ui.draw.shadow
import androidx.compose.ui.graphics.Color
import androidx.compose.ui.res.painterResource
import androidx.compose.ui.tooling.preview.Preview
import androidx.compose.ui.unit.dp

@Composable
fun UserMicButton(
    onClick: () -> Unit,
    micEnabled: Boolean,
    modifier: Modifier,
    isTalking: State<Boolean>,
    audioLevel: FloatState,
) {
    Box(
        modifier = modifier.padding(15.dp),
        contentAlignment = Alignment.Center
    ) {
        val borderThickness by animateDpAsState(
            if (isTalking.value) {
                (24.dp * Math.pow(audioLevel.floatValue.toDouble(), 0.3).toFloat()) + 3.dp
            } else {
                6.dp
            }
        )

        val color by animateColorAsState(
            if (!micEnabled) {
                Colors.mutedMicBackground
            } else if (isTalking.value) {
                Color.Black
            } else {
                Colors.unmutedMicBackground
            }
        )

        Box(
            Modifier
                .shadow(3.dp, CircleShape)
                .border(borderThickness, Color.White, CircleShape)
                .border(1.dp, Colors.lightGrey, CircleShape)
                .clip(CircleShape)
                .background(color)
                .clickable(onClick = onClick)
                .padding(36.dp),
            contentAlignment = Alignment.Center,
        ) {
            Icon(
                modifier = Modifier.size(48.dp),
                painter = painterResource(
                    if (micEnabled) {
                        R.drawable.microphone
                    } else {
                        R.drawable.microphone_off
                    }
                ),
                tint = Color.White,
                contentDescription = if (micEnabled) {
                    "Mute microphone"
                } else {
                    "Unmute microphone"
                },
            )
        }
    }
}

@Composable
@Preview
fun PreviewUserMicButton() {
    UserMicButton(
        onClick = {},
        micEnabled = true,
        modifier = Modifier,
        isTalking = remember { mutableStateOf(false) },
        audioLevel = remember { mutableFloatStateOf(1.0f) }
    )
}

@Composable
@Preview
fun PreviewUserMicButtonMuted() {
    UserMicButton(
        onClick = {},
        micEnabled = false,
        modifier = Modifier,
        isTalking = remember { mutableStateOf(false) },
        audioLevel = remember { mutableFloatStateOf(1.0f) }
    )
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/theme/Color.kt
================================================
package ai.pipecat.simple_chatbot_client.ui.theme

import androidx.compose.ui.graphics.Color

object Colors {
    val buttonNormal = Color(0xFF374151)
    val buttonWarning = Color(0xFFE53935)
    val buttonSection = Color(0xFFDFF1FF)

    val activityBackground = Color(0xFFF9FAFB)
    val mainSurfaceBackground = Color.White

    val lightGrey = Color(0x7FE5E7EB)
    val expiryTimerForeground = Color.Black
    val logoBorder = Color(0xFFE2E8F0)
    val endButton = Color(0xFF0F172A)
    val textFieldBorder = Color(0xFFDFE6EF)

    val botIndicatorBackground = Color(0xFF374151)
    val mutedMicBackground = Color(0xFFF04A4A)
    val unmutedMicBackground = Color(0xFF616978)

    val logTextColor = Color(0x98b3d9FF)
}


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/theme/Theme.kt
================================================
package ai.pipecat.simple_chatbot_client.ui.theme

import androidx.compose.material3.MaterialTheme
import androidx.compose.material3.TextFieldDefaults
import androidx.compose.material3.lightColorScheme
import androidx.compose.runtime.Composable
import androidx.compose.ui.graphics.Color

private val LightColorScheme = lightColorScheme(
    primary = Colors.buttonNormal,
    secondary = Colors.buttonWarning,
    background = Colors.activityBackground,
    surface = Colors.mainSurfaceBackground
)

@Composable
fun RTVIClientTheme(
    content: @Composable () -> Unit
) {
    val colorScheme = LightColorScheme

    MaterialTheme(
        colorScheme = colorScheme,
        typography = Typography,
        content = content
    )
}

@Composable
fun textFieldColors() = TextFieldDefaults.colors().copy(
    unfocusedContainerColor = Colors.activityBackground,
    focusedContainerColor = Colors.activityBackground,
    focusedIndicatorColor = Color.Transparent,
    disabledIndicatorColor = Color.Transparent,
    unfocusedIndicatorColor = Color.Transparent,
)


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/ui/theme/Type.kt
================================================
package ai.pipecat.simple_chatbot_client.ui.theme

import ai.pipecat.simple_chatbot_client.R
import androidx.compose.material3.Typography
import androidx.compose.ui.text.TextStyle
import androidx.compose.ui.text.font.Font
import androidx.compose.ui.text.font.FontFamily
import androidx.compose.ui.text.font.FontWeight
import androidx.compose.ui.unit.sp

object TextStyles {
    val base = TextStyle(fontFamily = FontFamily(Font(R.font.inter)))
    val mono = TextStyle(fontFamily = FontFamily(Font(R.font.robotomono)))
}

// Set of Material typography styles to start with
val Typography = Typography(
    bodyLarge = TextStyle(
        fontFamily = FontFamily.Default,
        fontWeight = FontWeight.Normal,
        fontSize = 16.sp,
        lineHeight = 24.sp,
        letterSpacing = 0.5.sp
    )
    /* Other default text styles to override
    titleLarge = TextStyle(
        fontFamily = FontFamily.Default,
        fontWeight = FontWeight.Normal,
        fontSize = 22.sp,
        lineHeight = 28.sp,
        letterSpacing = 0.sp
    ),
    labelSmall = TextStyle(
        fontFamily = FontFamily.Default,
        fontWeight = FontWeight.Medium,
        fontSize = 11.sp,
        lineHeight = 16.sp,
        letterSpacing = 0.5.sp
    )
    */
)


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/utils/RealTimeClock.kt
================================================
package ai.pipecat.simple_chatbot_client.utils

import androidx.compose.runtime.Composable
import androidx.compose.runtime.collectAsState
import kotlinx.coroutines.delay
import kotlinx.coroutines.flow.flow

private val rtcFlowSecs = flow {
    while(true) {
        val now = Timestamp.now().toEpochMilli()

        val rounded = ((now + 500) / 1000) * 1000
        emit(Timestamp.ofEpochMilli(rounded))

        val target = rounded + 1000
        delay(target - now)
    }
}

@Composable
fun rtcStateSecs() = rtcFlowSecs.collectAsState(initial = Timestamp.now())


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/java/ai/pipecat/simple_chatbot_client/utils/TimeUtils.kt
================================================
package ai.pipecat.simple_chatbot_client.utils

import androidx.compose.runtime.Composable
import androidx.compose.runtime.Immutable
import java.time.Duration
import java.time.Instant
import java.time.format.DateTimeFormatter
import java.util.Date

// Wrapper for Compose stability
@Immutable
@JvmInline
value class Timestamp(
    val value: Instant
) : Comparable<Timestamp> {
    val isInPast: Boolean
        get() = value < Instant.now()

    val isInFuture: Boolean
        get() = value > Instant.now()

    fun toEpochMilli() = value.toEpochMilli()

    operator fun plus(duration: Duration) = Timestamp(value + duration)

    operator fun minus(duration: Duration) = Timestamp(value - duration)

    operator fun minus(rhs: Timestamp) = Duration.between(rhs.value, value)

    override operator fun compareTo(other: Timestamp) = value.compareTo(other.value)

    fun toISOString(): String = DateTimeFormatter.ISO_INSTANT.format(value)

    override fun toString() = toISOString()

    companion object {
        fun now() = Timestamp(Instant.now())

        fun ofEpochMilli(value: Long) = Timestamp(Instant.ofEpochMilli(value))

        fun ofEpochSecs(value: Long) = ofEpochMilli(value * 1000)

        fun parse(value: CharSequence) = Timestamp(Instant.parse(value))

        fun from(date: Date) = Timestamp(date.toInstant())
    }
}

@Composable
fun formatTimer(duration: Duration): String {

    if (duration.seconds < 0) {
        return "0s"
    }

    val mins = duration.seconds / 60
    val secs = duration.seconds % 60

    return if (mins == 0L) {
        "${secs}s"
    } else {
        "${mins}m ${secs}s"
    }
}



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/ic_launcher_background.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    android:width="108dp"
    android:height="108dp"
    android:viewportWidth="108"
    android:viewportHeight="108">
    <path
        android:fillColor="#3DDC84"
        android:pathData="M0,0h108v108h-108z" />
    <path
        android:fillColor="#00000000"
        android:pathData="M9,0L9,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,0L19,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M29,0L29,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M39,0L39,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M49,0L49,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M59,0L59,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M69,0L69,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M79,0L79,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M89,0L89,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M99,0L99,108"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,9L108,9"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,19L108,19"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,29L108,29"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,39L108,39"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,49L108,49"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,59L108,59"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,69L108,69"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,79L108,79"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,89L108,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M0,99L108,99"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,29L89,29"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,39L89,39"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,49L89,49"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,59L89,59"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,69L89,69"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M19,79L89,79"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M29,19L29,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M39,19L39,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M49,19L49,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M59,19L59,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M69,19L69,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
    <path
        android:fillColor="#00000000"
        android:pathData="M79,19L79,89"
        android:strokeWidth="0.8"
        android:strokeColor="#33FFFFFF" />
</vector>



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/ic_launcher_foreground.xml
================================================
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    xmlns:aapt="http://schemas.android.com/aapt"
    android:width="108dp"
    android:height="108dp"
    android:viewportWidth="108"
    android:viewportHeight="108">
    <path android:pathData="M31,63.928c0,0 6.4,-11 12.1,-13.1c7.2,-2.6 26,-1.4 26,-1.4l38.1,38.1L107,108.928l-32,-1L31,63.928z">
        <aapt:attr name="android:fillColor">
            <gradient
                android:endX="85.84757"
                android:endY="92.4963"
                android:startX="42.9492"
                android:startY="49.59793"
                android:type="linear">
                <item
                    android:color="#44000000"
                    android:offset="0.0" />
                <item
                    android:color="#00000000"
                    android:offset="1.0" />
            </gradient>
        </aapt:attr>
    </path>
    <path
        android:fillColor="#FFFFFF"
        android:fillType="nonZero"
        android:pathData="M65.3,45.828l3.8,-6.6c0.2,-0.4 0.1,-0.9 -0.3,-1.1c-0.4,-0.2 -0.9,-0.1 -1.1,0.3l-3.9,6.7c-6.3,-2.8 -13.4,-2.8 -19.7,0l-3.9,-6.7c-0.2,-0.4 -0.7,-0.5 -1.1,-0.3C38.8,38.328 38.7,38.828 38.9,39.228l3.8,6.6C36.2,49.428 31.7,56.028 31,63.928h46C76.3,56.028 71.8,49.428 65.3,45.828zM43.4,57.328c-0.8,0 -1.5,-0.5 -1.8,-1.2c-0.3,-0.7 -0.1,-1.5 0.4,-2.1c0.5,-0.5 1.4,-0.7 2.1,-0.4c0.7,0.3 1.2,1 1.2,1.8C45.3,56.528 44.5,57.328 43.4,57.328L43.4,57.328zM64.6,57.328c-0.8,0 -1.5,-0.5 -1.8,-1.2s-0.1,-1.5 0.4,-2.1c0.5,-0.5 1.4,-0.7 2.1,-0.4c0.7,0.3 1.2,1 1.2,1.8C66.5,56.528 65.6,57.328 64.6,57.328L64.6,57.328z"
        android:strokeWidth="1"
        android:strokeColor="#00000000" />
</vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/microphone.xml
================================================
<!-- drawable/microphone.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M12,2A3,3 0 0,1 15,5V11A3,3 0 0,1 12,14A3,3 0 0,1 9,11V5A3,3 0 0,1 12,2M19,11C19,14.53 16.39,17.44 13,17.93V21H11V17.93C7.61,17.44 5,14.53 5,11H7A5,5 0 0,0 12,16A5,5 0 0,0 17,11H19Z" /></vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/microphone_off.xml
================================================
<!-- drawable/microphone_off.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M19,11C19,12.19 18.66,13.3 18.1,14.28L16.87,13.05C17.14,12.43 17.3,11.74 17.3,11H19M15,11.16L9,5.18V5A3,3 0 0,1 12,2A3,3 0 0,1 15,5V11L15,11.16M4.27,3L21,19.73L19.73,21L15.54,16.81C14.77,17.27 13.91,17.58 13,17.72V21H11V17.72C7.72,17.23 5,14.41 5,11H6.7C6.7,14 9.24,16.1 12,16.1C12.81,16.1 13.6,15.91 14.31,15.58L12.65,13.92L12,14A3,3 0 0,1 9,11V10.28L3,4.27L4.27,3Z" /></vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/phone_hangup.xml
================================================
<!-- drawable/phone_hangup.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M12,9C10.4,9 8.85,9.25 7.4,9.72V12.82C7.4,13.22 7.17,13.56 6.84,13.72C5.86,14.21 4.97,14.84 4.17,15.57C4,15.75 3.75,15.86 3.5,15.86C3.2,15.86 2.95,15.74 2.77,15.56L0.29,13.08C0.11,12.9 0,12.65 0,12.38C0,12.1 0.11,11.85 0.29,11.67C3.34,8.77 7.46,7 12,7C16.54,7 20.66,8.77 23.71,11.67C23.89,11.85 24,12.1 24,12.38C24,12.65 23.89,12.9 23.71,13.08L21.23,15.56C21.05,15.74 20.8,15.86 20.5,15.86C20.25,15.86 20,15.75 19.82,15.57C19.03,14.84 18.14,14.21 17.16,13.72C16.83,13.56 16.6,13.22 16.6,12.82V9.72C15.15,9.25 13.6,9 12,9Z" /></vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/send.xml
================================================
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    android:width="24dp"
    android:height="24dp"
    android:viewportWidth="960"
    android:viewportHeight="960"
    android:tint="?attr/colorControlNormal"
    android:autoMirrored="true">
  <path
      android:fillColor="@android:color/white"
      android:pathData="M120,800L120,160L880,480L120,800ZM200,680L674,480L200,280L200,420L440,480L200,540L200,680ZM200,680L200,480L200,280L200,420L200,420L200,540L200,540L200,680Z"/>
</vector>



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/three_dots.xml
================================================
<vector xmlns:android="http://schemas.android.com/apk/res/android"
    android:width="24dp"
    android:height="24dp"
    android:viewportWidth="24"
    android:viewportHeight="24">
  <path
      android:fillColor="@android:color/white"
      android:pathData="M12,8c1.1,0 2,-0.9 2,-2s-0.9,-2 -2,-2 -2,0.9 -2,2 0.9,2 2,2zM12,10c-1.1,0 -2,0.9 -2,2s0.9,2 2,2 2,-0.9 2,-2 -0.9,-2 -2,-2zM12,16c-1.1,0 -2,0.9 -2,2s0.9,2 2,2 2,-0.9 2,-2 -0.9,-2 -2,-2z"/>
</vector>



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/timer_outline.xml
================================================
<!-- drawable/timer_outline.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M12,20A7,7 0 0,1 5,13A7,7 0 0,1 12,6A7,7 0 0,1 19,13A7,7 0 0,1 12,20M19.03,7.39L20.45,5.97C20,5.46 19.55,5 19.04,4.56L17.62,6C16.07,4.74 14.12,4 12,4A9,9 0 0,0 3,13A9,9 0 0,0 12,22C17,22 21,17.97 21,13C21,10.88 20.26,8.93 19.03,7.39M11,14H13V8H11M15,1H9V3H15V1Z" /></vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/video.xml
================================================
<!-- drawable/video.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M17,10.5V7A1,1 0 0,0 16,6H4A1,1 0 0,0 3,7V17A1,1 0 0,0 4,18H16A1,1 0 0,0 17,17V13.5L21,17.5V6.5L17,10.5Z" /></vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/drawable/video_off.xml
================================================
<!-- drawable/video_off.xml --><vector xmlns:android="http://schemas.android.com/apk/res/android" android:height="24dp" android:width="24dp" android:viewportWidth="24" android:viewportHeight="24"><path android:fillColor="#000000" android:pathData="M3.27,2L2,3.27L4.73,6H4A1,1 0 0,0 3,7V17A1,1 0 0,0 4,18H16C16.2,18 16.39,17.92 16.54,17.82L19.73,21L21,19.73M21,6.5L17,10.5V7A1,1 0 0,0 16,6H9.82L21,17.18V6.5Z" /></vector>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-anydpi-v26/ic_launcher.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<adaptive-icon xmlns:android="http://schemas.android.com/apk/res/android">
    <background android:drawable="@drawable/ic_launcher_background" />
    <foreground android:drawable="@drawable/ic_launcher_foreground" />
    <monochrome android:drawable="@drawable/ic_launcher_foreground" />
</adaptive-icon>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-anydpi-v26/ic_launcher_round.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<adaptive-icon xmlns:android="http://schemas.android.com/apk/res/android">
    <background android:drawable="@drawable/ic_launcher_background" />
    <foreground android:drawable="@drawable/ic_launcher_foreground" />
    <monochrome android:drawable="@drawable/ic_launcher_foreground" />
</adaptive-icon>


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-hdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-hdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-mdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-mdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-xhdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-xhdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-xxhdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-xxhdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-xxxhdpi/ic_launcher.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/mipmap-xxxhdpi/ic_launcher_round.webp
================================================
[Binary file]


================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/values/strings.xml
================================================
<resources>
    <string name="app_name">Pipecat Simple Chatbot Client</string>
</resources>



================================================
FILE: simple-chatbot/client/android/simple-chatbot-client/src/main/res/values/themes.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<resources>

    <style name="Theme.RTVIClient" parent="android:Theme.Material.Light.NoActionBar" />
</resources>


================================================
FILE: simple-chatbot/client/ios/README.md
================================================
# iOS implementation

Basic implementation using the [Pipecat iOS SDK](https://docs.pipecat.ai/client/ios/introduction).

## Prerequisites

1. Run the server-side bot; see [README](../../server/README).
2. Install [Xcode 15](https://developer.apple.com/xcode/), and set up your device [to run your own applications](https://developer.apple.com/documentation/xcode/distributing-your-app-to-registered-devices).

## Running locally

1. Clone this repository locally.
2. Open the SimpleChatbot.xcodeproj in Xcode.
3. Tell Xcode to update its Package Dependencies by clicking File -> Packages -> Update to Latest Package Versions.
4. Build the project.
5. Run the project on your device.
6. Connect to the URL you are testing.



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>UIBackgroundModes</key>
	<array>
		<string>voip</string>
	</array>
	<key>NSCameraUsageDescription</key>
    <string>Camera is necessary for transmitting video in a call</string>
    <key>NSMicrophoneUsageDescription</key>
    <string>Microphone is necessary for transmitting audio in a call</string>
    <key>NSAppTransportSecurity</key>
    <dict>
        <key>NSAllowsLocalNetworking</key>
        <true/>
    </dict>
</dict>
</plist>



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/SimpleChatbotApp.swift
================================================
import SwiftUI

@main
struct SimpleChatbotApp: App {

    @StateObject var callContainerModel = CallContainerModel()

    var body: some Scene {
        WindowGroup {
            if (!callContainerModel.isInCall) {
                PreJoinView().environmentObject(callContainerModel)
            } else {
                MeetingView().environmentObject(callContainerModel)
            }
        }
    }

}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "appstore.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/Assets.xcassets/pipecat.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "Square Black.svg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/model/CallContainerModel.swift
================================================
import SwiftUI

import PipecatClientIOSDaily
import PipecatClientIOS

class CallContainerModel: ObservableObject {
    
    @Published var voiceClientStatus: String = TransportState.disconnected.description
    @Published var isInCall: Bool = false
    @Published var isBotReady: Bool = false
    @Published var timerCount = 0
    
    @Published var isMicEnabled: Bool = false
    
    @Published var toastMessage: String? = nil
    @Published var showToast: Bool = false
    
    @Published
    var remoteAudioLevel: Float = 0
    @Published
    var localAudioLevel: Float = 0
    
    private var meetingTimer: Timer?
    
    var pipecatClientIOS: PipecatClient?
    
    @Published var selectedMic: MediaDeviceId? = nil {
        didSet {
            guard let selectedMic else { return } // don't store nil
            var settings = SettingsManager.getSettings()
            settings.selectedMic = selectedMic.id
            SettingsManager.updateSettings(settings: settings)
        }
    }
    @Published var availableMics: [MediaDeviceInfo] = []
    
    init() {
        // Changing the log level
        PipecatClientIOS.setLogLevel(.warn)
    }
    
    @MainActor
    func connect(backendURL: String) {
        let baseUrl = backendURL.trimmingCharacters(in: .whitespacesAndNewlines)
        if(baseUrl.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty){
            self.showError(message: "Need to fill the backendURL. For more info visit: https://bots.daily.co")
            return
        }
        
        let currentSettings = SettingsManager.getSettings()
        let pipecatClientOptions = PipecatClientOptions.init(
            transport: DailyTransport.init(),
            enableMic: currentSettings.enableMic,
            enableCam: false,
        )
        self.pipecatClientIOS = PipecatClient.init(
            options: pipecatClientOptions
        )
        self.pipecatClientIOS?.delegate = self
        let startBotParams = APIRequest.init(endpoint: URL(string: baseUrl + "/start")!)
        self.pipecatClientIOS?.startBotAndConnect(startBotParams: startBotParams) { (result: Result<DailyTransportConnectionParams, AsyncExecutionError>) in
            switch result {
            case .failure(let error):
                self.showError(message: error.localizedDescription)
                self.pipecatClientIOS = nil
            case .success(_):
                // Apply initial mic preference
                if let selectedMic = currentSettings.selectedMic {
                    self.selectMic(MediaDeviceId(id: selectedMic))
                }
                // Populate available devices list
                self.availableMics = self.pipecatClientIOS?.getAllMics() ?? []
            }
        }
        self.saveCredentials(backendURL: baseUrl)
    }
    
    func disconnect() {
        Task { @MainActor in
            try await self.pipecatClientIOS?.disconnect()
            self.pipecatClientIOS?.release()
        }
    }
    
    func showError(message: String) {
        self.toastMessage = message
        self.showToast = true
        // Hide the toast after 5 seconds
        DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
            self.showToast = false
            self.toastMessage = nil
        }
    }
    
    @MainActor
    func toggleMicInput() {
        self.pipecatClientIOS?.enableMic(enable: !self.isMicEnabled) { result in
            switch result {
            case .success():
                self.isMicEnabled = self.pipecatClientIOS?.isMicEnabled ?? false
            case .failure(let error):
                self.showError(message: error.localizedDescription)
            }
        }
    }
    
    private func startTimer() {
        let currentTime = Int(Date().timeIntervalSince1970)
        self.timerCount = 0
        self.meetingTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { timer in
            DispatchQueue.main.async {
                self.timerCount += 1
            }
        }
    }
    
    private func stopTimer() {
        self.meetingTimer?.invalidate()
        self.meetingTimer = nil
        self.timerCount = 0
    }
    
    func saveCredentials(backendURL: String) {
        var currentSettings = SettingsManager.getSettings()
        currentSettings.backendURL = backendURL
        // Saving the settings
        SettingsManager.updateSettings(settings: currentSettings)
    }
    
    @MainActor
    func selectMic(_ mic: MediaDeviceId) {
        self.selectedMic = mic
        self.pipecatClientIOS?.updateMic(micId: mic, completion: nil)
    }
}

extension CallContainerModel:PipecatClientDelegate {
    private func handleEvent(eventName: String, eventValue: Any? = nil) {
        if let value = eventValue {
            print("RTVI Demo, received event:\(eventName), value:\(value)")
        } else {
            print("RTVI Demo, received event: \(eventName)")
        }
    }
    
    func onTransportStateChanged(state: TransportState) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTransportStateChanged", eventValue: state)
            self.voiceClientStatus = state.description
            self.isInCall = ( state == .connecting || state == .connected || state == .ready || state == .authenticating )
        }
    }
    
    func onBotReady(botReadyData: BotReadyData) {
        Task { @MainActor in
            self.handleEvent(eventName: "onBotReady.")
            self.isBotReady = true
            self.startTimer()
        }
    }
    
    func onConnected() {
        Task { @MainActor in
            self.isMicEnabled = self.pipecatClientIOS?.isMicEnabled ?? false
        }
    }
    
    func onDisconnected() {
        Task { @MainActor in
            self.stopTimer()
            self.isBotReady = false
        }
    }
    
    func onRemoteAudioLevel(level: Float, participant: Participant) {
        Task { @MainActor in
            self.remoteAudioLevel = level
        }
    }
    
    func onLocalAudioLevel(level: Float) {
        Task { @MainActor in
            self.localAudioLevel = level
        }
    }
    
    func onUserTranscript(data: Transcript) {
        Task { @MainActor in
            if (data.final ?? false) {
                self.handleEvent(eventName: "onUserTranscript", eventValue: data.text)
            }
        }
    }
    
    func onBotTranscript(data: BotLLMText) {
        Task { @MainActor in
            self.handleEvent(eventName: "onBotTranscript", eventValue: data)
        }
    }
    
    func onError(message: RTVIMessageInbound) {
        Task { @MainActor in
            self.handleEvent(eventName: "onError", eventValue: message)
            self.showError(message: message.data ?? "")
        }
    }
    
    func onTrackStarted(track: MediaStreamTrack, participant: Participant?) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTrackStarted", eventValue: track)
        }
    }

    func onTrackStopped(track: MediaStreamTrack, participant: Participant?) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTrackStopped", eventValue: track)
        }
    }
    
    func onAvailableMicsUpdated(mics: [MediaDeviceInfo]) {
        Task { @MainActor in
            self.availableMics = mics
        }
    }
    
    func onMicUpdated(mic: MediaDeviceInfo?) {
        Task { @MainActor in
            self.selectedMic = mic?.id
        }
    }
    
    func onMetrics(data: PipecatMetrics) {
        Task { @MainActor in
            self.handleEvent(eventName: "onMetrics", eventValue: data)
        }
    }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/model/MockCallContainerModel.swift
================================================
import SwiftUI
import PipecatClientIOS

class MockCallContainerModel: CallContainerModel {

    override init() {
    }

    override func connect(backendURL: String) {
        print("connect")
    }

    override func disconnect() {
        print("disconnect")
    }

    override func showError(message: String) {
        self.toastMessage = message
        self.showToast = true
        // Hide the toast after 5 seconds
        DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
            self.showToast = false
            self.toastMessage = nil
        }
    }

    func startAudioLevelSimulation() {
        // Simulate audio level changes
        Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { _ in
            let newLevel = Float.random(in: 0...1)
            self.remoteAudioLevel = newLevel
            self.localAudioLevel = newLevel
        }
    }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/MeetingView.swift
================================================
import SwiftUI

struct MeetingView: View {
    
    @State private var showingSettings = false
    @EnvironmentObject private var model: CallContainerModel
    
    var body: some View {
        VStack {
            // Header Toolbar
            HStack {
                Image("dailyBot")
                    .resizable()
                    .frame(width: 48, height: 48)
                Spacer()
                HStack {
                    Image(systemName: "stopwatch")
                        .resizable()
                        .frame(width: 24, height: 24)
                    Text(timerString(from: self.model.timerCount))
                        .font(.headline)
                }.padding()
                    .background(Color.timer)
                    .cornerRadius(12)
            }
            .padding()
            
            // Main Panel
            VStack {
                VStack {
                    WaveformView(audioLevel: model.remoteAudioLevel, isBotReady: model.isBotReady, voiceClientStatus: model.voiceClientStatus)
                }
                .frame(maxHeight: .infinity)
                VStack {
                    HStack {
                        MicrophoneView(audioLevel: model.localAudioLevel, isMuted: !self.model.isMicEnabled)
                            .frame(width: 160, height: 160)
                            .onTapGesture {
                                self.model.toggleMicInput()
                            }
                    }
                }
                .frame(height: 120)
            }
            .frame(maxHeight: .infinity)
            .padding()
            
            // Bottom Panel
            VStack {
                HStack {
                    Button(action: {
                        self.showingSettings = true
                    }) {
                        HStack {
                            Image(systemName: "gearshape")
                                .resizable()
                                .frame(width: 24, height: 24)
                            Text("Settings")
                        }
                        .frame(maxWidth: .infinity)
                        .padding()
                        .sheet(isPresented: $showingSettings) {
                            SettingsView(showingSettings: $showingSettings).environmentObject(self.model)
                        }
                    }
                    .border(Color.buttonsBorder, width: 1)
                    .cornerRadius(12)
                }
                .foregroundColor(.black)
                .padding([.top, .horizontal])
                
                Button(action: {
                    self.model.disconnect()
                }) {
                    HStack {
                        Image(systemName: "rectangle.portrait.and.arrow.right")
                            .resizable()
                            .frame(width: 24, height: 24)
                        Text("End")
                    }
                    .frame(maxWidth: .infinity)
                    .padding()
                }
                .foregroundColor(.white)
                .background(Color.black)
                .cornerRadius(12)
                .padding([.bottom, .horizontal])
            }
        }
        .background(Color.backgroundApp)
        .toast(message: model.toastMessage, isShowing: model.showToast)
    }
    
    func timerString(from count: Int) -> String {
        let hours = count / 3600
        let minutes = (count % 3600) / 60
        let seconds = count % 60
        return String(format: "%02d:%02d:%02d", hours, minutes, seconds)
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = MeetingView().environmentObject(mockModel as CallContainerModel)
    mockModel.startAudioLevelSimulation()
    return result
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/PreJoinView.swift
================================================
import SwiftUI

struct PreJoinView: View {
    
    @State var backendURL: String

    @EnvironmentObject private var model: CallContainerModel
    
    init() {
        let currentSettings = SettingsManager.getSettings()
        self.backendURL = currentSettings.backendURL
    }

    var body: some View {
        VStack(spacing: 20) {
            Image("pipecat")
                .resizable()
                .frame(width: 80, height: 80)
            Text("Pipecat Client iOS.")
                .font(.headline)
            TextField("Server URL", text: $backendURL)
                .textFieldStyle(RoundedBorderTextFieldStyle())
                .frame(maxWidth: .infinity)
                .padding([.bottom, .horizontal])
            Button("Connect") {
                self.model.connect(backendURL: self.backendURL)
            }
            .padding()
            .background(Color.black)
            .foregroundColor(.white)
            .cornerRadius(8)
        }
        .padding()
        .frame(maxHeight: .infinity)
        .background(Color.backgroundApp)
        .toast(message: model.toastMessage, isShowing: model.showToast)
    }
}

#Preview {
    PreJoinView().environmentObject(MockCallContainerModel() as CallContainerModel)
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/components/MicrophoneView.swift
================================================
import SwiftUI

struct MicrophoneView: View {
    var audioLevel: Float // Current audio level
    var isMuted: Bool // Muted state

    var body: some View {
        GeometryReader { geometry in
            let width = geometry.size.width
            let circleSize = width * 0.9
            let innerCircleSize = width * 0.82
            let audioCircleSize = CGFloat(audioLevel) * (width * 0.95)

            ZStack {
                Circle()
                    .stroke(Color.gray, lineWidth: 1)
                    .frame(width: circleSize)

                Circle()
                    .fill(isMuted ? Color.disabledMic : Color.backgroundCircle)
                    .frame(width: innerCircleSize)

                if !isMuted {
                    Circle()
                        .fill(Color.micVolume)
                        .opacity(0.5)
                        .frame(width: audioCircleSize)
                        .animation(.easeInOut(duration: 0.2), value: audioLevel)
                }

                Image(systemName: isMuted ? "mic.slash.fill" : "mic.fill")
                    .resizable()
                    .scaledToFit()
                    .frame(width: width * 0.2)
                    .foregroundColor(.white)
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity) // Ensures the ZStack is centered
        }
    }
}

#Preview {
    MicrophoneView(audioLevel: 1, isMuted: false)
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/components/ToastModifier.swift
================================================
import SwiftUI

struct ToastModifier: ViewModifier {
    var message: String?
    var isShowing: Bool
    
    func body(content: Content) -> some View {
        ZStack {
            content
            if isShowing, let message = message {
                VStack {
                    Text(message)
                        .padding()
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(8)
                        .transition(.slide)
                        .padding(.top, 50)
                    Spacer()
                }
                .animation(.easeInOut(duration: 0.5), value: isShowing)
            }
        }
    }
}

extension View {
    func toast(message: String?, isShowing: Bool) -> some View {
        self.modifier(ToastModifier(message: message, isShowing: isShowing))
    }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/components/WaveformView.swift
================================================
import SwiftUI

struct WaveformView: View {
    
    var audioLevel: Float
    var isBotReady: Bool
    var voiceClientStatus: String
    
    @State
    private var audioLevels: [Float] = Array(repeating: 0, count: 5)
    private let dotCount = 5
    
    var body: some View {
        GeometryReader { geometry in
            VStack {
                Spacer()
                HStack {
                    Spacer()
                    ZStack {
                        // Outer gray border
                        Circle()
                            .stroke(Color.gray, lineWidth: 1)
                            .frame(width: geometry.size.width * 0.9, height: geometry.size.width * 0.9)
                        
                        // Gray middle
                        Circle()
                            .fill(isBotReady ? Color.backgroundCircle : Color.backgroundCircleNotConnected)
                            .frame(width: geometry.size.width * 0.82, height: geometry.size.width * 0.82)
                        
                        if isBotReady {
                            if audioLevel > 0 {
                                // Waveform bars inside the circle
                                HStack(spacing: 10) {
                                    ForEach(0..<dotCount, id: \.self) { index in
                                        Rectangle()
                                            .fill(Color.white)
                                            .frame(height: CGFloat(audioLevels[index]) * (geometry.size.height))
                                            .cornerRadius(12)
                                            .animation(.easeInOut(duration: 0.2), value: audioLevels[index])
                                    }
                                    .frame(maxWidth: .infinity)
                                    .padding(.horizontal, 5)
                                }
                                .frame(width: geometry.size.width * 0.5, height: geometry.size.width * 0.5)
                                .mask(Circle().frame(width: geometry.size.width * 0.82, height: geometry.size.width * 0.82))
                            } else {
                                // Dots inside the circle
                                HStack(spacing: 10) {
                                    ForEach(0..<dotCount, id: \.self) { _ in
                                        Circle()
                                            .fill(Color.white)
                                            .frame(maxWidth: .infinity)
                                    }
                                    .frame(maxWidth: .infinity)
                                }
                                .frame(width: geometry.size.width * 0.5, height: geometry.size.height * 0.5)
                            }
                        } else {
                            // Gray circle with loading icon when not connected
                            VStack {
                                ProgressView()
                                    .progressViewStyle(CircularProgressViewStyle(tint: .white))
                                    .scaleEffect(2) // Adjust size of the loading spinner
                                    .padding()
                                Text(voiceClientStatus)
                                    .foregroundColor(.white)
                                    .font(.headline)
                            }
                        }
                    }
                    Spacer()
                }
                Spacer()
            }
        }
        .onChange(of: audioLevel) { oldLevel, newLevel in
            // The audio level that we receive from the bot is usually too low
            // so just increasing it so we can see a better graph but
            // making sure that it is not higher than the maximum 1
            var audioLevel = audioLevel + 0.4
            if(audioLevel > 1) {
                audioLevel = 1
            }
            // Update the array and shift values
            audioLevels.removeFirst()
            audioLevels.append(newLevel)
        }
    }
}

#Preview {
    WaveformView(audioLevel: 0, isBotReady: false, voiceClientStatus: "idle")
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/extensions/CustomColors.swift
================================================
import SwiftUI

public extension Color {
    
    static let backgroundCircle = Color(hex: "#374151")
    static let backgroundCircleNotConnected = Color(hex: "#D1D5DB")
    static let backgroundApp = Color(hex: "#F9FAFB")
    static let buttonsBorder = Color(hex: "#E5E7EB")
    static let micVolume = Color(hex: "#86EFAC")
    static let timer = Color(hex: "#E5E7EB")
    static let disabledMic = Color(hex: "#ee6b6e")
    static let disabledVision = Color(hex: "#BBF7D0")
        
    init(hex: String) {
        let scanner = Scanner(string: hex)
        _ = scanner.scanString("#")
        
        var rgb: UInt64 = 0
        scanner.scanHexInt64(&rgb)
        
        let red = Double((rgb >> 16) & 0xFF) / 255.0
        let green = Double((rgb >> 8) & 0xFF) / 255.0
        let blue = Double(rgb & 0xFF) / 255.0
        
        self.init(red: red, green: green, blue: blue)
    }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/settings/SettingsManager.swift
================================================
import Foundation

class SettingsManager {
    private static let preferencesKey = "settingsPreference"

    static func getSettings() -> SettingsPreference {
        if let data = UserDefaults.standard.data(forKey: preferencesKey),
           let settings = try? JSONDecoder().decode(SettingsPreference.self, from: data) {
            return settings
        } else {
            // default values in case we don't have any settings
            return SettingsPreference(enableMic: true, backendURL: "http://YOUR_IP:7860")
        }
    }
    
    static func updateSettings(settings: SettingsPreference) {
        if let data = try? JSONEncoder().encode(settings) {
            UserDefaults.standard.set(data, forKey: preferencesKey)
        }
    }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/settings/SettingsPreference.swift
================================================
import Foundation

struct SettingsPreference: Codable {
    var selectedMic: String?
    var enableMic: Bool
    var backendURL: String
}




================================================
FILE: simple-chatbot/client/ios/SimpleChatbot/views/settings/SettingsView.swift
================================================
import SwiftUI
import PipecatClientIOS

struct SettingsView: View {
    
    @EnvironmentObject private var model: CallContainerModel
    
    @Binding var showingSettings: Bool
    
    @State private var isMicEnabled: Bool = true
    @State private var backendURL: String = ""
    
    var body: some View {
        NavigationView {
            Form {
                Section {
                    List(model.availableMics, id: \.self.id.id) { mic in
                        Button(action: {
                            model.selectMic(mic.id)
                        }) {
                            HStack {
                                Text(mic.name)
                                Spacer()
                                if mic.id == model.selectedMic {
                                    Image(systemName: "checkmark")
                                }
                            }
                        }
                    }
                } header: {
                    VStack(alignment: .leading) {
                        Text("Audio Settings")
                        Text("(No selection = system default)")
                    }
                }
                Section(header: Text("Start options")) {
                    Toggle("Enable Microphone", isOn: $isMicEnabled)
                }
                Section(header: Text("Server")) {
                    TextField("Backend URL", text: $backendURL)
                        .keyboardType(.URL)
                }
            }
            .navigationTitle("Settings")
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Close") {
                        self.saveSettings()
                        self.showingSettings = false
                    }
                }
            }
            .onAppear {
                self.loadSettings()
            }
        }
    }
    
    private func saveSettings() {
        let newSettings = SettingsPreference(
            selectedMic: model.selectedMic?.id,
            enableMic: isMicEnabled,
            backendURL: backendURL
        )
        SettingsManager.updateSettings(settings: newSettings)
    }
    
    private func loadSettings() {
        let savedSettings = SettingsManager.getSettings()
        self.isMicEnabled = savedSettings.enableMic
        self.backendURL = savedSettings.backendURL
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = SettingsView(showingSettings: .constant(true)).environmentObject(mockModel as CallContainerModel)
    mockModel.startAudioLevelSimulation()
    return result
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbotTests/SimpleChatbotTests.swift
================================================
import XCTest
@testable import SimpleChatbot

final class SimpleChatbotTests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        // This is an example of a functional test case.
        // Use XCTAssert and related functions to verify your tests produce the correct results.
        // Any test you write for XCTest can be annotated as throws and async.
        // Mark your test throws to produce an unexpected failure when your test encounters an uncaught error.
        // Mark your test async to allow awaiting for asynchronous code to complete. Check the results with assertions afterwards.
    }

    func testPerformanceExample() throws {
        // This is an example of a performance test case.
        self.measure {
            // Put the code you want to measure the time of here.
        }
    }

}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbotUITests/SimpleChatbotUITests.swift
================================================
import XCTest

final class SimpleChatbotUITests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.

        // In UI tests it is usually best to stop immediately when a failure occurs.
        continueAfterFailure = false

        // In UI tests it’s important to set the initial state - such as interface orientation - required for your tests before they run. The setUp method is a good place to do this.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        // UI tests must launch the application that they test.
        let app = XCUIApplication()
        app.launch()

        // Use XCTAssert and related functions to verify your tests produce the correct results.
    }

    func testLaunchPerformance() throws {
        if #available(macOS 10.15, iOS 13.0, tvOS 13.0, watchOS 7.0, *) {
            // This measures how long it takes to launch your application.
            measure(metrics: [XCTApplicationLaunchMetric()]) {
                XCUIApplication().launch()
            }
        }
    }
}



================================================
FILE: simple-chatbot/client/ios/SimpleChatbotUITests/SimpleChatbotUITestsLaunchTests.swift
================================================
import XCTest

final class SimpleChatbotUITestsLaunchTests: XCTestCase {

    override class var runsForEachTargetApplicationUIConfiguration: Bool {
        true
    }

    override func setUpWithError() throws {
        continueAfterFailure = false
    }

    func testLaunch() throws {
        let app = XCUIApplication()
        app.launch()

        // Insert steps here to perform after app launch but before taking a screenshot,
        // such as logging into a test account or navigating somewhere in the app

        let attachment = XCTAttachment(screenshot: app.screenshot())
        attachment.name = "Launch Screen"
        attachment.lifetime = .keepAlways
        add(attachment)
    }
}



================================================
FILE: simple-chatbot/client/javascript/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the server-side bot; see [README](../../server/README).

2. Navigate to the `client/javascript` directory:

```bash
cd client/javascript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: simple-chatbot/client/javascript/env.example
================================================
# For local development:
VITE_BOT_START_URL="http://localhost:7860/start"

# For Pipecat Cloud (replace {agentName} with your agent name):
# VITE_BOT_START_URL="https://api.pipecat.daily.co/v1/public/{agentName}/start"
# VITE_BOT_START_PUBLIC_API_KEY="your-pipecat-cloud-public-api-key-here"

# Note: In production, these values will be exposed in the browser bundle.
# For production apps, consider using a backend proxy to protect secrets.


================================================
FILE: simple-chatbot/client/javascript/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Voice UI Kit - Vanilla JS</title>
</head>

<body>
  <div class="container">
    <!-- Header with controls -->
    <div class="header">
      <div class="transport-selector">
        <label for="transport-select">Transport:</label>
        <select id="transport-select">
          <option value="smallwebrtc">SmallWebRTC</option>
          <option value="daily">Daily</option>
        </select>
      </div>
      <div class="controls">
        <button id="mic-btn" class="control-btn" disabled>
          🎤 <span id="mic-status">Mic is Off</span>
        </button>
        <button id="connect-btn" class="connect-btn">Connect</button>
      </div>
    </div>

    <!-- Main content area with 50/50 split -->
    <div class="main-content">
      <!-- Video panel (left) -->
      <div class="video-panel">
        <div id="bot-video-container">
          <div class="video-placeholder">
            <span>Video will appear here when connected</span>
          </div>
        </div>
      </div>

      <!-- Conversation panel (right) -->
      <div class="conversation-panel">
        <h3>Conversation</h3>
        <div id="conversation-log"></div>
      </div>
    </div>

    <!-- Events panel -->
    <div class="events-panel">
      <h3>Events</h3>
      <div id="events-log"></div>
    </div>
  </div>

  <script type="module" src="/src/app.js"></script>
  <link rel="stylesheet" href="/src/style.css" />
</body>

</html>


================================================
FILE: simple-chatbot/client/javascript/package.json
================================================
{
  "name": "test-js",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "preview": "vite preview"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.5.0",
    "@pipecat-ai/daily-transport": "^1.5.0",
    "@pipecat-ai/small-webrtc-transport": "^1.8.0"
  },
  "devDependencies": {
    "vite": "^7.1.7"
  }
}



================================================
FILE: simple-chatbot/client/javascript/src/app.js
================================================
import { PipecatClient, RTVIEvent } from '@pipecat-ai/client-js';
import {
  AVAILABLE_TRANSPORTS,
  DEFAULT_TRANSPORT,
  TRANSPORT_CONFIG,
  createTransport,
} from './config';

class VoiceChatClient {
  constructor() {
    this.client = null;
    this.transportType = DEFAULT_TRANSPORT;
    this.isConnected = false;

    this.setupDOM();
    this.setupEventListeners();
    this.addEvent('initialized', 'Client initialized');
  }

  setupDOM() {
    this.transportSelect = document.getElementById('transport-select');
    this.connectBtn = document.getElementById('connect-btn');
    this.micBtn = document.getElementById('mic-btn');
    this.micStatus = document.getElementById('mic-status');
    this.conversationLog = document.getElementById('conversation-log');
    this.eventsLog = document.getElementById('events-log');
    this.botVideoContainer = document.getElementById('bot-video-container');

    // Populate transport selector with available transports
    this.transportSelect.innerHTML = '';
    AVAILABLE_TRANSPORTS.forEach((transport) => {
      const option = document.createElement('option');
      option.value = transport;
      option.textContent =
        transport.charAt(0).toUpperCase() + transport.slice(1);
      if (transport === 'smallwebrtc') {
        option.textContent = 'SmallWebRTC';
      } else if (transport === 'daily') {
        option.textContent = 'Daily';
      }
      this.transportSelect.appendChild(option);
    });

    // Hide transport selector if only one transport
    if (AVAILABLE_TRANSPORTS.length === 1) {
      this.transportSelect.parentElement.style.display = 'none';
    }

    // Add placeholder message
    this.addConversationMessage(
      'Connect to start talking with your bot',
      'placeholder'
    );
  }

  setupEventListeners() {
    this.transportSelect.addEventListener('change', (e) => {
      this.transportType = e.target.value;
      this.addEvent('transport-changed', this.transportType);
    });

    this.connectBtn.addEventListener('click', () => {
      if (this.isConnected) {
        this.disconnect();
      } else {
        this.connect();
      }
    });

    this.micBtn.addEventListener('click', () => {
      if (this.client) {
        const newState = !this.client.isMicEnabled;
        this.client.enableMic(newState);
        this.updateMicButton(newState);
      }
    });
  }

  async connect() {
    try {
      this.addEvent('connecting', `Using ${this.transportType} transport`);

      // Create transport using config
      const transport = await createTransport(this.transportType);

      // Create client
      this.client = new PipecatClient({
        transport,
        enableMic: true,
        enableCam: false,
        callbacks: {
          onConnected: () => {
            this.onConnected();
          },
          onDisconnected: () => {
            this.onDisconnected();
          },
          onTransportStateChanged: (state) => {
            this.addEvent('transport-state', state);
          },
          onBotReady: () => {
            this.addEvent('bot-ready', 'Bot is ready to talk');
          },
          onUserTranscript: (data) => {
            if (data.final) {
              this.addConversationMessage(data.text, 'user');
            }
          },
          onBotTranscript: (data) => {
            this.addConversationMessage(data.text, 'bot');
          },
          onError: (error) => {
            this.addEvent('error', error.message);
          },
        },
      });

      // Setup audio
      this.setupAudio();

      // Connect using config
      const connectParams = TRANSPORT_CONFIG[this.transportType];
      await this.client.connect(connectParams);
    } catch (error) {
      this.addEvent('error', error.message);
      console.error('Connection error:', error);
    }
  }

  async disconnect() {
    if (this.client) {
      await this.client.disconnect();
    }
  }

  setupAudio() {
    this.client.on(RTVIEvent.TrackStarted, (track, participant) => {
      if (!participant?.local) {
        if (track.kind === 'audio') {
          this.addEvent('track-started', 'Bot audio track');
          const audio = document.createElement('audio');
          audio.autoplay = true;
          audio.srcObject = new MediaStream([track]);
          document.body.appendChild(audio);
        } else if (track.kind === 'video') {
          this.addEvent('track-started', 'Bot video track');
          this.setupVideoTrack(track);
        }
      }
    });

    this.client.on(RTVIEvent.TrackStopped, (track, participant) => {
      if (!participant?.local && track.kind === 'video') {
        this.addEvent('track-stopped', 'Bot video track');
        this.clearVideoTrack();
      }
    });
  }

  /**
   * Set up a video track for display
   */
  setupVideoTrack(track) {
    // Check if we're already displaying this track
    const existingVideo = this.botVideoContainer.querySelector('video');
    if (existingVideo?.srcObject) {
      const oldTrack = existingVideo.srcObject.getVideoTracks()[0];
      if (oldTrack?.id === track.id) return;
    }

    // Clear placeholder and any existing video
    this.botVideoContainer.innerHTML = '';

    // Create video element
    const videoEl = document.createElement('video');
    videoEl.autoplay = true;
    videoEl.playsInline = true;
    videoEl.muted = true;

    // Create a new MediaStream with the track and set it as the video source
    videoEl.srcObject = new MediaStream([track]);
    this.botVideoContainer.appendChild(videoEl);
  }

  /**
   * Clear the video track and show placeholder
   */
  clearVideoTrack() {
    const video = this.botVideoContainer.querySelector('video');
    if (video?.srcObject) {
      video.srcObject.getTracks().forEach((track) => track.stop());
      video.srcObject = null;
    }
    this.botVideoContainer.innerHTML = `
      <div class="video-placeholder">
        <span>Video will appear here when connected</span>
      </div>
    `;
  }

  onConnected() {
    this.isConnected = true;
    this.connectBtn.textContent = 'Disconnect';
    this.connectBtn.classList.add('disconnect');
    this.micBtn.disabled = false;
    this.transportSelect.disabled = true;
    this.updateMicButton(this.client.isMicEnabled);
    this.addEvent('connected', 'Successfully connected to bot');

    // Clear placeholder
    if (this.conversationLog.querySelector('.placeholder')) {
      this.conversationLog.innerHTML = '';
    }
  }

  onDisconnected() {
    this.isConnected = false;
    this.connectBtn.textContent = 'Connect';
    this.connectBtn.classList.remove('disconnect');
    this.micBtn.disabled = true;
    this.transportSelect.disabled = false;
    this.updateMicButton(false);
    this.clearVideoTrack();
    this.addEvent('disconnected', 'Disconnected from bot');
  }

  updateMicButton(enabled) {
    this.micStatus.textContent = enabled ? 'Mic is On' : 'Mic is Off';
    this.micBtn.style.backgroundColor = enabled ? '#10b981' : '#1f2937';
  }

  addConversationMessage(text, role) {
    const messageDiv = document.createElement('div');
    messageDiv.className = `conversation-message ${role}`;

    if (role === 'placeholder') {
      messageDiv.textContent = text;
    } else {
      const roleSpan = document.createElement('div');
      roleSpan.className = 'role';
      roleSpan.textContent = role === 'user' ? 'You' : 'Bot';

      const textDiv = document.createElement('div');
      textDiv.textContent = text;

      messageDiv.appendChild(roleSpan);
      messageDiv.appendChild(textDiv);
    }

    this.conversationLog.appendChild(messageDiv);
    this.conversationLog.scrollTop = this.conversationLog.scrollHeight;
  }

  addEvent(eventName, data) {
    const eventDiv = document.createElement('div');
    eventDiv.className = 'event-entry';

    const timestamp = new Date().toLocaleTimeString();
    const timestampSpan = document.createElement('span');
    timestampSpan.className = 'timestamp';
    timestampSpan.textContent = timestamp;

    const nameSpan = document.createElement('span');
    nameSpan.className = 'event-name';
    nameSpan.textContent = eventName;

    const dataSpan = document.createElement('span');
    dataSpan.className = 'event-data';
    dataSpan.textContent =
      typeof data === 'string' ? data : JSON.stringify(data);

    eventDiv.appendChild(timestampSpan);
    eventDiv.appendChild(nameSpan);
    eventDiv.appendChild(dataSpan);

    this.eventsLog.appendChild(eventDiv);
    this.eventsLog.scrollTop = this.eventsLog.scrollHeight;
  }
}

// Initialize when DOM is loaded
window.addEventListener('DOMContentLoaded', () => {
  new VoiceChatClient();
});



================================================
FILE: simple-chatbot/client/javascript/src/config.js
================================================
/**
 * Project configuration
 * This file is auto-generated by pipecat-cli
 */

export const AVAILABLE_TRANSPORTS = [
  'daily',
  'smallwebrtc',
];

export const DEFAULT_TRANSPORT = 'daily';

const botStartUrl =
  import.meta.env.VITE_BOT_START_URL || 'http://localhost:7860/start';
const botStartPublicApiKey = import.meta.env.VITE_BOT_START_PUBLIC_API_KEY;

if (!import.meta.env.VITE_BOT_START_URL) {
  console.warn(
    'VITE_BOT_START_URL not configured, using default: http://localhost:7860/start'
  );
}

const dailyConfig = {
  endpoint: botStartUrl,
  requestData: {
    createDailyRoom: true,
    dailyRoomProperties: { start_video_off: true },
    transport: "daily",
  },
};

if (botStartPublicApiKey) {
  dailyConfig.headers = new Headers({
    Authorization: `Bearer ${botStartPublicApiKey}`,
  });
}

const smallWebRTCConfig = {
  endpoint: botStartUrl,
  requestData: {
    createDailyRoom: false,
    enableDefaultIceServers: true,
    transport: "webrtc",
  },
};

if (botStartPublicApiKey) {
  smallWebRTCConfig.headers = new Headers({
    Authorization: `Bearer ${botStartPublicApiKey}`,
  });
}

export const TRANSPORT_CONFIG = {
  daily: dailyConfig,
  smallwebrtc: smallWebRTCConfig,
};

/**
 * Create a transport instance based on the transport type
 * Uses dynamic imports to only load the required transport library
 */
export async function createTransport(transportType) {
  switch (transportType) {

    case 'daily': {
      const { DailyTransport } = await import('@pipecat-ai/daily-transport');
      return new DailyTransport();
    }

    case 'smallwebrtc': {
      const { SmallWebRTCTransport } = await import('@pipecat-ai/small-webrtc-transport');
      return new SmallWebRTCTransport();
    }

    default:
      throw new Error(`Unsupported transport type: ${transportType}`);
  }
}


================================================
FILE: simple-chatbot/client/javascript/src/style.css
================================================
* {
  box-sizing: border-box;
}

body {
  margin: 0;
  padding: 0;
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen',
    'Ubuntu', 'Cantarell', 'Fira Sans', 'Droid Sans', 'Helvetica Neue',
    sans-serif;
  background-color: #000;
  color: #e5e7eb;
  height: 100vh;
  overflow: hidden;
}

.container {
  display: flex;
  flex-direction: column;
  width: 100%;
  height: 100vh;
}

/* Header */
.header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 1rem;
  border-bottom: 1px solid #333;
}

.transport-selector {
  display: flex;
  align-items: center;
  gap: 0.5rem;
}

.transport-selector label {
  font-weight: 500;
}

#transport-select {
  padding: 0.5rem 1rem;
  background-color: #1f2937;
  color: #e5e7eb;
  border: 1px solid #374151;
  border-radius: 0.375rem;
  cursor: pointer;
  font-size: 0.875rem;
}

#transport-select:focus {
  outline: none;
  border-color: #10b981;
}

.controls {
  display: flex;
  gap: 0.5rem;
  align-items: center;
}

.control-btn,
.connect-btn {
  padding: 0.5rem 1rem;
  border: none;
  border-radius: 0.375rem;
  font-size: 0.875rem;
  font-weight: 500;
  cursor: pointer;
  transition: all 0.2s;
}

.control-btn {
  background-color: #1f2937;
  color: #e5e7eb;
  border: 1px solid #374151;
}

.control-btn:hover:not(:disabled) {
  background-color: #374151;
}

.control-btn:disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.connect-btn {
  background-color: #10b981;
  color: #000;
}

.connect-btn:hover {
  background-color: #059669;
}

.connect-btn.disconnect {
  background-color: #ef4444;
  color: #fff;
}

.connect-btn.disconnect:hover {
  background-color: #dc2626;
}

/* Main Content - 50/50 Split */
.main-content {
  flex: 1;
  display: flex;
  overflow: hidden;
}

/* Video Panel (Left) */
.video-panel {
  flex: 1;
  padding: 1rem;
  display: flex;
  flex-direction: column;
  border-right: 1px solid #333;
}

#bot-video-container {
  flex: 1;
  background-color: #111;
  border: 1px solid #333;
  border-radius: 0.5rem;
  overflow: hidden;
  display: flex;
  align-items: center;
  justify-content: center;
}

#bot-video-container video {
  width: 100%;
  height: 100%;
  object-fit: cover;
}

.video-placeholder {
  color: #6b7280;
  font-style: italic;
  text-align: center;
  padding: 2rem;
}

/* Conversation Panel (Right) */
.conversation-panel {
  flex: 1;
  padding: 1rem;
  overflow: hidden;
  display: flex;
  flex-direction: column;
}

.conversation-panel h3 {
  margin: 0 0 1rem 0;
  font-size: 1rem;
  font-weight: 600;
}

#conversation-log {
  flex: 1;
  overflow-y: auto;
  background-color: #111;
  border: 1px solid #333;
  border-radius: 0.5rem;
  padding: 1rem;
  font-size: 0.875rem;
  line-height: 1.5;
}

.conversation-message {
  margin-bottom: 0.75rem;
  padding: 0.5rem;
  border-radius: 0.25rem;
}

.conversation-message.user {
  background-color: #1e40af;
  color: #e0e7ff;
}

.conversation-message.bot {
  background-color: #065f46;
  color: #d1fae5;
}

.conversation-message .role {
  font-weight: 600;
  margin-bottom: 0.25rem;
}

.conversation-message.placeholder {
  color: #6b7280;
  font-style: italic;
  background-color: transparent;
}

/* Events Panel */
.events-panel {
  height: 256px;
  padding: 0 1rem 1rem 1rem;
  border-top: 1px solid #333;
}

.events-panel h3 {
  margin: 1rem 0;
  font-size: 1rem;
  font-weight: 600;
}

#events-log {
  height: calc(100% - 3rem);
  overflow-y: auto;
  background-color: #111;
  border: 1px solid #333;
  border-radius: 0.5rem;
  padding: 0.75rem;
  font-family: 'Courier New', monospace;
  font-size: 0.75rem;
  line-height: 1.4;
}

.event-entry {
  margin-bottom: 0.5rem;
  display: flex;
  gap: 0.5rem;
}

.event-entry .timestamp {
  color: #6b7280;
  flex-shrink: 0;
}

.event-entry .event-name {
  color: #10b981;
  font-weight: 600;
  flex-shrink: 0;
}

.event-entry .event-data {
  color: #9ca3af;
}

/* Scrollbar styling */
#conversation-log::-webkit-scrollbar,
#events-log::-webkit-scrollbar {
  width: 8px;
}

#conversation-log::-webkit-scrollbar-track,
#events-log::-webkit-scrollbar-track {
  background: #1f2937;
  border-radius: 4px;
}

#conversation-log::-webkit-scrollbar-thumb,
#events-log::-webkit-scrollbar-thumb {
  background: #374151;
  border-radius: 4px;
}

#conversation-log::-webkit-scrollbar-thumb:hover,
#events-log::-webkit-scrollbar-thumb:hover {
  background: #4b5563;
}



================================================
FILE: simple-chatbot/client/prebuilt/README.md
================================================
# Daily Prebuilt Connection

The simplest way to connect to the chatbot using Daily's Prebuilt UI.

1. Start either the OpenAI or Gemini Live bot

   OpenAI:

   ```bash
   python server/bot-openai.py --transport daily
   ```

   Gemini Live:

   ```bash
   python server/bot-gemini.py --transport daily
   ```

2. Visit http://localhost:7860

3. Allow microphone access when prompted

4. Start talking with the bot



================================================
FILE: simple-chatbot/client/react/README.md
================================================
# React Implementation

Basic implementation using the [Pipecat React SDK](https://docs.pipecat.ai/client/react/introduction).

## Setup

1. Run the server-side bot; see [README](../../server/README).

2. Navigate to the `client/react` directory:

```bash
cd client/react
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: simple-chatbot/client/react/env.example
================================================
# For local development:
BOT_START_URL="http://localhost:7860/start"

# For Pipecat Cloud (replace {agentName} with your agent name):
# BOT_START_URL="https://api.pipecat.daily.co/v1/public/{agentName}/start"
# BOT_START_PUBLIC_API_KEY="your-pipecat-cloud-public-api-key-here"


================================================
FILE: simple-chatbot/client/react/eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
  {
    ignores: [
      "node_modules/**",
      ".next/**",
      "out/**",
      "build/**",
      "next-env.d.ts",
    ],
  },
];

export default eslintConfig;



================================================
FILE: simple-chatbot/client/react/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;



================================================
FILE: simple-chatbot/client/react/package.json
================================================
{
  "name": "test-react-nextjs",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev --turbopack",
    "build": "next build --turbopack",
    "start": "next start",
    "lint": "eslint"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.5.0",
    "@pipecat-ai/client-react": "^1.1.0",
    "@pipecat-ai/daily-transport": "^1.5.0",
    "@pipecat-ai/small-webrtc-transport": "^1.8.0",
    "@pipecat-ai/voice-ui-kit": "^0.6.0",
    "next": "15.5.5",
    "react": "19.1.0",
    "react-dom": "19.1.0"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@tailwindcss/postcss": "^4",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.5.5",
    "tailwindcss": "^4",
    "typescript": "^5"
  }
}


================================================
FILE: simple-chatbot/client/react/postcss.config.mjs
================================================
const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;



================================================
FILE: simple-chatbot/client/react/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": ["dom", "dom.iterable", "esnext"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": ["./src/*"]
    }
  },
  "include": ["next-env.d.ts", "**/*.ts", "**/*.tsx", ".next/types/**/*.ts"],
  "exclude": ["node_modules"]
}



================================================
FILE: simple-chatbot/client/react/.prettierrc
================================================
{
  "semi": true,
  "tabWidth": 2,
  "useTabs": false,
  "singleQuote": false
}



================================================
FILE: simple-chatbot/client/react/src/config.ts
================================================
/**
 * Project configuration
 * This file is auto-generated by pipecat-cli
 */

import type { APIRequest } from "@pipecat-ai/client-js";

export type TransportType = "daily" | "smallwebrtc";

export const AVAILABLE_TRANSPORTS: TransportType[] = ["daily", "smallwebrtc"];

export const DEFAULT_TRANSPORT: TransportType = "daily";

export const TRANSPORT_LABELS: Record<TransportType, string> = {
  daily: "Daily",
  smallwebrtc: "SmallWebRTC",
};

export const TRANSPORT_CONFIG: Record<TransportType, APIRequest> = {
  daily: {
    endpoint: "/api/start",
    requestData: {
      createDailyRoom: true,
      dailyRoomProperties: { start_video_off: true },
      transport: "daily",
    },
  },
  smallwebrtc: {
    endpoint: "/api/start",
    requestData: {
      createDailyRoom: false,
      enableDefaultIceServers: true,
      transport: "webrtc",
    },
  },
};



================================================
FILE: simple-chatbot/client/react/src/app/globals.css
================================================
@import "@pipecat-ai/voice-ui-kit/styles";



================================================
FILE: simple-chatbot/client/react/src/app/layout.tsx
================================================
import type { Metadata } from "next";

import "./globals.css";

export const metadata: Metadata = {
  title: "Voice UI Kit - Simple Chatbot",
  icons: { icon: "/pipecat.svg" },
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body>{children}</body>
    </html>
  );
}



================================================
FILE: simple-chatbot/client/react/src/app/page.tsx
================================================
"use client";

import { useState } from "react";

import { ThemeProvider } from "@pipecat-ai/voice-ui-kit";

import type { PipecatBaseChildProps } from "@pipecat-ai/voice-ui-kit";
import {
  ErrorCard,
  FullScreenContainer,
  PipecatAppBase,
  SpinLoader,
} from "@pipecat-ai/voice-ui-kit";

import { App } from "./components/App";
import {
  AVAILABLE_TRANSPORTS,
  DEFAULT_TRANSPORT,
  TRANSPORT_CONFIG,
} from "../config";
import type { TransportType } from "../config";

export default function Home() {
  const [transportType, setTransportType] =
    useState<TransportType>(DEFAULT_TRANSPORT);

  const connectParams = TRANSPORT_CONFIG[transportType];

  return (
    <ThemeProvider defaultTheme="terminal" disableStorage>
      <FullScreenContainer>
        <PipecatAppBase
          connectParams={connectParams}
          transportType={transportType}
        >
          {({
            client,
            handleConnect,
            handleDisconnect,
            error,
          }: PipecatBaseChildProps) =>
            !client ? (
              <SpinLoader />
            ) : error ? (
              <ErrorCard>{error}</ErrorCard>
            ) : (
              <App
                client={client}
                handleConnect={handleConnect}
                handleDisconnect={handleDisconnect}
                transportType={transportType}
                onTransportChange={setTransportType}
                availableTransports={AVAILABLE_TRANSPORTS}
              />
            )
          }
        </PipecatAppBase>
      </FullScreenContainer>
    </ThemeProvider>
  );
}



================================================
FILE: simple-chatbot/client/react/src/app/api/sessions/[sessionId]/[...path]/route.ts
================================================
import { NextResponse } from "next/server";

async function handleRequest(
  request: Request,
  { params }: { params: Promise<{ sessionId: string; path: string[] }> }
) {
  const botBaseUrl =
    process.env.BOT_START_URL?.replace("/start", "") || "http://localhost:7860";
  const { sessionId, path } = await params;
  const pathString = path.join("/");
  const targetUrl = `${botBaseUrl}/sessions/${sessionId}/${pathString}`;

  try {
    const body = await request.text();
    const headers: Record<string, string> = {
      "Content-Type": "application/json",
    };

    if (process.env.BOT_START_PUBLIC_API_KEY) {
      headers.Authorization = `Bearer ${process.env.BOT_START_PUBLIC_API_KEY}`;
    }

    const response = await fetch(targetUrl, {
      method: request.method,
      headers,
      body,
    });

    if (!response.ok) {
      throw new Error(`Failed to proxy request: ${response.statusText}`);
    }

    const data = await response.json();

    if (data.error) {
      throw new Error(data.error);
    }

    return NextResponse.json(data);
  } catch (error) {
    return NextResponse.json(
      { error: `Failed to proxy session request: ${error}` },
      { status: 500 }
    );
  }
}

export async function POST(
  request: Request,
  context: { params: Promise<{ sessionId: string; path: string[] }> }
) {
  return handleRequest(request, context);
}

export async function PATCH(
  request: Request,
  context: { params: Promise<{ sessionId: string; path: string[] }> }
) {
  return handleRequest(request, context);
}



================================================
FILE: simple-chatbot/client/react/src/app/api/start/route.ts
================================================
import { NextResponse } from "next/server";

export async function POST(request: Request) {
  const botStartUrl =
    process.env.BOT_START_URL || "http://localhost:7860/start";

  if (!process.env.BOT_START_URL) {
    console.warn(
      "BOT_START_URL not configured, using default: http://localhost:7860/start"
    );
  }

  try {
    // Parse the request body from the client
    const requestData = await request.json();

    const headers: Record<string, string> = {
      "Content-Type": "application/json",
    };

    if (process.env.BOT_START_PUBLIC_API_KEY) {
      headers.Authorization = `Bearer ${process.env.BOT_START_PUBLIC_API_KEY}`;
    }

    // Pass through the request data from the client
    const response = await fetch(botStartUrl, {
      method: "POST",
      headers,
      body: JSON.stringify(requestData),
    });

    if (!response.ok) {
      throw new Error(`Failed to connect to Pipecat: ${response.statusText}`);
    }

    const data = await response.json();

    if (data.error) {
      throw new Error(data.error);
    }

    return NextResponse.json(data);
  } catch (error) {
    return NextResponse.json(
      { error: `Failed to process connection request: ${error}` },
      { status: 500 }
    );
  }
}



================================================
FILE: simple-chatbot/client/react/src/app/components/App.tsx
================================================
import { useEffect } from "react";

import type { PipecatBaseChildProps } from "@pipecat-ai/voice-ui-kit";
import {
  BotVideoPanel,
  ConnectButton,
  ConversationPanel,
  EventsPanel,
  UserAudioControl,
} from "@pipecat-ai/voice-ui-kit";

import type { TransportType } from "../../config";
import { TransportSelect } from "./TransportSelect";

interface AppProps extends PipecatBaseChildProps {
  transportType: TransportType;
  onTransportChange: (type: TransportType) => void;
  availableTransports: TransportType[];
}

export const App = ({
  client,
  handleConnect,
  handleDisconnect,
  transportType,
  onTransportChange,
  availableTransports,
}: AppProps) => {
  useEffect(() => {
    client?.initDevices();
  }, [client]);

  const showTransportSelector = availableTransports.length > 1;

  return (
    <div className="flex flex-col w-full h-full">
      <div className="flex items-center justify-between gap-4 p-4">
        {showTransportSelector ? (
          <TransportSelect
            transportType={transportType}
            onTransportChange={onTransportChange}
            availableTransports={availableTransports}
          />
        ) : (
          <div /> /* Spacer */
        )}
        <div className="flex items-center gap-4">
          <UserAudioControl size="lg" />
          <ConnectButton
            size="lg"
            onConnect={handleConnect}
            onDisconnect={handleDisconnect}
          />
        </div>
      </div>
      <div className="flex-1 overflow-hidden px-4 flex gap-4">
        <BotVideoPanel className="flex-1 h-full" />
        <div className="flex-1 overflow-hidden">
          <ConversationPanel />
        </div>
      </div>
      <div className="h-96 overflow-hidden px-4 pb-4">
        <EventsPanel />
      </div>
    </div>
  );
};



================================================
FILE: simple-chatbot/client/react/src/app/components/TransportSelect.tsx
================================================
import {
  Select,
  SelectTrigger,
  SelectValue,
  SelectContent,
  SelectItem,
  SelectGuide,
} from "@pipecat-ai/voice-ui-kit";

import { TRANSPORT_LABELS, type TransportType } from "../../config";

interface TransportSelectProps {
  transportType: TransportType;
  onTransportChange: (type: TransportType) => void;
  availableTransports: TransportType[];
}

export const TransportSelect = ({
  transportType,
  onTransportChange,
  availableTransports,
}: TransportSelectProps) => {
  return (
    <Select value={transportType} onValueChange={onTransportChange}>
      <SelectTrigger size="lg">
        <SelectGuide>Transport</SelectGuide>
        <SelectValue />
      </SelectTrigger>
      <SelectContent>
        {availableTransports.map((transport) => (
          <SelectItem key={transport} value={transport}>
            {TRANSPORT_LABELS[transport]}
          </SelectItem>
        ))}
      </SelectContent>
    </Select>
  );
};



================================================
FILE: simple-chatbot/client/react-native/README.md
================================================
# React Native implementation

Basic implementation using the [Pipecat RN SDK](https://docs.pipecat.ai/client/react-native/introduction).

## Usage

### Expo requirements

This project cannot be used with an [Expo Go](https://docs.expo.dev/workflow/expo-go/) app because [it requires custom native code](https://docs.expo.io/workflow/customizing/).

When a project requires custom native code or a config plugin, we need to transition from using [Expo Go](https://docs.expo.dev/workflow/expo-go/)
to a [development build](https://docs.expo.dev/development/introduction/).

More details about the custom native code used by this demo can be found in [rn-daily-js-expo-config-plugin](https://github.com/daily-co/rn-daily-js-expo-config-plugin).

### Building remotely

If you do not have experience with Xcode and Android Studio builds or do not have them installed locally on your computer, you will need to follow [this guide from Expo to use EAS Build](https://docs.expo.dev/development/create-development-builds/#create-and-install-eas-build).

### Building locally

You will need to have installed locally on your computer:
- [Xcode](https://developer.apple.com/xcode/) to build for iOS;
- [Android Studio](https://developer.android.com/studio) to build for Android;

#### Install the demo dependencies

```bash
# Use the version of node specified in .nvmrc
nvm i

# Install dependencies
yarn install

# Before a native app can be compiled, the native source code must be generated.
npx expo prebuild
```

#### Running on Android

After plugging in an Android device [configured for debugging](https://developer.android.com/studio/debug/dev-options), run the following command:

```
npm run android
```

#### Running on iOS

First, you'll need to do a one-time setup. This is required to build to a physical device.

If you're familiar with Xcode, open `ios/RNSimpleChatbot.xcworkspace` and, in the target settings, provide a development team registered with Apple.

If you're newer to Xcode, here are some more detailed instructions to get you started.

First, open the project in Xcode. Make sure to specifically select `RNSimpleChatbot.xcworkspace` from `/ios`. The `/ios` directory will have been generated by running `npx expo prebuild` as instructed above. This is also a good time to plug in your iOS device to be sure the following steps are successful.

From the main menu, select `Settings` and then `Accounts`. Click the `+` sign to add an account (e.g. an Apple ID).

![xcode-accounts.png](./docsAssets/xcode-accounts.png)

Once an account is added, perform the following steps:

 1. Close `Settings`.
 1. Select the folder icon in the top left corner.
 1. Select `RNSimpleChatbot` from the side panel
 1. Navigate to `Signing & Capabilities` in the top nav bar.
 1. Open the "Team" dropdown
 1. Select the account added in the previous step.

The "Signing Certificate" section should update accordingly with your account information.

![xcode-signing.png](./docsAssets/xcode-signing.png)

**Troubleshooting common errors:**

- If you see the error `Change your bundle identifier to a unique string to try again`, update the "Bundle Identifier" input in `Signing & Capabilities` to make it unique. This should resolve the error.

- If you see an error that says `Xcode was unable to launch because it has an invalid code signature, inadequate entitlements or its profile has not been explicitly trusted by the user`, you may need to update the settings on your iPhone to enable the required permissions as follows:

1. Open `Settings` on your iPhone
1. Select `General`, then `Device Management`
1. Click `Trust` for DailyPlayground

- You may also be prompted to enter you login keychain password. Be sure to click `Always trust` to avoid the prompt showing multiple times.

After, run the following command:
```
npm run ios
```



================================================
FILE: simple-chatbot/client/react-native/App.js
================================================
// Disabling the logs from react-native-webrtc
import debug from 'debug';
debug.disable('rn-webrtc:*');

// Ignoring the warnings from react-native-background-timer while they don't fix this issue:
// https://github.com/ocetnik/react-native-background-timer/issues/366
import { LogBox } from 'react-native';
LogBox.ignoreLogs([
  "`new NativeEventEmitter()` was called with a non-null argument without the required `addListener` method.",
  "`new NativeEventEmitter()` was called with a non-null argument without the required `removeListeners` method."
]);

// Enable debug logs
/*window.localStorage = window.localStorage || {};
window.localStorage.debug = '*';
window.localStorage.getItem = (itemName) => {
  console.log('Requesting the localStorage item ', itemName);
  return window.localStorage[itemName];
};*/

export { default } from './src/App';



================================================
FILE: simple-chatbot/client/react-native/app.json
================================================
{
  "expo": {
    "name": "RN Simple Chatbot",
    "slug": "simple-chatbot-demo",
    "newArchEnabled": true,
    "version": "1.0.0",
    "orientation": "portrait",
    "icon": "./assets/images/pipecat.png",
    "userInterfaceStyle": "light",
    "splash": {
      "image": "./assets/images/splash.png",
      "resizeMode": "contain",
      "backgroundColor": "#ffffff"
    },
    "updates": {
      "fallbackToCacheTimeout": 0
    },
    "assetBundlePatterns": [
      "**/*"
    ],
    "ios": {
      "supportsTablet": true,
      "bitcode": false,
      "bundleIdentifier": "co.daily.SimpleChatbot",
      "infoPlist": {
        "UIBackgroundModes": [
          "voip"
        ]
      }
    },
    "android": {
      "adaptiveIcon": {
        "foregroundImage": "./assets/images/pipecat.png",
        "backgroundColor": "#FFFFFF"
      },
      "package": "co.daily.SimpleChatbot",
      "permissions": [
        "android.permission.ACCESS_NETWORK_STATE",
        "android.permission.BLUETOOTH",
        "android.permission.CAMERA",
        "android.permission.INTERNET",
        "android.permission.MODIFY_AUDIO_SETTINGS",
        "android.permission.RECORD_AUDIO",
        "android.permission.SYSTEM_ALERT_WINDOW",
        "android.permission.WAKE_LOCK",
        "android.permission.FOREGROUND_SERVICE",
        "android.permission.FOREGROUND_SERVICE_CAMERA",
        "android.permission.FOREGROUND_SERVICE_MICROPHONE",
        "android.permission.FOREGROUND_SERVICE_MEDIA_PROJECTION",
        "android.permission.POST_NOTIFICATIONS"
      ]
    },
    "web": {
      "favicon": "./assets/images/pipecat.png"
    },
    "plugins": [
      "@daily-co/config-plugin-rn-daily-js",
      [
        "expo-build-properties",
        {
          "android": {
            "minSdkVersion": 24
          },
          "ios": {
            "deploymentTarget": "15.1"
          }
        }
      ]
    ]
  }
}



================================================
FILE: simple-chatbot/client/react-native/babel.config.js
================================================
module.exports = function(api) {
  api.cache(true);
  return {
    presets: ['babel-preset-expo'],
  };
};



================================================
FILE: simple-chatbot/client/react-native/env.example
================================================
EXPO_PUBLIC_SIMPLE_CHATBOT_SERVER=http://$YOUR_IP:7860



================================================
FILE: simple-chatbot/client/react-native/metro.config.js
================================================
const { getDefaultConfig } = require('expo/metro-config');
module.exports = getDefaultConfig(__dirname);



================================================
FILE: simple-chatbot/client/react-native/package.json
================================================
{
  "name": "simple-chatbot-demo",
  "version": "1.0.0",
  "scripts": {
    "start": "expo start --dev-client",
    "android": "expo run:android --device",
    "ios": "expo run:ios --device",
    "web": "expo start --web",
    "update": "(cd ../rtvi-client-react-native-daily; yarn build); cp -R ../rtvi-client-react-native-daily/lib/* ./node_modules/react-native-realtime-ai-daily/lib/;"
  },
  "dependencies": {
    "@daily-co/config-plugin-rn-daily-js": "0.0.11",
    "@daily-co/react-native-daily-js": "^0.82.0",
    "@daily-co/react-native-webrtc": "^124.0.6-daily.1",
    "@react-native-async-storage/async-storage": "1.24.0",
    "@react-navigation/native": "^7.1.19",
    "@react-navigation/stack": "^7.6.1",
    "expo": "~54.0.21",
    "expo-asset": "~12.0.9",
    "expo-status-bar": "~3.0.8",
    "react": "19.1.0",
    "react-native": "^0.81.5",
    "react-native-background-timer": "^2.4.1",
    "react-native-gesture-handler": "^2.29.0",
    "react-native-get-random-values": "^1.11.0",
    "@pipecat-ai/react-native-daily-transport": "^1.4.0",
    "react-native-safe-area-context": "^5.6.2",
    "react-native-toast-message": "^2.3.3"
  },
  "devDependencies": {
    "@babel/core": "^7.28.5",
    "@types/react-native": "^0.73.0",
    "typescript": "~5.9.3",
    "@types/react": "19.1.0"
  },
  "private": true,
  "resolutions": {
    "@daily-co/react-native-webrtc/debug": "^4.0.0",
    "@daily-co/react-native-webrtc/@types/react-native": "^0.73.0"
  }
}



================================================
FILE: simple-chatbot/client/react-native/tsconfig.json
================================================
{
  "compilerOptions": {
    "rootDir": ".",
    "allowUnreachableCode": false,
    "allowUnusedLabels": false,
    "esModuleInterop": true,
    "forceConsistentCasingInFileNames": true,
    "jsx": "react-jsx",
    "lib": [
      "ESNext"
    ],
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "noEmit": true,
    "noFallthroughCasesInSwitch": true,
    "noImplicitReturns": true,
    "noImplicitUseStrict": false,
    "noStrictGenericChecks": false,
    "noUncheckedIndexedAccess": true,
    "noUnusedLocals": false,
    "noUnusedParameters": true,
    "resolveJsonModule": true,
    "skipLibCheck": true,
    "strict": true,
    "target": "ESNext",
    "verbatimModuleSyntax": false
  },
  "extends": "expo/tsconfig.base"
}



================================================
FILE: simple-chatbot/client/react-native/.jshintrc
================================================
{
  "esversion": 6
}


================================================
FILE: simple-chatbot/client/react-native/.nvmrc
================================================
22.14



================================================
FILE: simple-chatbot/client/react-native/src/App.tsx
================================================
import React from "react"

import { NavigationContainer } from '@react-navigation/native';
import { createStackNavigator } from '@react-navigation/stack';
import PreJoinView from './views/PreJoinView';
import MeetingView from './views/MeetingView';
import { VoiceClientProvider } from './context/VoiceClientContext';
import Toast from 'react-native-toast-message';

import { useVoiceClientNavigation } from './hooks/useVoiceClientNavigation';

const Stack = createStackNavigator();

const NavigationManager: React.FC = () => {
  useVoiceClientNavigation();  // This hook now controls the navigation based on the connection state.
  return null; // This component doesn't render anything but manages navigation.
};

const App: React.FC = () => {
  return (
    <VoiceClientProvider>
      <NavigationContainer>
        <Stack.Navigator initialRouteName="Prejoin">
          <Stack.Screen name="Prejoin" component={PreJoinView} options={{ headerShown: false }}/>
          <Stack.Screen name="Meeting" component={MeetingView} options={{ headerShown: false }}/>
        </Stack.Navigator>
        <NavigationManager />
        <Toast />
      </NavigationContainer>
    </VoiceClientProvider>
  );
};

export default App;



================================================
FILE: simple-chatbot/client/react-native/src/components/MicrophoneView.tsx
================================================
import React, { useState, useMemo } from 'react';
import { View, StyleSheet, LayoutChangeEvent, ViewStyle } from 'react-native';
import { MaterialIcons } from '@expo/vector-icons';
import Colors from '../theme/Colors';
import { useVoiceClient } from '../context/VoiceClientContext';

interface MicrophoneViewProps {
  style?: ViewStyle;
}

const MicrophoneView: React.FC<MicrophoneViewProps> = ({ style }) => {
  const { isMicEnabled, localAudioLevel: audioLevel } = useVoiceClient();
  const [dimensions, setDimensions] = useState({ width: 0, height: 0 });

  const onLayout = (event: LayoutChangeEvent) => {
    const { width, height } = event.nativeEvent.layout;
    setDimensions({ width, height });
  };

  const { width } = dimensions;

  const circleSize = useMemo(() => width * 0.9, [width]);
  const innerCircleSize = useMemo(() => width * 0.82, [width]);
  const audioCircleSize = useMemo(() => audioLevel * width * 0.95, [audioLevel, width]);
  const iconSize = useMemo(() => width * 0.2, [width]);

  return (
    <View style={[styles.container, style]} onLayout={onLayout}>
      {width > 0 && (
        <View
          style={[
            styles.outerCircle,
            { width: circleSize, height: circleSize, borderRadius: circleSize / 2 },
          ]}
        >
          <View
            style={[
              styles.innerCircle,
              {
                backgroundColor: !isMicEnabled ? Colors.disabledMic : Colors.backgroundCircle,
                width: innerCircleSize,
                height: innerCircleSize,
                borderRadius: innerCircleSize / 2,
              },
            ]}
          />

          {isMicEnabled && audioCircleSize > 0 && (
            <View
              style={[
                styles.audioCircle,
                {
                  width: audioCircleSize,
                  height: audioCircleSize,
                  borderRadius: audioCircleSize / 2,
                },
              ]}
            />
          )}

          <MaterialIcons
            name={!isMicEnabled ? "mic-off" : "mic"}
            size={iconSize > 0 ? iconSize : 1}
            color="white"
            style={styles.micIcon}
          />
        </View>
      )}
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  outerCircle: {
    borderWidth: 1,
    borderColor: Colors.buttonsBorder,
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  innerCircle: {
    position: 'absolute',
  } as ViewStyle,
  audioCircle: {
    position: 'absolute',
    backgroundColor: Colors.micVolume,
    opacity: 0.5,
  } as ViewStyle,
  micIcon: {
    position: 'absolute',
  },
});

export default MicrophoneView;


================================================
FILE: simple-chatbot/client/react-native/src/components/WaveformView.tsx
================================================
import React, { useEffect, useState } from 'react';
import { LayoutChangeEvent, StyleSheet, Text, View, ViewStyle } from 'react-native';
import { MaterialIcons } from '@expo/vector-icons';
import Colors from '../theme/Colors';
import { useVoiceClient } from '../context/VoiceClientContext';

const dotCount = 5;

const WaveformView: React.FC = () => {
  const [audioLevels, setAudioLevels] = useState(Array(dotCount).fill(0));
  const [dimensions, setDimensions] = useState({ width: 0, height: 0 });

  const { currentState: voiceClientStatus, botReady: isBotReady, remoteAudioLevel: audioLevel } = useVoiceClient();

  const onLayout = (event: LayoutChangeEvent) => {
    const { width, height } = event.nativeEvent.layout;
    setDimensions({ width, height });
  };

  useEffect(() => {
    setAudioLevels((prevLevels) => [...prevLevels.slice(1), audioLevel]);
  }, [audioLevel]);

  const { width, height } = dimensions;
  const circleSize = width * 0.9;
  const innerCircleSize = width * 0.82;
  const barWidth = (width * 0.5) / dotCount;

  return (
    <View style={styles.container} onLayout={onLayout}>
      <View style={[styles.outerCircle, { width: circleSize, height: circleSize, borderRadius: circleSize / 2 }]}>
        <View
          style={[
            styles.innerCircle,
            {
              backgroundColor: isBotReady ? Colors.backgroundCircle : Colors.backgroundCircleNotConnected,
              width: innerCircleSize,
              height: innerCircleSize,
              borderRadius: innerCircleSize / 2,
            },
          ]}
        >
          {isBotReady ? (
            audioLevel > 0 ? (
              <View style={[styles.waveformContainer, { width: width * 0.5, height: width * 0.5 }]}>
                {audioLevels.map((level, index) => (
                  <View
                    key={index}
                    style={[
                      styles.waveformBar,
                      {
                        width: barWidth - 10, // Subtract some margin
                        height: level * height,
                      },
                    ]}
                  />
                ))}
              </View>
            ) : (
              <View style={[styles.dotContainer, { width: width * 0.5, height: height * 0.5 }]}>
                {Array(dotCount)
                  .fill(0)
                  .map((_, index) => (
                    <View key={index} style={[styles.dot, { width: height * 0.1, height: height * 0.1 }]} />
                  ))}
              </View>
            )
          ) : (
            <View style={styles.notReadyContainer}>
              <MaterialIcons name="hourglass-empty" size={32} color="white" />
              <Text style={styles.voiceClientStatusText}>{voiceClientStatus}</Text>
            </View>
          )}
        </View>
      </View>
    </View>
  );
};

const styles = StyleSheet.create({
  container: {
    justifyContent: 'center',
    alignItems: 'center',
    width: "100%",
  } as ViewStyle,
  outerCircle: {
    borderWidth: 1,
    borderColor: 'gray',
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  innerCircle: {
    justifyContent: 'center',
    alignItems: 'center',
    position: 'relative',
  } as ViewStyle,
  waveformContainer: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
  } as ViewStyle,
  waveformBar: {
    backgroundColor: 'white',
    maxHeight: '100%',
    borderRadius: 12,
  } as ViewStyle,
  dotContainer: {
    flexDirection: 'row',
    justifyContent: 'space-between',
    alignItems: 'center',
  } as ViewStyle,
  dot: {
    backgroundColor: 'white',
    borderRadius: 50,
  } as ViewStyle,
  notReadyContainer: {
    justifyContent: 'center',
    alignItems: 'center',
  } as ViewStyle,
  voiceClientStatusText: {
    color: 'white',
    marginTop: 10,
    fontSize: 16,
    fontWeight: 'bold',
  } as ViewStyle,
});

export default WaveformView;



================================================
FILE: simple-chatbot/client/react-native/src/context/VoiceClientContext.tsx
================================================
import React, { createContext, useState, useContext, ReactNode, useCallback, useMemo, useRef, useEffect } from 'react'
import Toast from 'react-native-toast-message'
import { RNDailyTransport } from '@pipecat-ai/react-native-daily-transport'
import { PipecatClient, TransportState, Participant } from '@pipecat-ai/client-js'
import { MediaStreamTrack } from '@daily-co/react-native-webrtc'
import { SettingsManager } from '../settings/SettingsManager';

interface VoiceClientContextProps {
  voiceClient: PipecatClient | null
  inCall: boolean
  currentState: string
  botReady: boolean
  localAudioLevel: number
  remoteAudioLevel: number
  isMicEnabled: boolean
  isCamEnabled: boolean
  videoTrack?: MediaStreamTrack
  // methods
  start: (url: string) => Promise<void>
  leave: () => void
  toggleMicInput: () => void
  toggleCamInput: () => void
}

export const VoiceClientContext = createContext<VoiceClientContextProps | undefined>(undefined)

interface VoiceClientProviderProps {
  children: ReactNode
}

export const VoiceClientProvider: React.FC<VoiceClientProviderProps> = ({ children }) => {

  const [voiceClient, setVoiceClient] = useState<PipecatClient | null>(null)
  const [inCall, setInCall] = useState<boolean>(false)
  const [currentState, setCurrentState] = useState<TransportState>("disconnected")
  const [botReady, setBotReady] = useState<boolean>(false)
  const [isMicEnabled, setIsMicEnabled] = useState<boolean>(false)
  const [isCamEnabled, setIsCamEnabled] = useState<boolean>(false)
  const [videoTrack, setVideoTrack] = useState<MediaStreamTrack>()
  const [localAudioLevel, setLocalAudioLevel] = useState<number>(0)
  const [remoteAudioLevel, setRemoteAudioLevel] = useState<number>(0)
  const botSpeakingRef = useRef(false)

  const handleError = useCallback((error: any) => {
    console.log("Error occurred:", error)
    const errorMessage = error.message || error.data?.error || "An unexpected error occurred"
    Toast.show({
      type: 'error',
      text1: errorMessage,
    })
  }, [])

  const createVoiceClient = useCallback((): PipecatClient => {
    const inCallStates = new Set(["authenticating", "authenticated", "connecting", "connected", "ready"])
    const client = new PipecatClient({
      transport: new RNDailyTransport(),
      enableMic: true,
      enableCam: false,
      callbacks: {
        onTransportStateChanged: (state) => {
          setCurrentState(state)
          setInCall(inCallStates.has(state))
        },
        onError: (error) => {
          handleError(error)
        },
        onBotReady: () => {
          setBotReady(true)
        },
        onDisconnected: () => {
          setBotReady(false)
          setIsMicEnabled(false)
          setIsCamEnabled(false)
        },
        onLocalAudioLevel: (level: number) => {
          setLocalAudioLevel(level)
        },
        onRemoteAudioLevel: (level: number) => {
          if (botSpeakingRef.current) {
            setRemoteAudioLevel(level)
          }
        },
        onBotStartedSpeaking: () => {
          botSpeakingRef.current = true
        },
        onBotStoppedSpeaking: () => {
          botSpeakingRef.current = false
          setRemoteAudioLevel(0)
        },
        onConnected: () => {
          setIsMicEnabled(client.isMicEnabled)
          setIsCamEnabled(client.isCamEnabled)
        },
        onTrackStarted: (track: MediaStreamTrack, p?: Participant) => {
          if (p?.local && track.kind === 'video'){
            setVideoTrack(track)
          }
        }
      },
    })
    return client
  }, [handleError])

  const start = useCallback(async (url: string): Promise<void> => {
    const client = createVoiceClient()
    setVoiceClient(client)
    try {
      await client?.initDevices()
      await client?.startBotAndConnect({
        endpoint: url + '/start',
      });
      // updating the preferences
      const newSettings = await SettingsManager.getSettings();
      newSettings.backendURL = url
      await SettingsManager.updateSettings(newSettings)
    } catch (error) {
      handleError(error)
    }
  }, [createVoiceClient, handleError])

  const leave = useCallback(async (): Promise<void> => {
    if (voiceClient) {
      await voiceClient.disconnect()
      setVoiceClient(null)
    }
  }, [voiceClient])

  const toggleMicInput = useCallback(async (): Promise<void> => {
    if (voiceClient) {
      try {
        let enableMic = !isMicEnabled
        voiceClient.enableMic(enableMic)
        setIsMicEnabled(enableMic)
      } catch (e) {
        handleError(e)
      }
    }
  }, [voiceClient, isMicEnabled])

  const toggleCamInput = useCallback(async (): Promise<void> => {
    if (voiceClient) {
      try {
        let enableCam = !isCamEnabled
        voiceClient.enableCam(enableCam)
        setIsCamEnabled(enableCam)
      } catch (e) {
        handleError(e)
      }
    }
  }, [voiceClient, isCamEnabled])

  useEffect(() => {
    return () => {
      if (voiceClient) {
        voiceClient.removeAllListeners() // Cleanup on unmount
      }
    }
  }, [voiceClient])

  const contextValue = useMemo(() => ({
    voiceClient,
    inCall,
    currentState,
    botReady,
    isMicEnabled,
    isCamEnabled,
    localAudioLevel,
    remoteAudioLevel,
    videoTrack,
    start,
    leave,
    toggleMicInput,
    toggleCamInput
  }), [voiceClient, inCall, currentState, botReady, isMicEnabled, isCamEnabled, localAudioLevel, remoteAudioLevel, videoTrack, start, leave, toggleMicInput, toggleCamInput])

  return (
    <VoiceClientContext.Provider value={contextValue}>
      {children}
    </VoiceClientContext.Provider>
  )
}

export const useVoiceClient = (): VoiceClientContextProps => {
  const context = useContext(VoiceClientContext)
  if (!context) {
    throw new Error('useVoiceClient must be used within a VoiceClientProvider')
  }
  return context
}



================================================
FILE: simple-chatbot/client/react-native/src/hooks/useVoiceClientNavigation.ts
================================================
import { useEffect } from 'react';
import { useNavigation, NavigationProp } from '@react-navigation/native';
import { useVoiceClient } from '../context/VoiceClientContext';

export type RootStackParamList = {
  Meeting: undefined;
  Prejoin: undefined;
};

export const useVoiceClientNavigation = () => {
  const navigation = useNavigation<NavigationProp<RootStackParamList>>();
  const { inCall } = useVoiceClient();

  useEffect(() => {
    if (inCall) {
      navigation.navigate('Meeting');
    } else {
      navigation.navigate('Prejoin');
    }
  }, [inCall, navigation]);

};



================================================
FILE: simple-chatbot/client/react-native/src/settings/SettingsManager.ts
================================================
import AsyncStorage from '@react-native-async-storage/async-storage';

export interface SettingsManager {
  enableCam: boolean;
  enableMic: boolean;
  backendURL: string;
}

// Define the settings object
const defaultSettings: SettingsManager = {
  enableCam: false,
  enableMic: true,
  backendURL: process.env.EXPO_PUBLIC_SIMPLE_CHATBOT_SERVER || "",
};

export class SettingsManager {
  private static preferencesKey = 'settingsPreference';

  static async getSettings(): Promise<SettingsManager> {
    try {
      const data = await AsyncStorage.getItem(this.preferencesKey);
      if (data !== null) {
        return JSON.parse(data) as SettingsManager;
      } else {
        return defaultSettings;
      }
    } catch (error) {
      console.error("Failed to load settings:", error);
      return defaultSettings;
    }
  }

  static async updateSettings(settings: SettingsManager): Promise<void> {
    try {
      const data = JSON.stringify(settings);
      await AsyncStorage.setItem(this.preferencesKey, data);
    } catch (error) {
      console.error("Failed to save settings:", error);
    }
  }
}




================================================
FILE: simple-chatbot/client/react-native/src/theme/Assets.ts
================================================
export const Images = {
  dailyBot: require('../../assets/images/pipecat.png'),
};

export const Icons = {
  vision: require('../../assets/icons/vision.png'),
};



================================================
FILE: simple-chatbot/client/react-native/src/theme/Colors.ts
================================================
type ColorsType = {
  white: string;
  black: string;
  backgroundCircle: string;
  backgroundCircleNotConnected: string;
  backgroundApp: string;
  buttonsBorder: string;
  micVolume: string;
  disabledMic: string;
  disabledVision: string;
};

const Colors: ColorsType = {
  white: '#ffffff',
  black: '#000000',
  backgroundCircle: '#374151',
  backgroundCircleNotConnected: '#D1D5DB',
  backgroundApp: '#F9FAFB',
  buttonsBorder: '#E5E7EB',
  micVolume: '#86EFAC',
  disabledMic: '#ee6b6e',
  disabledVision: '#BBF7D0',
};

export default Colors;



================================================
FILE: simple-chatbot/client/react-native/src/theme/CustomButton.tsx
================================================
import React from 'react';
import { TouchableOpacity, Text, StyleSheet, ViewStyle, TextStyle, GestureResponderEvent } from 'react-native';
import { MaterialIcons } from '@expo/vector-icons';

interface CustomButtonProps {
  title: string;
  onPress: (event: GestureResponderEvent) => void;
  backgroundColor?: string; // Optional prop for background color
  textColor?: string; // Optional prop for text color
  style?: ViewStyle; // Optional additional styles for the button container
  textStyle?: TextStyle; // Optional additional styles for the text
  iconName?: string; // Optional prop for the icon name from MaterialIcons
  iconPosition?: 'left' | 'right'; // Optional prop to control icon position
  iconSize?: number; // Optional prop for icon size
  iconColor?: string; // Optional prop for icon color
}

const CustomButton: React.FC<CustomButtonProps> = ({
  title,
  onPress,
  backgroundColor = 'black',
  textColor = 'white',
  style,
  textStyle,
  iconName,
  iconPosition = 'left',
  iconSize = 24,
  iconColor = 'white',
}) => {
  return (
    <TouchableOpacity
      onPress={onPress}
      style={[styles.button, { backgroundColor }, style]}>
      {iconName && iconPosition === 'left' && (
        <MaterialIcons name={iconName as keyof typeof MaterialIcons.glyphMap} size={iconSize} color={iconColor} style={styles.icon} />
      )}
      <Text style={[styles.text, { color: textColor }, textStyle]}>{title}</Text>
      {iconName && iconPosition === 'right' && (
        <MaterialIcons name={iconName as keyof typeof MaterialIcons.glyphMap} size={iconSize} color={iconColor} style={styles.icon} />
      )}
    </TouchableOpacity>
  );
};

const styles = StyleSheet.create({
  button: {
    padding: 12,
    borderRadius: 8,
    alignItems: 'center',
    justifyContent: 'center',
    flexDirection: 'row', // Ensures icon and text are aligned in a row
  },
  text: {
    fontSize: 16,
    fontWeight: 'bold',
  },
  icon: {
    marginHorizontal: 5, // Adds space between the icon and text
  },
});

export default CustomButton;



================================================
FILE: simple-chatbot/client/react-native/src/views/MeetingView.tsx
================================================
import {
  View,
  StyleSheet,
  Text,
  Image,
  TouchableOpacity,
} from 'react-native';

import React from "react"

import { useVoiceClient } from '../context/VoiceClientContext';

import { Images } from '../theme/Assets';
import { MaterialIcons } from '@expo/vector-icons';

import WaveformView from '../components/WaveformView';
import MicrophoneView from '../components/MicrophoneView';
import { SafeAreaView } from 'react-native-safe-area-context';
import Colors from '../theme/Colors';
import CustomButton from '../theme/CustomButton';

const MeetingView: React.FC = () => {

  const { leave, toggleMicInput } = useVoiceClient();

  return (
    <SafeAreaView style={styles.safeArea}>
      <View style={styles.container}>
        <View style={styles.header}>
          <Image source={Images.dailyBot} style={styles.botImage} />
        </View>

        <View style={styles.mainPanel}>
          <WaveformView/>
          <View style={styles.bottomControls}>
            <TouchableOpacity onPress={toggleMicInput}>
              <MicrophoneView
                style={styles.microphone}
              />
            </TouchableOpacity>
          </View>
        </View>

        {/* Bottom Panel */}
        <View style={styles.bottomPanel}>
          <CustomButton
            title="End"
            iconName={"exit-to-app"}
            onPress={leave}
            backgroundColor={Colors.black}
          />
        </View>
      </View>
    </SafeAreaView>
  );
};

const styles = StyleSheet.create({
  safeArea: {
    flex: 1,
    width: "100%",
    backgroundColor: Colors.backgroundApp,
  },
  container: {
    flex: 1,
    padding: 20,
  },
  header: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'space-between',
    paddingBottom: 10,
  },
  botImage: {
    width: 48,
    height: 48,
  },
  mainPanel: {
    flex: 1,
    justifyContent: 'center',
    alignItems: 'center',
  },
  bottomControls: {
    flexDirection: 'row',
    justifyContent: 'center',
    alignItems: 'center',
    width: '100%',
    paddingBottom: 20,
  },
  microphone: {
    width: 160,
    height: 160,
  },
  camera: {
    width: 120,
    height: 120,
  },
  bottomPanel: {
    paddingVertical: 10,
  },
  endButton: {
    flexDirection: 'row',
    alignItems: 'center',
    justifyContent: 'center',
    backgroundColor: 'black',
    borderRadius: 12,
    padding: 10,
  },
  endText: {
    marginLeft: 5,
    color: 'white',
  },
});

export default MeetingView;



================================================
FILE: simple-chatbot/client/react-native/src/views/PreJoinView.tsx
================================================
import {
  View,
  StyleSheet,
  Text,
  TextInput,
  Image
} from "react-native"

import React, { useEffect, useState } from 'react';

import { useVoiceClient } from '../context/VoiceClientContext';

import Colors from '../theme/Colors';
import { Images } from '../theme/Assets';
import CustomButton from '../theme/CustomButton';
import { SettingsManager } from '../settings/SettingsManager';

const styles = StyleSheet.create({
  container: {
    flex: 1,
    padding: 20,
    backgroundColor: Colors.backgroundApp,
    justifyContent: 'center',
    alignItems: 'center',
  },
  image: {
    width: 64,
    height: 64,
    marginBottom: 20,
  },
  header: {
    fontSize: 18,
    fontWeight: 'bold',
    marginBottom: 20,
  },
  textInput: {
    width: '100%',
    padding: 10,
    borderColor: Colors.buttonsBorder,
    backgroundColor: Colors.white,
    borderWidth: 1,
    borderRadius: 5,
    marginBottom: 10,
  },
  lastTextInput: {
    marginBottom: 20,
  },
});

const PreJoinView: React.FC = () => {
  const { start } = useVoiceClient();

  const [backendURL, setBackendURL] = useState<string>('')

  useEffect(() => {
    const loadSettings = async () => {
      const loadedSettings = await SettingsManager.getSettings();
      setBackendURL(loadedSettings.backendURL)
    };
    loadSettings();
  }, []);

  return (
    <View style={styles.container}>
      <Image source={Images.dailyBot} style={styles.image} />
      <Text style={styles.header}>Connect to Pipecat.</Text>
      <TextInput
        placeholder="Server URL"
        value={backendURL}
        onChangeText={setBackendURL}
        style={[styles.textInput, styles.lastTextInput]}
      />
      <CustomButton
        title="Connect"
        onPress={() => start(backendURL)}
        backgroundColor={Colors.backgroundCircle}
      />
    </View>
  )
};

export default PreJoinView;



================================================
FILE: simple-chatbot/server/README.md
================================================
# Simple Chatbot Server

A Pipecat server-side bot that connects to a Pipecat client, enabling a user to talk to the bot through their browser or mobile device.

## Available Bots

The server supports two bot implementations:

1. **OpenAI Bot**

   - Uses gpt-4o for conversation
   - Requires `OPENAI_API_KEY`

2. **Gemini Bot**
   - Uses Google's Gemini model
   - Requires `GOOGLE_API_KEY`

Select your preferred bot by running the corresponding bot.py file:

- `bot-openai.py` for OpenAI
- `bot-gemini.py` for Gemini

## Setup

1. Configure environment variables

   Create a `.env` file:

   ```bash
   cp env.example .env
   ```

   Then, add your API keys:

   ```ini
   # Required API Keys
   DAILY_API_KEY=           # Your Daily API key
   OPENAI_API_KEY=          # Your OpenAI API key (required for OpenAI bot)
   GOOGLE_API_KEY=          # Your Google Gemini API key (required for Gemini bot)
   ELEVENLABS_API_KEY=      # Your ElevenLabs API key

   # Optional Configuration
   DAILY_API_URL=           # Optional: Daily API URL (defaults to https://api.daily.co/v1)
   DAILY_SAMPLE_ROOM_URL=   # Optional: Fixed room URL for development
   ```

2. Set up a virtual environment and install dependencies

   ```bash
   cd server
   uv sync
   ```

3. Run the bot:

   ```bash
   uv run bot-openai.py --transport daily
   ```

## Troubleshooting

If you encounter this error:

```bash
aiohttp.client_exceptions.ClientConnectorCertificateError: Cannot connect to host api.daily.co:443 ssl:True [SSLCertVerificationError: (1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)')]
```

It's because Python cannot verify the SSL certificate from https://api.daily.co when making a POST request to create a room or token.

This issue occurs when the system doesn't have the proper CA certificates.

Install SSL Certificates (macOS): `/Applications/Python\ 3.12/Install\ Certificates.command`



================================================
FILE: simple-chatbot/server/bot-gemini.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""simple-chatbot - Pipecat Voice Agent

This module implements a chatbot using Google's Gemini Live model for natural language
processing. It includes:
- Real-time audio/video interaction through Daily
- Animated robot avatar

The bot runs as part of a pipeline that processes audio/video frames and manages
the conversation flow.

Required AI services:
- Gemini Live (LLM)

Run the bot using::

    uv run bot.py
"""

import os

from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    BotStartedSpeakingFrame,
    BotStoppedSpeakingFrame,
    Frame,
    LLMRunFrame,
    OutputImageRawFrame,
    SpriteFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import DailyRunnerArguments, RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)

sprites = []
script_dir = os.path.dirname(__file__)

# Load sequential animation frames
for i in range(1, 26):
    # Build the full path to the image file
    full_path = os.path.join(script_dir, f"assets/robot0{i}.png")
    # Get the filename without the extension to use as the dictionary key
    # Open the image and convert it to bytes
    with Image.open(full_path) as img:
        sprites.append(OutputImageRawFrame(image=img.tobytes(), size=img.size, format=img.format))

# Create a smooth animation by adding reversed frames
flipped = sprites[::-1]
sprites.extend(flipped)

# Define static and animated states
quiet_frame = sprites[0]  # Static frame for when bot is listening
talking_frame = SpriteFrame(images=sprites)  # Animation sequence for when bot is talking


class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states.

    Switches between static (listening) and animated (talking) states based on
    the bot's current speaking status.
    """

    def __init__(self):
        super().__init__()
        self._is_talking = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and update animation state.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Switch to talking animation when bot starts speaking
        if isinstance(frame, BotStartedSpeakingFrame):
            if not self._is_talking:
                await self.push_frame(talking_frame)
                self._is_talking = True
        # Return to static frame when bot stops speaking
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
            self._is_talking = False

        await self.push_frame(frame, direction)


async def run_bot(transport: BaseTransport):
    """Main bot execution function.

    Sets up and runs the bot pipeline including:
    - Gemini Live model integration
    - Voice activity detection
    - Animation processing
    - RTVI event handling
    """

    # Initialize the Gemini Live model
    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Charon",  # Aoede, Charon, Fenrir, Kore, Puck
    )

    messages = [
        {
            "role": "user",
            "content": "You are Chatbot, a friendly, helpful robot. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by introducing yourself.",
        },
    ]

    # Set up conversation context and management
    # The context_aggregator will automatically collect conversation context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor()

    ta = TalkingAnimation()

    # Pipeline - assembled from reusable components
    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            context_aggregator.user(),
            llm,
            ta,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
            RTVIObserver(rtvi),
        ],
    )

    # Queue initial static frame so video starts immediately
    await task.queue_frame(quiet_frame)

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    transport = None

    match runner_args:
        case DailyRunnerArguments():
            transport = DailyTransport(
                runner_args.room_url,
                runner_args.token,
                "Pipecat Bot",
                params=DailyParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    video_out_enabled=True,
                    video_out_width=1024,
                    video_out_height=576,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    video_out_enabled=True,
                    video_out_width=1024,
                    video_out_height=576,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: simple-chatbot/server/bot-openai.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""simple-chatbot - Pipecat Voice Agent

This module implements a chatbot using OpenAI for natural language
processing. It includes:
- Real-time audio/video interaction through Daily
- Animated robot avatar
- Text-to-speech using ElevenLabs

The bot runs as part of a pipeline that processes audio/video frames and manages
the conversation flow.

Required AI services:
- Deepgram (Speech-to-Text)
- Openai (LLM)
- ElevenLabs (Text-to-Speech)

Run the bot using::

    uv run bot.py
"""

import os

from dotenv import load_dotenv
from loguru import logger
from PIL import Image
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    BotStartedSpeakingFrame,
    BotStoppedSpeakingFrame,
    Frame,
    LLMRunFrame,
    OutputImageRawFrame,
    SpriteFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.runner.types import DailyRunnerArguments, RunnerArguments, SmallWebRTCRunnerArguments
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams, DailyTransport
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)

sprites = []
script_dir = os.path.dirname(__file__)

# Load sequential animation frames
for i in range(1, 26):
    # Build the full path to the image file
    full_path = os.path.join(script_dir, f"assets/robot0{i}.png")
    # Get the filename without the extension to use as the dictionary key
    # Open the image and convert it to bytes
    with Image.open(full_path) as img:
        sprites.append(OutputImageRawFrame(image=img.tobytes(), size=img.size, format=img.format))

# Create a smooth animation by adding reversed frames
flipped = sprites[::-1]
sprites.extend(flipped)

# Define static and animated states
quiet_frame = sprites[0]  # Static frame for when bot is listening
talking_frame = SpriteFrame(images=sprites)  # Animation sequence for when bot is talking


class TalkingAnimation(FrameProcessor):
    """Manages the bot's visual animation states.

    Switches between static (listening) and animated (talking) states based on
    the bot's current speaking status.
    """

    def __init__(self):
        super().__init__()
        self._is_talking = False

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process incoming frames and update animation state.

        Args:
            frame: The incoming frame to process
            direction: The direction of frame flow in the pipeline
        """
        await super().process_frame(frame, direction)

        # Switch to talking animation when bot starts speaking
        if isinstance(frame, BotStartedSpeakingFrame):
            if not self._is_talking:
                await self.push_frame(talking_frame)
                self._is_talking = True
        # Return to static frame when bot stops speaking
        elif isinstance(frame, BotStoppedSpeakingFrame):
            await self.push_frame(quiet_frame)
            self._is_talking = False

        await self.push_frame(frame, direction)


async def run_bot(transport: BaseTransport):
    """Main bot logic."""
    logger.info("Starting bot")

    # Speech-to-Text service
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    # Text-to-Speech service
    tts = ElevenLabsTTSService(
        api_key=os.getenv("ELEVENLABS_API_KEY"), voice_id="pNInz6obpgDQGcFmaJgB"
    )

    # LLM service
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    messages = [
        {
            "role": "system",
            "content": "You are Chatbot, a friendly, helpful robot. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way, but keep your responses brief. Start by introducing yourself.",
        },
    ]

    # Set up conversation context and management
    # The context_aggregator will automatically collect conversation context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    rtvi = RTVIProcessor()

    ta = TalkingAnimation()

    # Pipeline - assembled from reusable components
    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt,
            context_aggregator.user(),
            llm,
            tts,
            ta,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[
            RTVIObserver(rtvi),
        ],
    )

    # Queue initial static frame so video starts immediately
    await task.queue_frame(quiet_frame)

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point."""

    transport = None

    match runner_args:
        case DailyRunnerArguments():
            transport = DailyTransport(
                runner_args.room_url,
                runner_args.token,
                "Pipecat Bot",
                params=DailyParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    video_out_enabled=True,
                    video_out_width=1024,
                    video_out_height=576,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case SmallWebRTCRunnerArguments():
            webrtc_connection: SmallWebRTCConnection = runner_args.webrtc_connection

            transport = SmallWebRTCTransport(
                webrtc_connection=webrtc_connection,
                params=TransportParams(
                    audio_in_enabled=True,
                    audio_out_enabled=True,
                    video_out_enabled=True,
                    video_out_width=1024,
                    video_out_height=576,
                    vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
                    turn_analyzer=LocalSmartTurnAnalyzerV3(),
                ),
            )
        case _:
            logger.error(f"Unsupported runner arguments type: {type(runner_args)}")
            return

    await run_bot(transport)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: simple-chatbot/server/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./assets assets
# Select the bot implementation by uncommenting the appropriate line
COPY ./bot-openai.py bot.py
# COPY ./bot-gemini.py bot.py


================================================
FILE: simple-chatbot/server/env.example
================================================
# Copy this file to .env and fill in your API keys

# Daily (Transport)
DAILY_API_KEY=

# ElevenLabs (STT/TTS)
ELEVENLABS_API_KEY=

# Deepgram (STT/TTS)
DEEPGRAM_API_KEY=

# OpenAI (STT/LLM/TTS/Realtime)
OPENAI_API_KEY=

# Google (LLM)
GOOGLE_API_KEY=


================================================
FILE: simple-chatbot/server/pcc-deploy.toml
================================================
agent_name = "simple-chatbot"
image = "your_dockerhub_username/simple-chatbot:0.1"
secret_set = "simple-chatbot-secrets"
agent_profile = "agent-1x"

# RECOMMENDED: Set an image pull secret:
# https://docs.pipecat.ai/deployment/pipecat-cloud/fundamentals/secrets#image-pull-secrets
# image_credentials = "your_dockerhub_image_pull_secret"

[scaling]
	min_agents = 1


================================================
FILE: simple-chatbot/server/pyproject.toml
================================================
[project]
name = "simple-chatbot"
version = "0.1.0"
description = "Simple chatbot built with Pipecat"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,deepgram,elevenlabs,google,local-smart-turn-v3,openai,runner,silero,webrtc]",
    "pipecatcloud",
]

[dependency-groups]
dev = [
    "pyright>=1.1.404,<2",
    "ruff>=0.12.11,<1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: storytelling-chatbot/README.md
================================================
[![Try](https://img.shields.io/badge/try_it-here-blue)](https://gemini-storybot.vercel.app/)

# Storytelling Chatbot

<img src="image.png" width="420px">

This example shows how to build a voice-driven interactive storytelling experience.
It periodically prompts the user for input for a 'choose your own adventure' style experience.

We use Gemini 2.0 for creating the story and image prompts, and we add visual elements to the story by generating images using Google's Imagen.

---

### It uses the following AI services:

**Deepgram - Speech-to-Text**

Transcribes inbound participant voice media to text.

**Google Gemini 2.0 - LLM**

Our creative writer LLM. You can see the context used to prompt it [here](server/prompts.py)

**ElevenLabs - Text-to-Speech**

Converts and streams the LLM response from text to audio

**Google Imagen - Image Generation**

Adds pictures to our story. Prompting is quite key for style consistency, so we task the LLM to turn each story page into a short image prompt.

---

## Setup

### Client

1. Navigate to the client directory:

   ```shell
   cd client
   ```

2. Install dependencies:

   ```shell
   npm install
   ```

3. Build the client:

   ```shell
   npm run build
   ```

### Server

1. Navigate to the server directory

   ```shell
   cd ../server
   ```

2. Set up your virtual environment and install requirements

   ```shell
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

3. Create environment file and set variables

   ```shell
   mv env.example .env
   ```

   You'll need API keys for:

   - DAILY_API_KEY
   - ELEVENLABS_API_KEY
   - ELEVENLABS_VOICE_ID
   - GOOGLE_API_KEY

4. (Optional) Deployment:

   When deploying to production, to ensure only this app can spawn new bot processes, set your `ENV` to `production`

## Run it locally

1. Navigate back to the demo's root directory:

   ```shell
   cd ..
   ```

2. Run the application:

   ```shell
   python server/bot_runner.py --host localhost
   ```

   You can run with a custom domain or port using: `python server/bot_runner.py --host somehost --p someport`

3. ➡️ Open the host URL in your browser: http://localhost:7860

---

## Improvements to make

- Wait for track_started event to avoid rushed intro
- Show 5 minute timer on the UI



================================================
FILE: storytelling-chatbot/client/README.md
================================================
This is a [Next.js](https://nextjs.org/) project bootstrapped with [`create-next-app`](https://github.com/vercel/next.js/tree/canary/packages/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/basic-features/font-optimization) to automatically optimize and load Inter, a custom Google Font.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js/) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/deployment) for more details.



================================================
FILE: storytelling-chatbot/client/components.json
================================================
{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "default",
  "rsc": true,
  "tsx": true,
  "tailwind": {
    "config": "tailwind.config.ts",
    "css": "app/globals.css",
    "baseColor": "gray",
    "cssVariables": true,
    "prefix": ""
  },
  "aliases": {
    "components": "@/components",
    "utils": "@/app"
  }
}


================================================
FILE: storytelling-chatbot/client/env.example
================================================
SITE_URL=


================================================
FILE: storytelling-chatbot/client/next.config.mjs
================================================
/** @type {import('next').NextConfig} */
const nextConfig = {
  output: "export",

  async rewrites() {
    return [
      {
        source: "/:path*",
        destination: "http://localhost:7860/:path*",
      },
    ];
  },
};

export default nextConfig;



================================================
FILE: storytelling-chatbot/client/package.json
================================================
{
  "name": "client",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@daily-co/daily-js": "^0.62.0",
    "@daily-co/daily-react": "^0.18.0",
    "@radix-ui/react-select": "^2.1.2",
    "@radix-ui/react-slot": "^1.0.2",
    "@tabler/icons-react": "^3.19.0",
    "class-variance-authority": "^0.7.0",
    "clsx": "^2.1.1",
    "framer-motion": "^11.9.0",
    "next": "^14.2.25",
    "react": "^18.3.1",
    "react-dom": "^18.3.1",
    "recoil": "^0.7.7",
    "tailwind-merge": "^2.5.2",
    "tailwindcss-animate": "^1.0.7"
  },
  "devDependencies": {
    "@types/node": "^20.16.10",
    "@types/react": "^18.3.11",
    "@types/react-dom": "^18.3.0",
    "autoprefixer": "^10.4.20",
    "eslint": "^8.57.1",
    "eslint-config-next": "14.1.4",
    "postcss": "^8.4.47",
    "tailwindcss": "^3.4.13",
    "typescript": "^5.6.2"
  }
}



================================================
FILE: storytelling-chatbot/client/postcss.config.js
================================================
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {},
  },
};



================================================
FILE: storytelling-chatbot/client/tailwind.config.ts
================================================
import type { Config } from "tailwindcss";

import { fontFamily } from "tailwindcss/defaultTheme";

const config = {
  darkMode: ["class"],
  content: [
    "./pages/**/*.{ts,tsx}",
    "./components/**/*.{ts,tsx}",
    "./app/**/*.{ts,tsx}",
    "./src/**/*.{ts,tsx}",
  ],
  prefix: "",
  theme: {
    container: {
      center: true,
      padding: "2rem",
      screens: {
        "2xl": "1400px",
      },
    },
    extend: {
      fontFamily: {
        sans: ["var(--font-sans)", ...fontFamily.sans],
        mono: ["var(--font-mono)", ...fontFamily.mono],
      },
      colors: {
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        primary: {
          DEFAULT: "hsl(var(--primary))",
          foreground: "hsl(var(--primary-foreground))",
        },
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      keyframes: {
        "accordion-down": {
          from: { height: "0" },
          to: { height: "var(--radix-accordion-content-height)" },
        },
        "accordion-up": {
          from: { height: "var(--radix-accordion-content-height)" },
          to: { height: "0" },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
      },
    },
  },
  plugins: [require("tailwindcss-animate")],
} satisfies Config;

export default config;



================================================
FILE: storytelling-chatbot/client/tsconfig.json
================================================
{
  "compilerOptions": {
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": [
        "./*"
      ]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    "build/types/**/*.ts"
  ],
  "exclude": [
    "node_modules"
  ]
}



================================================
FILE: storytelling-chatbot/client/.eslintrc.json
================================================
{
  "extends": "next/core-web-vitals"
}



================================================
FILE: storytelling-chatbot/client/app/globals.css
================================================
@tailwind base;
@tailwind components;
@tailwind utilities;

@layer base {
  :root {
    --background: 0 0% 100%;
    --foreground: 224 71.4% 4.1%;

    --card: 0 0% 100%;
    --card-foreground: 224 71.4% 4.1%;

    --popover: 0 0% 100%;
    --popover-foreground: 224 71.4% 4.1%;

    --primary: 220.9 39.3% 11%;
    --primary-foreground: 210 20% 98%;

    --secondary: 220 14.3% 95.9%;
    --secondary-foreground: 220.9 39.3% 11%;

    --muted: 220 14.3% 95.9%;
    --muted-foreground: 220 8.9% 46.1%;

    --accent: 220 14.3% 95.9%;
    --accent-foreground: 220.9 39.3% 11%;

    --destructive: 0 84.2% 60.2%;
    --destructive-foreground: 210 20% 98%;

    --border: 220 13% 91%;
    --input: 220 13% 91%;
    --ring: 224 71.4% 4.1%;

    --radius: 0.5rem;
  }

  .dark {
    --background: 224 71.4% 4.1%;
    --foreground: 210 20% 98%;

    --card: 224 71.4% 4.1%;
    --card-foreground: 210 20% 98%;

    --popover: 224 71.4% 4.1%;
    --popover-foreground: 210 20% 98%;

    --primary: 210 20% 98%;
    --primary-foreground: 220.9 39.3% 11%;

    --secondary: 215 27.9% 16.9%;
    --secondary-foreground: 210 20% 98%;

    --muted: 215 27.9% 16.9%;
    --muted-foreground: 217.9 10.6% 64.9%;

    --accent: 215 27.9% 16.9%;
    --accent-foreground: 210 20% 98%;

    --destructive: 0 62.8% 30.6%;
    --destructive-foreground: 210 20% 98%;

    --border: 215 27.9% 16.9%;
    --input: 215 27.9% 16.9%;
    --ring: 216 12.2% 83.9%;
  }
}

@layer base {
  * {
    @apply border-border;
  }
  body {
    @apply bg-background text-foreground;
  }
}

body{
  background: url("/bg.jpg") no-repeat center center; 
  background-size: cover;
}

.cardShadow{
  box-shadow: 0px 124px 35px 0px rgba(0, 0, 0, 0.00), 0px 79px 32px 0px rgba(0, 0, 0, 0.01), 0px 45px 27px 0px rgba(0, 0, 0, 0.05), 0px 20px 20px 0px rgba(0, 0, 0, 0.09), 0px 5px 11px 0px rgba(0, 0, 0, 0.10);
}

@keyframes fadeInSlideUp {
  0% {
    opacity: 0;
    transform: translateY(50px);
  }
  100% {
    opacity: 1;
    transform: translateY(0);
  }
}

.fade-in {
  animation: fadeIn 1s ease-out;
}

@keyframes fadeIn {
  0% {
    opacity: 0;
  }
  100% {
    opacity: 1;
  }
}


================================================
FILE: storytelling-chatbot/client/app/layout.tsx
================================================
import React from "react";

import "./globals.css";
import type { Metadata } from "next";
import { Space_Grotesk, Space_Mono } from "next/font/google";

import { cn } from "@/app/utils";

// Font
const sans = Space_Grotesk({
  subsets: ["latin"],
  weight: ["400", "500", "600"],
  variable: "--font-sans",
});

const mono = Space_Mono({
  subsets: ["latin"],
  weight: ["400", "700"],
  variable: "--font-mono",
});

export const metadata: Metadata = {
  title: "Storytelling Chatbot - Daily AI",
  description: "Built with git.new/ai",
  metadataBase: new URL(process.env.SITE_URL || "http://localhost:3000"),
};

export default function RootLayout({
  children,
}: Readonly<{
  children: React.ReactNode;
}>) {
  return (
    <html lang="en">
      <body
        className={cn(
          "min-h-screen bg-background font-sans antialiased flex flex-col",
          sans.variable,
          mono.variable
        )}
      >
        <main className="flex flex-1">{children}</main>
      </body>
    </html>
  );
}



================================================
FILE: storytelling-chatbot/client/app/page.tsx
================================================
"use client";

import React from "react";
import { DailyProvider, useCallObject } from "@daily-co/daily-react";

import App from "../components/App";

export default function Home() {
  const callObject = useCallObject({});

  return (
    <DailyProvider callObject={callObject}>
      <App />
    </DailyProvider>
  );
}



================================================
FILE: storytelling-chatbot/client/app/utils.ts
================================================
import { type ClassValue, clsx } from "clsx"
import { twMerge } from "tailwind-merge"

export function cn(...inputs: ClassValue[]) {
  return twMerge(clsx(inputs))
}



================================================
FILE: storytelling-chatbot/client/components/App.tsx
================================================
"use client";

import React, { useState } from "react";

import { useDaily } from "@daily-co/daily-react";
import Setup from "./Setup";
import Story from "./Story";

type State =
  | "idle"
  | "connecting"
  | "connected"
  | "started"
  | "finished"
  | "error";

export default function Call() {
  const daily = useDaily();

  const [state, setState] = useState<State>("idle");
  const [room, setRoom] = useState<string | null>(null);

  async function start() {
    setState("connecting");

    if (!daily) return;

    // Create a new room for the story session
    try {
      const response = await fetch("/", {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
        },
      });

      const { room_url, token } = await response.json();

      // Keep a reference to the room url for later
      setRoom(room_url);

      // Join the WebRTC session
      await daily.join({
        url: room_url,
        token,
        videoSource: false,
        startAudioOff: true,
      });

      setState("connected");

      // Disable local audio, the bot will say hello first
      daily.setLocalAudio(false);

      setState("started");
    } catch (error) {
      setState("error");
    }
  }

  async function leave() {
    await daily?.leave();
    setState("finished");
  }

  if (state === "error") {
    return (
      <div className="flex items-center mx-auto">
        <p className="text-red-500 font-semibold bg-white px-4 py-2 shadow-xl rounded-lg">
          This demo is currently at capacity. Please try again later.
        </p>
      </div>
    );
  }

  if (state === "started") {
    return <Story handleLeave={() => leave()} />;
  }

  return <Setup handleStart={() => start()} />;
}



================================================
FILE: storytelling-chatbot/client/components/Setup.tsx
================================================
import React from "react";
import { Button } from "@/components/ui/button";
import DevicePicker from "@/components/DevicePicker";
import { IconAlertCircle, IconEar, IconLoader2 } from "@tabler/icons-react";

type SetupProps = {
  handleStart: () => void;
};

const buttonLabel = {
  intro: "Next",
  setup: "Let's begin!",
  loading: "Joining...",
};
export const Setup: React.FC<SetupProps> = ({ handleStart }) => {
  const [state, setState] = React.useState<"intro" | "setup" | "loading">(
    "intro"
  );

  return (
    <div className="w-full flex flex-col items-center justify-between">
      <div className="bg-white rounded-3xl cardAnim cardShadow p-9 max-w-screen-sm mx-auto outline outline-[5px] outline-gray-600/10 my-auto">
        <div className="flex flex-col gap-6">
          <h1 className="text-4xl font-bold text-pretty tracking-tighter mb-4">
            Welcome to <span className="text-sky-500">Storytime</span>
          </h1>
          {state === "intro" ? (
            <>
              <p className="text-gray-600 leading-relaxed text-pretty">
                This app demos a voice-controlled storytelling chatbot. It will
                start with the bot asking you what kind of story you&apos;d like
                to hear (e.g. a fairy tale, a mystery, etc.). After each scene,
                the bot will pause to ask for your input. Direct the story any
                way you choose!
              </p>
              <p className="flex flex-row gap-2 text-gray-600 font-medium">
                <IconEar size={24} /> For best results, try in a quiet
                environment!
              </p>
              <p className="flex flex-row gap-2 text-gray-600 font-medium text-red-500">
                <IconAlertCircle size={24} /> This demo expires after 5 minutes.
              </p>
            </>
          ) : (
            <>
              <p className="text-gray-600 leading-relaxed text-pretty">
                Since you&apos;ll be talking to Storybot, we need to make sure
                it can hear you! Please configure your microphone and speakers
                below.
              </p>
              <DevicePicker />
            </>
          )}
          <hr className="border-gray-150 my-2" />

          <Button
            size="lg"
            disabled={state === "loading"}
            onClick={() => {
              if (state === "intro") {
                setState("setup");
              } else {
                setState("loading");
                handleStart();
              }
            }}
          >
            {state === "loading" && (
              <IconLoader2
                size={21}
                stroke={2}
                className="mr-2 h-4 w-4 animate-spin"
              />
            )}
            {buttonLabel[state]}
          </Button>
        </div>
      </div>
      <footer className="flex-0 text-center font-mono text-sm text-gray-100 py-6">
        <span className="bg-gray-800/70 px-3 py-1 rounded-md">
          Created with{" "}
          <a
            href="https://git.new/ai"
            className="text-violet-300 underline decoration-violet-400 hover:text-violet-100"
          >
            git.new/ai
          </a>
        </span>
      </footer>
    </div>
  );
};

export default Setup;



================================================
FILE: storytelling-chatbot/client/components/Story.tsx
================================================
import React, { useState, useEffect } from "react";
import {
  useDaily,
  useParticipantIds,
  useAppMessage,
  DailyAudio,
} from "@daily-co/daily-react";
import { IconLogout, IconLoader2 } from "@tabler/icons-react";

import VideoTile from "@/components/VideoTile";
import { Button } from "@/components/ui/button";
import UserInputIndicator from "@/components/UserInputIndicator";
import WaveText from "@/components/WaveText";

interface StoryProps {
  handleLeave: () => void;
}

const Story: React.FC<StoryProps> = ({ handleLeave }) => {
  const daily = useDaily();
  const participantIds = useParticipantIds({ filter: "remote" });
  const [storyState, setStoryState] = useState<"user" | "assistant">(
    "assistant"
  );

  useAppMessage({
    onAppMessage: (e) => {
      if (!daily || !e.data?.cue) return;

      // Determine the UI state from the cue sent by the bot
      if (e.data?.cue === "user_turn") {
        // Delay enabling local mic input to avoid feedback from LLM
        setTimeout(() => daily.setLocalAudio(true), 500);
        setStoryState("user");
      } else {
        // Uncomment the next line to mute the mic while the 
        // assistant it talking. Leave it commented to allow for interruptions
        // daily.setLocalAudio(false);
        setStoryState("assistant");
      }
    },
  });

  return (
    <div className="w-full flex flex-col flex-1 self-stretch">
      {/* Absolute elements */}
      <div className="absolute top-20 w-full text-center z-50">
        <WaveText active={storyState === "user"} />
      </div>
      <header className="flex absolute top-0 w-full z-50 p-6 justify-end">
        <Button variant="secondary" onClick={() => handleLeave()}>
          <IconLogout size={21} className="mr-2" />
          Exit
        </Button>
      </header>
      <div className="absolute inset-0 bg-gray-800 bg-opacity-90 z-10 fade-in"></div>

      {/* Static elements */}
      <div className="relative z-20 flex-1 flex items-center justify-center">
        {participantIds.length >= 1 ? (
          <VideoTile
            sessionId={participantIds[0]}
            inactive={false}
          />
        ) : (
          <span className="p-3 rounded-full bg-gray-900/60 animate-pulse">
            <IconLoader2
              size={42}
              stroke={2}
              className="animate-spin text-white z-20 self-center"
            />
          </span>
        )}
        <DailyAudio />
      </div>
      <UserInputIndicator active={true} />
    </div>
  );
};

export default Story;



================================================
FILE: storytelling-chatbot/client/components/AudioIndicator/index.tsx
================================================
import {
  useAudioLevel,
  useAudioTrack,
  useLocalSessionId,
} from "@daily-co/daily-react";
import { useCallback, useRef } from "react";

export const AudioIndicator: React.FC = () => {
  const localSessionId = useLocalSessionId();
  const audioTrack = useAudioTrack(localSessionId);
  const volRef = useRef<HTMLDivElement>(null);

  useAudioLevel(
    audioTrack?.persistentTrack,
    useCallback((volume) => {
      // this volume number will be between 0 and 1
      // give it a minimum scale of 0.15 to not completely disappear 👻
      if (volRef.current) {
        const v = volume * 1.75;
        volRef.current.style.transform = `scale(${Math.max(0.1, v)})`;
      }
    }, [])
  );

  // Your audio track's audio volume visualized in a small circle,
  // whose size changes depending on the volume level
  return (
    <>
      <div className="vol bg-teal-700" ref={volRef} />
      <style jsx>{`
        .vol {
          position: absolute;
          overflow: hidden;
          inset: 0px;
          z-index: 0;
          border-radius: 999px;
          transition: all 0.1s ease;
          transform: scale(0);
        }
      `}</style>
    </>
  );
};

export const AudioIndicatorBar: React.FC = () => {
  const localSessionId = useLocalSessionId();
  const audioTrack = useAudioTrack(localSessionId);

  const volRef = useRef<HTMLDivElement>(null);

  useAudioLevel(
    audioTrack?.persistentTrack,
    useCallback((volume) => {
      if (volRef.current)
        volRef.current.style.width = Math.max(2, volume * 100) + "%";
    }, [])
  );

  return (
    <div className="flex-1 bg-gray-200 h-[8px] rounded-full overflow-hidden">
      <div
        className="bg-green-500 h-[8px] w-[0] rounded-full transition-all duration-100 ease"
        ref={volRef}
      />
    </div>
  );
};

export default AudioIndicator;



================================================
FILE: storytelling-chatbot/client/components/DevicePicker/index.tsx
================================================
"use client";

import { useEffect } from "react";
import { DailyMeetingState } from "@daily-co/daily-js";
import { useDaily, useDevices } from "@daily-co/daily-react";
import {
  Select,
  SelectContent,
  SelectItem,
  SelectTrigger,
  SelectValue,
} from "@/components/ui/select";
import { IconMicrophone, IconDeviceSpeaker } from "@tabler/icons-react";
import { AudioIndicatorBar } from "../AudioIndicator";

interface Props {}

export default function DevicePicker({}: Props) {
  const daily = useDaily();
  const {
    currentMic,
    hasMicError,
    micState,
    microphones,
    setMicrophone,
    currentSpeaker,
    speakers,
    setSpeaker,
  } = useDevices();

  const handleMicrophoneChange = (value: string) => {
    setMicrophone(value);
  };

  const handleSpeakerChange = (value: string) => {
    setSpeaker(value);
  };

  useEffect(() => {
    if (microphones.length > 0 || !daily || daily.isDestroyed()) return;
    const meetingState = daily.meetingState();
    const meetingStatesBeforeJoin: DailyMeetingState[] = [
      "new",
      "loading",
      "loaded",
    ];
    if (meetingStatesBeforeJoin.includes(meetingState)) {
      daily.startCamera({ startVideoOff: true, startAudioOff: false });
    }
  }, [daily, microphones]);

  return (
    <div className="flex flex-col gap-5">
      <section>
        <label className="uppercase text-sm tracking-wider text-gray-500">
          Microphone:
        </label>
        <div className="flex flex-row gap-4 items-center mt-2">
          <IconMicrophone size={24} />
          <div className="flex flex-col flex-1 gap-3">
            <Select onValueChange={handleMicrophoneChange}>
              <SelectTrigger className="">
                <SelectValue
                  placeholder={
                    hasMicError ? "error" : currentMic?.device?.label
                  }
                />
              </SelectTrigger>
              <SelectContent>
                {hasMicError && (
                  <option value="error" disabled>
                    No microphone access.
                  </option>
                )}

                {microphones.map((m) => (
                  <SelectItem key={m.device.deviceId} value={m.device.deviceId}>
                    {m.device.label}
                  </SelectItem>
                ))}
              </SelectContent>
            </Select>
            <AudioIndicatorBar />
          </div>
        </div>
      </section>

      <section>
        <label className="uppercase text-sm tracking-wider text-gray-500">
          Speakers:
        </label>
        <div className="flex flex-row gap-4 items-center mt-2">
          <IconDeviceSpeaker size={24} />
          <Select onValueChange={handleSpeakerChange}>
            <SelectTrigger className="">
              <SelectValue placeholder={currentSpeaker?.device?.label} />
            </SelectTrigger>
            <SelectContent>
              {speakers.map((m) => (
                <SelectItem key={m.device.deviceId} value={m.device.deviceId}>
                  {m.device.label}
                </SelectItem>
              ))}
            </SelectContent>
          </Select>
        </div>
      </section>
      {hasMicError && (
        <div className="error">
          {micState === "blocked" ? (
            <p className="text-red-500">
              Please check your browser and system permissions. Make sure that
              this app is allowed to access your microphone.
            </p>
          ) : micState === "in-use" ? (
            <p className="text-red-500">
              Your microphone is being used by another app. Please close any
              other apps using your microphone and restart this app.
            </p>
          ) : micState === "not-found" ? (
            <p className="text-red-500">
              No microphone seems to be connected. Please connect a microphone.
            </p>
          ) : micState === "not-supported" ? (
            <p className="text-red-500">
              This app is not supported on your device. Please update your
              software or use a different device.
            </p>
          ) : (
            <p className="text-red-500">
              There seems to be an issue accessing your microphone. Try
              restarting the app or consult a system administrator.
            </p>
          )}
        </div>
      )}
    </div>
  );
}



================================================
FILE: storytelling-chatbot/client/components/MicToggle/index.tsx
================================================
import {
  useDaily,
  useLocalSessionId,
  useMediaTrack,
} from "@daily-co/daily-react";
import { useCallback } from "react";
import { IconMicrophone, IconMicrophoneOff } from "@tabler/icons-react";

export const MicToggle: React.FC = () => {
  const daily = useDaily();
  const localSessionId = useLocalSessionId();
  const audioTrack = useMediaTrack(localSessionId, "audio");
  const isMicMuted =
    audioTrack.state === "blocked" || audioTrack.state === "off";

  const handleClick = useCallback(() => {
    if (!daily) return;
    daily.setLocalAudio(isMicMuted);
  }, [daily, isMicMuted]);

  const text = isMicMuted ? (
    <IconMicrophone size={21} />
  ) : (
    <IconMicrophoneOff size={21} />
  );

  return (
    <button className="MicToggle UIButton" onClick={handleClick}>
      {text}
    </button>
  );
};

export default MicToggle;



================================================
FILE: storytelling-chatbot/client/components/StoryTranscript/index.tsx
================================================
"use client";

import React, { useEffect, useRef, useState } from "react";
import { useAppMessage } from "@daily-co/daily-react";
import { DailyEventObjectAppMessage } from "@daily-co/daily-js";

import styles from "./StoryTranscript.module.css";

export default function StoryTranscript() {
  const [partialText, setPartialText] = useState<string>("");
  const [sentences, setSentences] = useState<string[]>([]);
  const intervalRef = useRef<any | null>(null);

  useEffect(() => {
    clearInterval(intervalRef.current);

    intervalRef.current = setInterval(() => {
      if (sentences.length > 2) {
        setSentences((s) => s.slice(1));
      }
    }, 2500);

    return () => clearInterval(intervalRef.current);
  }, [sentences]);

  useAppMessage({
    onAppMessage: (e: DailyEventObjectAppMessage<any>) => {
      if (e.fromId && e.fromId === "transcription") {
        // Check for LLM transcripts only
        if (e.data.user_id !== "") {
          setPartialText(e.data.text);
          if (e.data.is_final) {
            setPartialText("");
            setSentences((s) => [...s, e.data.text]);
          }
        }
      }
    },
  });

  return (
    <div className={styles.container}>
      {sentences.map((sentence, index) => (
        <p key={index} className={`${styles.transcript} ${styles.sentence}`}>
          <span>{sentence}</span>
        </p>
      ))}
      {partialText && (
        <p className={`${styles.transcript}`}>
          <span>{partialText}</span>
        </p>
      )}
    </div>
  );
}



================================================
FILE: storytelling-chatbot/client/components/StoryTranscript/StoryTranscript.module.css
================================================
.container{
    position: absolute;
    color:white;
    z-index: 50;    
    margin: 0 auto;
    display: flex;
    flex-direction: column;
    @apply gap-4 inset-6;
    align-items: center;
    justify-content: end;
    text-align: center;
}

.transcript{
    @apply font-semibold;
}

.transcript span{
    box-decoration-break: clone;
    @apply bg-gray-900/80 rounded-md px-4 py-2;
}

.sentence{
    opacity: 1;
    animation: fadeOut 2.5s linear forwards;
    animation-delay: 1s;
}
@keyframes fadeOut {
    0% {
        opacity: 1;
        transform: scale(1);
    }
    20% {
        transform: scale(1);
        filter: blur(0);
    }
    100% {
        transform: scale(0.8) translateY(-50%);
        filter: blur(25px);
        opacity: 0;
    }
}


================================================
FILE: storytelling-chatbot/client/components/ui/button.tsx
================================================
import * as React from "react";
import { Slot } from "@radix-ui/react-slot";
import { cva, type VariantProps } from "class-variance-authority";

import { cn } from "@/app/utils";

const buttonVariants = cva(
  "inline-flex items-center justify-center whitespace-nowrap rounded-md text-md font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50",
  {
    variants: {
      variant: {
        default: "bg-primary text-primary-foreground hover:bg-primary/90",
        destructive:
          "bg-destructive text-destructive-foreground hover:bg-destructive/90",
        outline:
          "border border-input bg-background hover:bg-accent hover:text-accent-foreground",
        secondary:
          "bg-secondary text-secondary-foreground hover:bg-secondary/80",
        ghost: "hover:bg-accent hover:text-accent-foreground",
        link: "text-primary underline-offset-4 hover:underline",
      },
      size: {
        default: "h-11 rounded-lg px-6 py-2",
        sm: "h-9 rounded-md px-3",
        lg: "h-12 rounded-xl px-8",
        icon: "h-10 w-10",
      },
    },
    defaultVariants: {
      variant: "default",
      size: "default",
    },
  }
);

export interface ButtonProps
  extends React.ButtonHTMLAttributes<HTMLButtonElement>,
    VariantProps<typeof buttonVariants> {
  asChild?: boolean;
}

const Button = React.forwardRef<HTMLButtonElement, ButtonProps>(
  ({ className, variant, size, asChild = false, ...props }, ref) => {
    const Comp = asChild ? Slot : "button";
    return (
      <Comp
        className={cn(buttonVariants({ variant, size, className }))}
        ref={ref}
        {...props}
      />
    );
  }
);
Button.displayName = "Button";

export { Button, buttonVariants };



================================================
FILE: storytelling-chatbot/client/components/ui/select.tsx
================================================
"use client";

import * as React from "react";
import * as SelectPrimitive from "@radix-ui/react-select";
import { IconCheck, IconChevronDown, IconChevronUp } from "@tabler/icons-react";

import { cn } from "@/app/utils";

const Select = SelectPrimitive.Root;

const SelectGroup = SelectPrimitive.Group;

const SelectValue = SelectPrimitive.Value;

const SelectTrigger = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Trigger>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Trigger>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Trigger
    ref={ref}
    className={cn(
      "flex gap-3 h-12 w-full items-center justify-between rounded-xl border border-input bg-background px-4 py-2 text-sm ring-offset-background placeholder:text-muted-foreground focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 disabled:cursor-not-allowed disabled:opacity-50 [&>span]:line-clamp-1",
      className
    )}
    {...props}
  >
    {children}
    <SelectPrimitive.Icon asChild>
      <IconChevronDown className="h-4 w-4 opacity-50" />
    </SelectPrimitive.Icon>
  </SelectPrimitive.Trigger>
));
SelectTrigger.displayName = SelectPrimitive.Trigger.displayName;

const SelectScrollUpButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollUpButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollUpButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollUpButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <IconChevronUp className="h-4 w-4" />
  </SelectPrimitive.ScrollUpButton>
));
SelectScrollUpButton.displayName = SelectPrimitive.ScrollUpButton.displayName;

const SelectScrollDownButton = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.ScrollDownButton>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.ScrollDownButton>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.ScrollDownButton
    ref={ref}
    className={cn(
      "flex cursor-default items-center justify-center py-1",
      className
    )}
    {...props}
  >
    <IconChevronDown className="h-4 w-4" />
  </SelectPrimitive.ScrollDownButton>
));
SelectScrollDownButton.displayName =
  SelectPrimitive.ScrollDownButton.displayName;

const SelectContent = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Content>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Content>
>(({ className, children, position = "popper", ...props }, ref) => (
  <SelectPrimitive.Portal>
    <SelectPrimitive.Content
      ref={ref}
      className={cn(
        "relative z-50 max-h-96 min-w-[8rem] overflow-hidden rounded-md border bg-popover text-popover-foreground shadow-md data-[state=open]:animate-in data-[state=closed]:animate-out data-[state=closed]:fade-out-0 data-[state=open]:fade-in-0 data-[state=closed]:zoom-out-95 data-[state=open]:zoom-in-95 data-[side=bottom]:slide-in-from-top-2 data-[side=left]:slide-in-from-right-2 data-[side=right]:slide-in-from-left-2 data-[side=top]:slide-in-from-bottom-2",
        position === "popper" &&
          "data-[side=bottom]:translate-y-1 data-[side=left]:-translate-x-1 data-[side=right]:translate-x-1 data-[side=top]:-translate-y-1",
        className
      )}
      position={position}
      {...props}
    >
      <SelectScrollUpButton />
      <SelectPrimitive.Viewport
        className={cn(
          "p-1",
          position === "popper" &&
            "h-[var(--radix-select-trigger-height)] w-full min-w-[var(--radix-select-trigger-width)]"
        )}
      >
        {children}
      </SelectPrimitive.Viewport>
      <SelectScrollDownButton />
    </SelectPrimitive.Content>
  </SelectPrimitive.Portal>
));
SelectContent.displayName = SelectPrimitive.Content.displayName;

const SelectLabel = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Label>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Label>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Label
    ref={ref}
    className={cn("py-1.5 pl-8 pr-2 text-sm font-semibold", className)}
    {...props}
  />
));
SelectLabel.displayName = SelectPrimitive.Label.displayName;

const SelectItem = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Item>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Item>
>(({ className, children, ...props }, ref) => (
  <SelectPrimitive.Item
    ref={ref}
    className={cn(
      "relative flex w-full cursor-default select-none items-center rounded-sm py-1.5 pl-8 pr-2 text-sm outline-none focus:bg-accent focus:text-accent-foreground data-[disabled]:pointer-events-none data-[disabled]:opacity-50",
      className
    )}
    {...props}
  >
    <span className="absolute left-2 flex h-3.5 w-3.5 items-center justify-center">
      <SelectPrimitive.ItemIndicator>
        <IconCheck className="h-4 w-4" />
      </SelectPrimitive.ItemIndicator>
    </span>

    <SelectPrimitive.ItemText>{children}</SelectPrimitive.ItemText>
  </SelectPrimitive.Item>
));
SelectItem.displayName = SelectPrimitive.Item.displayName;

const SelectSeparator = React.forwardRef<
  React.ElementRef<typeof SelectPrimitive.Separator>,
  React.ComponentPropsWithoutRef<typeof SelectPrimitive.Separator>
>(({ className, ...props }, ref) => (
  <SelectPrimitive.Separator
    ref={ref}
    className={cn("-mx-1 my-1 h-px bg-muted", className)}
    {...props}
  />
));
SelectSeparator.displayName = SelectPrimitive.Separator.displayName;

export {
  Select,
  SelectGroup,
  SelectValue,
  SelectTrigger,
  SelectContent,
  SelectLabel,
  SelectItem,
  SelectSeparator,
  SelectScrollUpButton,
  SelectScrollDownButton,
};



================================================
FILE: storytelling-chatbot/client/components/ui/typewriter.tsx
================================================
"use client";

import { cn } from "@/app/utils";
import { motion } from "framer-motion";

export const TypewriterEffect = ({
  words,
  className,
}: {
  words: string[];
  className?: string;
  cursorClassName?: string;
}) => {
  const renderWords = () => {
    return (
      <div>
        {words.map((word, idx) => {
          return (
            <div key={`word-${idx}`} className="inline-block">
              {word.split("").map((char, index) => (
                <span key={`char-${index}`}>{char}</span>
              ))}
              &nbsp;
            </div>
          );
        })}
      </div>
    );
  };

  return (
    <div className={cn("flex", className)}>
      {words.length < 1 ? (
        <span>...</span>
      ) : (
        <motion.div
          className="overflow-hidden"
          initial={{
            width: "0%",
          }}
          whileInView={{
            width: "fit-content",
          }}
          transition={{
            duration: 0.5,
            ease: "linear",
            delay: 1,
          }}
        >
          <div
            style={{
              whiteSpace: "nowrap",
            }}
          >
            {renderWords()}
          </div>
        </motion.div>
      )}
    </div>
  );
};



================================================
FILE: storytelling-chatbot/client/components/UserInputIndicator/index.tsx
================================================
import React, { useState, useEffect, useRef } from "react";

import { useAppMessage } from "@daily-co/daily-react";
import { DailyEventObjectAppMessage } from "@daily-co/daily-js";
import styles from "./UserInputIndicator.module.css";
import { IconMicrophone } from "@tabler/icons-react";
import { TypewriterEffect } from "../ui/typewriter";
import AudioIndicator from "../AudioIndicator";

interface Props {
  active: boolean;
}

export default function UserInputIndicator({ active }: Props) {
  const [transcription, setTranscription] = useState<string[]>([]);
  const timeoutRef = useRef<NodeJS.Timeout>();

  const resetTimeout = () => {
    if (timeoutRef.current) {
      clearTimeout(timeoutRef.current);
    }
    timeoutRef.current = setTimeout(() => {
      setTranscription([]);
    }, 5000);
  };

  useEffect(() => {
    return () => {
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current);
      }
    };
  }, []);

  useAppMessage({
    onAppMessage: (e: DailyEventObjectAppMessage<any>) => {
      if (e.fromId && e.fromId === "transcription") {
        if (e.data.user_id === "" && e.data.is_final) {
          setTranscription((t) => [...t, ...e.data.text.split(" ")]);
          resetTimeout();
        }
      }
    },
  });

  useEffect(() => {
    if (active) return;
    const t = setTimeout(() => setTranscription([]), 4000);
    return () => clearTimeout(t);
  }, [active]);

  return (
    <div className={`${styles.panel} ${active ? styles.active : ""}`}>
      <div className="relative z-20 flex flex-col">
        <div
          className={`${styles.micIcon} ${active ? styles.micIconActive : ""}`}
        >
          <IconMicrophone size={42} />
          {active && <AudioIndicator />}
        </div>
        <footer className={styles.transcript}>
          <TypewriterEffect words={transcription} />
        </footer>
      </div>
    </div>
  );
}



================================================
FILE: storytelling-chatbot/client/components/UserInputIndicator/UserInputIndicator.module.css
================================================
.panel{
    width: 100%;
    pointer-events: none;
    color: #ffffff;
    text-align: center;
    position: relative;
    @apply pb-8;
}

.panel::after{
    content: "";
    position: absolute;
    inset: 0px;
    z-index: 10;
    opacity: 0;
    transition: all 0.3s ease;
    @apply bg-gradient-to-t from-gray-950 to-transparent;
}

.active::after{ opacity: 1;}

.micIcon{
    position: relative;
    width: 120px;
    height: 120px;
    border-radius: 120px;
    border: 6px solid;
    outline: 6px solid;
    display: flex;
    justify-content: center;
    align-items: center;
    margin: 0 auto;
    z-index: 20;
    transition: all 0.5s ease;
    @apply bg-gray-700/60 border-gray-600 outline-gray-900/20;

}

.micIcon svg{
    position: relative;
    z-index: 20;
    opacity: 0.25;
    transition: opacity 0.5s ease; 
}

.micIconActive{
    @apply bg-teal-950 border-teal-500 outline-teal-500/20;
}

.micIconActive svg{
    opacity: 1;
}

.transcript{
    flex: 0;
    align-self: center;
    opacity: 0.25;
    transition: opacity 1s ease;
    transition-delay: 2.5s;
    @apply bg-gray-900/90 font-medium py-1 px-2 rounded-sm mt-4;
}

.active .transcript{
    opacity: 1;
}


================================================
FILE: storytelling-chatbot/client/components/VideoTile/index.tsx
================================================
import React from "react";
import styles from "./VideoTile.module.css";
import { DailyVideo } from "@daily-co/daily-react";
import StoryTranscript from "@/components/StoryTranscript";

interface Props {
  sessionId: string;
  inactive: boolean;
}

const VideoTile = ({ sessionId, inactive }: Props) => {
  return (
    <div className={`${styles.container} ${inactive ? styles.inactive : ""} `}>
      <StoryTranscript />

      <div className={styles.videoTile}>
        <DailyVideo
          sessionId={sessionId}
          type={"video"}
          className="aspect-square"
        />
      </div>
    </div>
  );
};

export default VideoTile;



================================================
FILE: storytelling-chatbot/client/components/VideoTile/VideoTile.module.css
================================================
.container{
    position: relative;
    animation: fadeIn 3s ease;
    transition: all 3s ease-out;
}

.videoTile{
    @apply bg-gray-950;
    width: 760px;
    height: 760px;
    mask-image: url('/alpha-mask.gif');
    mask-size: cover;
    mask-repeat: no-repeat;
    margin:0 auto;
    z-index: 10;
    position: relative;
}

.inactive{
    opacity: 0.7;
    filter:blur(3px);
    transform: scale(0.95)
}

@keyframes fadeIn {
    0% {
        filter: blur(100px);
        opacity: 0;
    }
    100% {
        filter: blur(0px);
        opacity: 1;
    }
}


================================================
FILE: storytelling-chatbot/client/components/WaveText/index.tsx
================================================
import React from "react";
import styles from "./WaveText.module.css";

interface Props {
  active: boolean;
}

export default function WaveText({ active }: Props) {
  return (
    <div className={`${styles.waveText} ${active ? styles.active : ""}`}>
      <span>W</span>
      <span>h</span>
      <span>a</span>
      <span>t</span>
      <span>&nbsp;&nbsp;</span>
      <span>n</span>
      <span>e</span>
      <span>x</span>
      <span>t</span>
      <span>?</span>
    </div>
  );
}



================================================
FILE: storytelling-chatbot/client/components/WaveText/WaveText.module.css
================================================
.waveText{
    color: white;
    text-shadow: 3px 3px 0px rgba(0,0,0,0.5);
    opacity: 0;
    transition: opacity 2s ease;
    position: relative;
    @apply text-4xl font-bold;
}

.active{
    opacity: 1;
}

@keyframes jump {
    0% {
        opacity: 0.5;
        transform:translateY(0px)
    }
    50% {
        opacity: 1;
        transform:translateY(-30px);
    }
    100% {
        opacity: 0.55;
        transform:translateY(0px) 
    }
  }

.waveText span{
    display:inline-block;
    animation:jump 2s infinite ease-in-out;

}

.waveText span:nth-child(1) {
    animation-delay:0s;
}

.waveText span:nth-child(1) {
    animation-delay:0.1s;
}
.waveText span:nth-child(2) {
    animation-delay:0.2s;
}
.waveText span:nth-child(3) {
    animation-delay:0.3s;
}
.waveText span:nth-child(4) {
    animation-delay:0.4s;
}
.waveText span:nth-child(5) {
    animation-delay:0.5s;
}
.waveText span:nth-child(6) {
    animation-delay:0.6s;
}
.waveText span:nth-child(7) {
    animation-delay:0.7s;
}
.waveText span:nth-child(8) {
    animation-delay:0.8s;
}
.waveText span:nth-child(9) {
    animation-delay:0.9s;
}
.waveText span:nth-child(10) {
    animation-delay:1s;
}


================================================
FILE: storytelling-chatbot/server/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import asyncio
import os
import sys

import aiohttp
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import EndFrame, LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.services.elevenlabs.tts import ElevenLabsTTSService
from pipecat.services.google.image import GoogleImageGenService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.daily.transport import (
    DailyParams,
    DailyTransport,
    DailyTransportMessageFrame,
)
from processors import StoryImageProcessor, StoryProcessor
from prompts import CUE_USER_TURN, LLM_BASE_PROMPT
from utils.helpers import load_images, load_sounds

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")

sounds = load_sounds(["listening.wav"])
images = load_images(["book1.png", "book2.png"])


async def main(room_url, token=None):
    async with aiohttp.ClientSession() as session:
        # -------------- Transport --------------- #

        transport = DailyTransport(
            room_url,
            token,
            "Storytelling Bot",
            DailyParams(
                audio_in_enabled=True,
                audio_out_enabled=True,
                video_out_enabled=True,
                video_out_width=1024,
                video_out_height=1024,
                transcription_enabled=True,
                vad_analyzer=SileroVADAnalyzer(),
            ),
        )

        logger.debug("Transport created for room:" + room_url)

        # -------------- Services --------------- #

        llm_service = GoogleLLMService(api_key=os.getenv("GOOGLE_API_KEY"))

        tts_service = ElevenLabsTTSService(
            api_key=os.getenv("ELEVENLABS_API_KEY"), voice_id=os.getenv("ELEVENLABS_VOICE_ID")
        )

        image_gen = GoogleImageGenService(api_key=os.getenv("GOOGLE_API_KEY"))

        # --------------- Setup ----------------- #

        message_history = [LLM_BASE_PROMPT]
        story_pages = []

        # We need aggregators to keep track of user and LLM responses
        context = LLMContext(message_history)
        context_aggregator = LLMContextAggregatorPair(context)

        # -------------- Processors ------------- #

        story_processor = StoryProcessor(message_history, story_pages)
        image_processor = StoryImageProcessor(image_gen)

        # -------------- Story Loop ------------- #

        runner = PipelineRunner()

        logger.debug("Waiting for participant...")
        main_pipeline = Pipeline(
            [
                transport.input(),
                context_aggregator.user(),
                llm_service,
                story_processor,
                image_processor,
                tts_service,
                transport.output(),
                context_aggregator.assistant(),
            ]
        )

        main_task = PipelineTask(
            main_pipeline,
            params=PipelineParams(
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
        )

        @transport.event_handler("on_first_participant_joined")
        async def on_first_participant_joined(transport, participant):
            logger.debug("Participant joined, storytime commence!")
            await transport.capture_participant_transcription(participant["id"])
            await main_task.queue_frames(
                [
                    images["book1"],
                    LLMRunFrame(),
                    DailyTransportMessageFrame(CUE_USER_TURN),
                    # sounds["listening"],
                    images["book2"],
                ]
            )

        @transport.event_handler("on_participant_left")
        async def on_participant_left(transport, participant, reason):
            await main_task.cancel()

        @transport.event_handler("on_call_state_updated")
        async def on_call_state_updated(transport, state):
            if state == "left":
                # Here we don't want to cancel, we just want to finish sending
                # whatever is queued, so we use an EndFrame().
                await main_task.queue_frame(EndFrame())

        await runner.run(main_task)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Daily Storyteller Bot")
    parser.add_argument("-u", type=str, help="Room URL")
    parser.add_argument("-t", type=str, help="Token")
    config = parser.parse_args()

    asyncio.run(main(config.u, config.t))



================================================
FILE: storytelling-chatbot/server/bot_runner.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os
import subprocess
from contextlib import asynccontextmanager
from pathlib import Path
from typing import Optional

import aiohttp
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pipecat.transports.daily.utils import (
    DailyRESTHelper,
    DailyRoomObject,
    DailyRoomParams,
    DailyRoomProperties,
)

load_dotenv(override=True)

# ------------ Fast API Config ------------ #

MAX_SESSION_TIME = 5 * 60  # 5 minutes

daily_helpers = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    aiohttp_session = aiohttp.ClientSession()
    daily_helpers["rest"] = DailyRESTHelper(
        daily_api_key=os.getenv("DAILY_API_KEY", ""),
        daily_api_url=os.getenv("DAILY_API_URL", "https://api.daily.co/v1"),
        aiohttp_session=aiohttp_session,
    )
    yield
    await aiohttp_session.close()


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mount the static directory
STATIC_DIR = "client/out"


# ------------ Fast API Routes ------------ #

app.mount("/static", StaticFiles(directory=STATIC_DIR, html=True), name="static")


@app.post("/")
async def start_bot(request: Request) -> JSONResponse:
    if os.getenv("ENV", "dev") == "production":
        # Only allow requests from the specified domain
        host_header = request.headers.get("host")
        allowed_domains = ["storytelling-chatbot.fly.dev", "www.storytelling-chatbot.fly.dev"]
        # Check if the Host header matches the allowed domain
        if host_header not in allowed_domains:
            raise HTTPException(status_code=403, detail="Access denied")

    try:
        data = await request.json()
        # Is this a webhook creation request?
        if "test" in data:
            return JSONResponse({"test": True})
    except Exception as e:
        pass

    # Use specified room URL, or create a new one if not specified
    room_url = os.getenv("DAILY_SAMPLE_ROOM_URL", "")

    if not room_url:
        params = DailyRoomParams(properties=DailyRoomProperties())
        try:
            room: DailyRoomObject = await daily_helpers["rest"].create_room(params=params)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Unable to provision room {e}")
    else:
        # Check passed room URL exists, we should assume that it already has a sip set up
        try:
            room: DailyRoomObject = await daily_helpers["rest"].get_room_from_url(room_url)
        except Exception:
            raise HTTPException(status_code=500, detail=f"Room not found: {room_url}")

    # Give the agent a token to join the session
    token = await daily_helpers["rest"].get_token(room.url, MAX_SESSION_TIME)

    if not room or not token:
        raise HTTPException(status_code=500, detail=f"Failed to get token for room: {room_url}")

    # Launch a new VM, or run as a shell process (not recommended)
    if os.getenv("RUN_AS_VM", False):
        try:
            await virtualize_bot(room.url, token)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to spawn VM: {e}")
    else:
        try:
            subprocess.Popen(
                [f"python -m bot -u {room.url} -t {token}"],
                shell=True,
                bufsize=1,
                cwd=os.path.dirname(os.path.abspath(__file__)),
            )
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to start subprocess: {e}")

    # Grab a token for the user to join with
    user_token = await daily_helpers["rest"].get_token(room.url, MAX_SESSION_TIME)

    return JSONResponse(
        {
            "room_url": room.url,
            "token": user_token,
        }
    )


@app.get("/{path_name:path}", response_class=FileResponse)
async def catch_all(path_name: Optional[str] = ""):
    if path_name == "":
        return FileResponse(f"{STATIC_DIR}/index.html")

    file_path = Path(STATIC_DIR) / (path_name or "")

    if file_path.is_file():
        return file_path

    html_file_path = file_path.with_suffix(".html")
    if html_file_path.is_file():
        return FileResponse(html_file_path)

    raise HTTPException(status_code=450, detail="Incorrect API call")


# ------------ Virtualization ------------ #


async def virtualize_bot(room_url: str, token: str):
    """This is an example of how to virtualize the bot using Fly.io
    You can adapt this method to use whichever cloud provider you prefer.
    """
    FLY_API_HOST = os.getenv("FLY_API_HOST", "https://api.machines.dev/v1")
    FLY_APP_NAME = os.getenv("FLY_APP_NAME", "storytelling-chatbot")
    FLY_API_KEY = os.getenv("FLY_API_KEY", "")
    FLY_HEADERS = {"Authorization": f"Bearer {FLY_API_KEY}", "Content-Type": "application/json"}

    async with aiohttp.ClientSession() as session:
        # Use the same image as the bot runner
        async with session.get(
            f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines", headers=FLY_HEADERS
        ) as r:
            if r.status != 200:
                text = await r.text()
                raise Exception(f"Unable to get machine info from Fly: {text}")

            data = await r.json()
            image = data[0]["config"]["image"]

        # Machine configuration
        cmd = f"python server/bot.py -u {room_url} -t {token}"
        cmd = cmd.split()
        worker_props = {
            "config": {
                "image": image,
                "auto_destroy": True,
                "init": {"cmd": cmd},
                "restart": {"policy": "no"},
                "guest": {"cpu_kind": "shared", "cpus": 1, "memory_mb": 512},
            },
        }

        # Spawn a new machine instance
        async with session.post(
            f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines", headers=FLY_HEADERS, json=worker_props
        ) as r:
            if r.status != 200:
                text = await r.text()
                raise Exception(f"Problem starting a bot worker: {text}")

            data = await r.json()
            # Wait for the machine to enter the started state
            vm_id = data["id"]

        async with session.get(
            f"{FLY_API_HOST}/apps/{FLY_APP_NAME}/machines/{vm_id}/wait?state=started",
            headers=FLY_HEADERS,
        ) as r:
            if r.status != 200:
                text = await r.text()
                raise Exception(f"Bot was unable to enter started state: {text}")

        print(f"Machine joined room: {room_url}")


# ------------ Main ------------ #

if __name__ == "__main__":
    # Check environment variables
    required_env_vars = [
        "GOOGLE_API_KEY",
        "DAILY_API_KEY",
        "ELEVENLABS_VOICE_ID",
        "ELEVENLABS_API_KEY",
    ]
    for env_var in required_env_vars:
        if env_var not in os.environ:
            raise Exception(f"Missing environment variable: {env_var}.")

    import uvicorn

    default_host = os.getenv("HOST", "0.0.0.0")
    default_port = int(os.getenv("FAST_API_PORT", "7860"))

    parser = argparse.ArgumentParser(description="Daily Storyteller FastAPI server")
    parser.add_argument("--host", type=str, default=default_host, help="Host address")
    parser.add_argument("--port", type=int, default=default_port, help="Port number")
    parser.add_argument("--reload", action="store_true", help="Reload code on change")

    config = parser.parse_args()

    uvicorn.run("bot_runner:app", host=config.host, port=config.port, reload=config.reload)



================================================
FILE: storytelling-chatbot/server/Dockerfile
================================================
FROM python:3.11-slim-bookworm

ARG DEBIAN_FRONTEND=noninteractive
ARG USE_PERSISTENT_DATA
ENV PYTHONUNBUFFERED=1
ENV NODE_MAJOR=20

# Expose FastAPI port
ENV FAST_API_PORT=7860
EXPOSE 7860

# Install system dependencies
RUN apt-get update && apt-get install --no-install-recommends -y \
    build-essential \
    git \
    ffmpeg \
    google-perftools \
    ca-certificates curl gnupg \
    && apt-get clean && rm -rf /var/lib/apt/lists/*

# Install Node.js
RUN mkdir -p /etc/apt/keyrings 
RUN curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg
RUN echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_${NODE_MAJOR}.x nodistro main" | tee /etc/apt/sources.list.d/nodesource.list > /dev/null
RUN apt-get update && apt-get install nodejs -y

# Set up a new user named "user" with user ID 1000
RUN useradd -m -u 1000 user

# Set home to the user's home directory
ENV HOME=/home/user \
    PATH=/home/user/.local/bin:$PATH \
    PYTHONPATH=$HOME/app \
    PYTHONUNBUFFERED=1

# Switch to the "user" user
USER user

# Set the working directory to the user's home directory
WORKDIR $HOME/app

# Install Python dependencies
COPY ./requirements.txt requirements.txt
RUN pip3 install --no-cache-dir --upgrade -r requirements.txt

# Copy everything else
COPY --chown=user ./server/ server/

# Copy client app and build
COPY --chown=user ./client/ client/
RUN cd client && npm install && npm run build

# Start the FastAPI server
CMD python3 server/bot_runner.py --port ${FAST_API_PORT}


================================================
FILE: storytelling-chatbot/server/env.example
================================================
DAILY_API_KEY=
DAILY_SAMPLE_ROOM_URL=
ELEVENLABS_API_KEY=
ELEVENLABS_VOICE_ID=
GOOGLE_API_KEY=
ENV=dev



================================================
FILE: storytelling-chatbot/server/processors.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
import re

import google.ai.generativelanguage as glm
from async_timeout import timeout
from loguru import logger
from pipecat.frames.frames import (
    Frame,
    LLMFullResponseEndFrame,
    TextFrame,
    UserStoppedSpeakingFrame,
)
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.daily.transport import DailyTransportMessageFrame
from prompts import (
    CUE_ASSISTANT_TURN,
    CUE_USER_TURN,
    FIRST_IMAGE_PROMPT,
    IMAGE_GEN_PROMPT,
    NEXT_IMAGE_PROMPT,
)
from utils.helpers import load_sounds

sounds = load_sounds(["talking.wav", "listening.wav", "ding.wav"])

# -------------- Frame Types ------------- #


class StoryPageFrame(TextFrame):
    # Frame for each sentence in the story before a [break]
    pass


class StoryImageFrame(TextFrame):
    # Frame for trigger image generation
    pass


class StoryPromptFrame(TextFrame):
    # Frame for prompting the user for input
    pass


# ------------ Frame Processors ----------- #


class StoryImageProcessor(FrameProcessor):
    """Processor for image prompt frames that will be sent to the FAL service.

    This processor is responsible for consuming frames of type `StoryImageFrame`.
    It processes them by passing it to the FAL service.
    The processed frames are then yielded back.

    Attributes:
        _image_gen_service: The FAL service, generates the images (fast fast!).
    """

    def __init__(self, image_gen_service):
        super().__init__()
        self._image_gen_service = image_gen_service
        # Create a new LLM service to use a different system prompt, etc
        self._llm_service = GoogleLLMService(api_key=os.getenv("GOOGLE_API_KEY"))

        self.pages = []
        self.image_descriptions = []

    def can_generate_metrics(self) -> bool:
        return True

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, StoryPageFrame):
            # Special syntax for the first page
            if self.pages == []:
                prompt = FIRST_IMAGE_PROMPT % frame.text
            else:
                prompt = NEXT_IMAGE_PROMPT % (
                    " ".join(self.pages),
                    "; ".join(self.image_descriptions),
                    frame.text,
                )

            await self.start_ttfb_metrics()
            # TODO: This is coupled to google implementation now
            txt = glm.Content(role="user", parts=[glm.Part(text=prompt)])
            llm_response = await self._llm_service._client.generate_content_async(
                contents=[txt], stream=False
            )
            image_description = llm_response.text
            self.pages.append(frame.text)
            self.image_descriptions.append(image_description)
            try:
                async with timeout(15):
                    async for i in self._image_gen_service.run_image_gen(
                        IMAGE_GEN_PROMPT % image_description
                    ):
                        await self.push_frame(i)
            except TimeoutError:
                logger.debug("Image gen timeout")
                pass
            await self.stop_ttfb_metrics()
            # Push the StoryPageFrame so it gets TTS
            await self.push_frame(frame)
        else:
            await self.push_frame(frame)


class StoryProcessor(FrameProcessor):
    """Primary frame processor. It takes the frames generated by the LLM
    and processes them into image prompts and story pages (sentences).
    For a clearer picture of how this works, reference prompts.py

    Attributes:
        _messages (list): A list of llm messages.
        _text (str): A buffer to store the text from text frames.
        _story (list): A list to store the story sentences, or 'pages'.

    Methods:
        process_frame: Processes a frame and removes any [break] or [image] tokens.
    """

    def __init__(self, messages, story):
        super().__init__()
        self._messages = messages
        self._text = ""
        self._story = story

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if isinstance(frame, UserStoppedSpeakingFrame):
            # Send an app message to the UI
            await self.push_frame(DailyTransportMessageFrame(CUE_ASSISTANT_TURN))
            await self.push_frame(sounds["talking"])

        elif isinstance(frame, TextFrame):
            # Add new text to the buffer
            # (character replace hack to fix TTS sequencing)
            self._text += frame.text.replace(";", "—")
            # Process any complete patterns in the order they appear
            await self.process_text_content()

        # End of a full LLM response
        # Driven by the prompt, the LLM should have asked the user for input
        elif isinstance(frame, LLMFullResponseEndFrame):
            # We use a different frame type, as to avoid image generation ingest
            await self.push_frame(StoryPromptFrame(self._text))
            self._text = ""
            await self.push_frame(frame)
            # Send an app message to the UI
            await self.push_frame(DailyTransportMessageFrame(CUE_USER_TURN))
            await self.push_frame(sounds["listening"])

        # Anything that is not a TextFrame pass through
        else:
            await self.push_frame(frame)

    async def process_text_content(self):
        """Process text content in order of appearance, handling both image prompts and story breaks."""
        while True:
            # Find the first occurrence of each pattern
            image_match = re.search(r"<(.*?)>", self._text)
            break_match = re.search(r"\[[bB]reak\]", self._text)

            # If neither pattern is found, we're done processing
            if not image_match and not break_match:
                break

            # Find which pattern comes first in the text
            image_pos = image_match.start() if image_match else float("inf")
            break_pos = break_match.start() if break_match else float("inf")

            if image_pos < break_pos:
                # Process image prompt first
                image_prompt = image_match.group(1)
                # Remove the image prompt from the text
                self._text = self._text[: image_match.start()] + self._text[image_match.end() :]
                await self.push_frame(StoryImageFrame(image_prompt))
            else:
                # Process story break first
                parts = re.split(r"\[[bB]reak\]", self._text, flags=re.IGNORECASE, maxsplit=1)
                before_break = parts[0].replace("\n", " ").strip()

                if len(before_break) > 2:
                    self._story.append(before_break)
                    await self.push_frame(StoryPageFrame(before_break))
                    # await self.push_frame(sounds["ding"])
                    await self.push_frame(DailyTransportMessageFrame(CUE_ASSISTANT_TURN))

                # Keep the remainder (if any) in the buffer
                self._text = parts[1].strip() if len(parts) > 1 else ""



================================================
FILE: storytelling-chatbot/server/prompts.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

LLM_BASE_PROMPT = {
    "role": "system",
    "content": """You are a creative storyteller who loves tell whimsical, fantastical stories.
        Your goal is to craft an engaging and fun story.
        Keep all responses short and no more than a few sentences.
        Start by asking the user what kind of story they'd like to hear. Don't provide any examples.
        After they've answered the question, start telling the story. Include three story sentences in your response. Add [break] after each sentence of the story.

        EXAMPLE OUTPUT FORMAT:
        story sentence 1 [break]
        story sentence 2 [break]
        story sentence 3 [break]
        How would you like the story to continue?
        END OF EXAMPLE OUTPUT

        Generate three story sentences, then ask what should happen next and wait for my input. You can propose an idea for how the story should proceed, but make sure to tell me I can suggest whatever I want.
        Please ensure your responses are less than 5 sentences long.
        Please refrain from using any explicit language or content. Do not tell scary stories.
        Once you've started telling the story, EVERY RESPONSE should follow the story sentence output format. It is VERY IMPORTANT that you continue to include [break] between story sentences. DO NOT RESPOND without story sentences and break tags.""",
}


IMAGE_GEN_PROMPT = "an illustration of %s. colorful, whimsical, painterly, concept art."

CUE_USER_TURN = {"cue": "user_turn"}
CUE_ASSISTANT_TURN = {"cue": "assistant_turn"}


"""        Start each sentence with an image prompt, wrapped in triangle braces, that I can use to generate an illustration representing the upcoming scene.
        Image prompts should always be wrapped in triangle braces, like this: <image prompt goes here>.
        You should provide as much descriptive detail in your image prompt as you can to help recreate the current scene depicted by the sentence.
        For any recurring characters, you should provide a description of them in the image prompt each time, for example: <a brown fluffy dog ...>.
        Please do not include any character names in the image prompts, just their descriptions.
        Image prompts should focus on key visual attributes of all characters each time, for example <a brown fluffy dog and the tiny red cat ...>.
        Please use the following structure for your image prompts: characters, setting, action, and mood.
        Image prompts should be less than 150-200 characters and start in lowercase."""

FIRST_IMAGE_PROMPT = """You are creating a prompt to generate an image for a child's story book.

You should provide as much descriptive detail in your image prompt as you can to help recreate the current scene depicted by the sentence.
For any recurring characters, you should provide a description of them in the image prompt each time, for example: <a brown fluffy dog ...>.
Please do not include any character names in the image prompts, just their descriptions.
Image prompts should focus on key visual attributes of all characters each time, for example <a brown fluffy dog and the tiny red cat ...>.
Please use the following structure for your image prompts: characters, setting, action, and mood.
Image prompts should be less than 150-200 characters and start in lowercase.


Here's the first page of the story:
%s
"""

NEXT_IMAGE_PROMPT = """You are creating a prompt to generate an image for a child's story book.

Here is the text of the story so far:
%s

Here are the previous image prompts:
%s

You should provide as much descriptive detail in your image prompt as you can to help recreate the current scene depicted by the sentence.
For any recurring characters, you should try to use the same description of them in the image prompt each time.
Please do not include any character names in the image prompts, just their descriptions.
Image prompts should focus on key visual attributes of all characters each time, for example <a brown fluffy dog and the tiny red cat ...>.
Please use the following structure for your image prompts: characters, setting, action, and mood.
Image prompts should be less than 150-200 characters and start in lowercase.
Here's the next page of the story:
%s
"""



================================================
FILE: storytelling-chatbot/server/requirements.txt
================================================
async_timeout
fastapi
uvicorn
python-dotenv
pipecat-ai[daily,silero,openai,cartesia,google]>=0.0.82


================================================
FILE: storytelling-chatbot/server/.dockerignore
================================================
client/node_modules
client/out


================================================
FILE: storytelling-chatbot/server/utils/helpers.py
================================================
import os
import wave

from PIL import Image
from pipecat.frames.frames import OutputAudioRawFrame, OutputImageRawFrame

script_dir = os.path.dirname(__file__)


def load_images(image_files):
    images = {}
    for file in image_files:
        # Build the full path to the image file
        full_path = os.path.join(script_dir, "../assets", file)
        # Get the filename without the extension to use as the dictionary key
        filename = os.path.splitext(os.path.basename(full_path))[0]
        # Open the image and convert it to bytes
        with Image.open(full_path) as img:
            images[filename] = OutputImageRawFrame(
                image=img.tobytes(), size=img.size, format=img.format
            )
    return images


def load_sounds(sound_files):
    sounds = {}

    for file in sound_files:
        # Build the full path to the sound file
        full_path = os.path.join(script_dir, "../assets", file)
        # Get the filename without the extension to use as the dictionary key
        filename = os.path.splitext(os.path.basename(full_path))[0]
        # Open the sound and convert it to bytes
        with wave.open(full_path) as audio_file:
            sounds[filename] = OutputAudioRawFrame(
                audio=audio_file.readframes(-1),
                sample_rate=audio_file.getframerate(),
                num_channels=audio_file.getnchannels(),
            )

    return sounds



================================================
FILE: studypal/README.md
================================================
# studypal

### Have a conversation about any article on the web

studypal is a fast conversational AI built using [Daily](https://www.daily.co/) or SmallWebRTC for real-time media transport, [Deepgram](https://deepgram.com/) for speech-to-text, [OpenAI](https://openai.com/) for LLM inference, and [Cartesia](https://cartesia.ai) for text-to-speech. Everything is orchestrated together (VAD -> STT -> LLM -> TTS) using [Pipecat](https://www.pipecat.ai/).

## Setup

1. Clone the repository
2. Copy `env.example` to a `.env` file and add API keys
3. Install the required packages: `uv sync`
4. Run from your command line:

   - Daily: `uv run studypal.py -t daily`
   - SmallWebRTC: `uv run studypal.py`

5. Connect using your browser by clicking on the link generating in the console.



================================================
FILE: studypal/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#


import io
import os

import aiohttp
import tiktoken
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams
from pypdf import PdfReader

load_dotenv(override=True)

# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
}


# Count number of tokens used in model and truncate the content
def truncate_content(content, model_name):
    encoding = tiktoken.encoding_for_model(model_name)
    tokens = encoding.encode(content)

    max_tokens = 10000
    if len(tokens) > max_tokens:
        truncated_tokens = tokens[:max_tokens]
        return encoding.decode(truncated_tokens)
    return content


# Main function to extract content from url


async def get_article_content(url: str, aiohttp_session: aiohttp.ClientSession):
    if "arxiv.org" in url:
        return await get_arxiv_content(url, aiohttp_session)
    else:
        return await get_wikipedia_content(url, aiohttp_session)


# Helper function to extract content from Wikipedia url (this is
# technically agnostic to URL type but will work best with Wikipedia
# articles)


async def get_wikipedia_content(url: str, aiohttp_session: aiohttp.ClientSession):
    async with aiohttp_session.get(url) as response:
        if response.status != 200:
            return "Failed to download Wikipedia article."

        text = await response.text()
        soup = BeautifulSoup(text, "html.parser")

        content = soup.find("div", {"class": "mw-parser-output"})

        if content:
            return content.get_text()
        else:
            return "Failed to extract Wikipedia article content."


# Helper function to extract content from arXiv url


async def get_arxiv_content(url: str, aiohttp_session: aiohttp.ClientSession):
    if "/abs/" in url:
        url = url.replace("/abs/", "/pdf/")
    if not url.endswith(".pdf"):
        url += ".pdf"

    async with aiohttp_session.get(url) as response:
        if response.status != 200:
            return "Failed to download arXiv PDF."

        content = await response.read()
        pdf_file = io.BytesIO(content)
        pdf_reader = PdfReader(pdf_file)
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    url = input("Enter the URL of the article you would like to talk about: ")

    async with aiohttp.ClientSession() as session:
        article_content = await get_article_content(url, session)
        article_content = truncate_content(article_content, model_name="gpt-4o-mini")

        stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

        tts = CartesiaTTSService(
            api_key=os.getenv("CARTESIA_API_KEY"),
            voice_id=os.getenv("CARTESIA_VOICE_ID", "4d2fd738-3b3d-4368-957a-bb4805275bd9"),
            # British Narration Lady: 4d2fd738-3b3d-4368-957a-bb4805275bd9
        )

        llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"), model="gpt-4o-mini")

        messages = [
            {
                "role": "system",
                "content": f"""You are an AI study partner. You have been given the following article content:

    {article_content}

    Your task is to help the user understand and learn from this article in 2 sentences. THESE RESPONSES SHOULD BE ONLY MAX 2 SENTENCES. THIS INSTRUCTION IS VERY IMPORTANT. RESPONSES SHOULDN'T BE LONG.
    """,
            },
        ]

        context = LLMContext(messages)
        context_aggregator = LLMContextAggregatorPair(context)

        pipeline = Pipeline(
            [
                transport.input(),  # Transport user input
                stt,
                context_aggregator.user(),  # User responses
                llm,  # LLM
                tts,  # TTS
                transport.output(),  # Transport bot output
                context_aggregator.assistant(),  # Assistant spoken responses
            ]
        )

        task = PipelineTask(
            pipeline,
            params=PipelineParams(
                audio_out_sample_rate=44100,
                enable_metrics=True,
                enable_usage_metrics=True,
            ),
            idle_timeout_secs=runner_args.pipeline_idle_timeout_secs,
        )

        @transport.event_handler("on_client_connected")
        async def on_client_connected(transport, client):
            logger.info(f"Client connected")
            # Kick off the conversation.
            messages.append(
                {
                    "role": "system",
                    "content": "Hello! I'm ready to discuss the article with you. What would you like to learn about?",
                }
            )
            await task.queue_frames([LLMRunFrame()])

        @transport.event_handler("on_client_disconnected")
        async def on_client_disconnected(transport, client):
            logger.info(f"Client disconnected")
            await task.cancel()

        runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

        await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: studypal/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: studypal/env.example
================================================
DAILY_SAMPLE_ROOM_URL= # Follow instructions here and put your https://YOURDOMAIN.daily.co/YOURROOM (Instructions: https://docs.pipecat.ai/getting-started/installation)
DAILY_API_KEY= # Create here: https://dashboard.daily.co/developers 
OPENAI_API_KEY= # Create here: https://platform.openai.com/docs/overview
CARTESIA_API_KEY= # Create here: https://play.cartesia.ai/console
CARTESIA_VOICE_ID= # Find here: https://play.cartesia.ai/
DEEPGRAM_API_KEY= # Create here: https://console.deepgram.com/


================================================
FILE: studypal/pcc-deploy.toml
================================================
agent_name = "studypal"
image = "your_username/studypal:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "studypal-secrets"

[scaling]
	min_agents = 1



================================================
FILE: studypal/pyproject.toml
================================================
[project]
name = "studypal"
version = "0.1.0"
description = "Studypal example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[daily,webrtc,websocket,cartesia,openai,silero,deepgram,runner,local-smart-turn-v3]>=0.0.91",
  "pipecatcloud>=0.2.7",
  "beautifulsoup4==4.12.3",
  "pypdf==4.3.1",
  "tiktoken==0.7.0"
]



================================================
FILE: telnyx-chatbot/README.md
================================================
# Telnyx Voice Bot Examples

This repository contains examples of voice bots that integrate with Telnyx's Voice API using Pipecat. The examples demonstrate both inbound and outbound calling scenarios using Telnyx's TeXML and WebSocket streaming for real-time audio processing.

## Examples

### 🔽 [Inbound Calling](./inbound/)

Demonstrates how to handle incoming phone calls where users call your Telnyx number and interact with a voice bot.

### 🔼 [Outbound Calling](./outbound/)

Shows how to initiate outbound phone calls programmatically where your bot calls users.

## Architecture

Both examples use the same core architecture:

```
Phone Call ↔ Telnyx ↔ TeXML/WebSocket ↔ Pipecat ↔ AI Services
```

**Components:**

- **Telnyx**: Handles phone call routing and audio transport
- **TeXML**: XML-based call control and WebSocket streaming
- **Pipecat**: Audio processing pipeline and AI service orchestration
- **AI Services**: OpenAI (LLM), Deepgram (STT), Cartesia (TTS)

## Getting Help

- **Detailed Setup**: See individual README files in `inbound/` and `outbound/` directories
- **Pipecat Documentation**: [docs.pipecat.ai](https://docs.pipecat.ai)
- **Telnyx Documentation**: [developers.telnyx.com](https://developers.telnyx.com)



================================================
FILE: telnyx-chatbot/inbound/README.md
================================================
# Telnyx Chatbot: Inbound

This project is a Pipecat-based chatbot that integrates with Telnyx to handle WebSocket connections and provide real-time communication. The project includes FastAPI endpoints for starting a call and handling WebSocket connections.

## Table of Contents

- [Telnyx Chatbot](#telnyx-chatbot)
  - [Table of Contents](#table-of-contents)
  - [Features](#features)
  - [Requirements](#requirements)
  - [Installation](#installation)
  - [Configure Telnyx TeXML application](#configure-telnyx-texml-application)
  - [Running the Application](#running-the-application)
    - [Using Python (Option 1)](#using-python-option-1)
    - [Using Docker (Option 2)](#using-docker-option-2)
  - [Usage](#usage)

## Prerequisites

### Telnyx

- A Telnyx account with:
  - API Key
  - A purchased phone number that supports voice calls

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

   ```sh
   cd inbound
   uv sync
   ```

2. Create an .env file and add API keys:

   ```sh
   cp env.example .env
   ```

## Local development

To run the twilio-chatbot for inbound calling locally, we'll first set up an ngrok tunnel and a TeXML application. Then, we'll run our bot and call into it to speak with the bot.

### Configure Telnyx TeXML application

1. Start ngrok:
   In a new terminal, start ngrok to tunnel the local server:

   ```sh
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

2. If you haven't already, purchase a number from Telnyx.

   - Log in to the Telnyx developer portal: https://portal.telnyx.com/
   - Buy a number: https://portal.telnyx.com/#/numbers/buy-numbers

3. Create your TeXML Bin:

   - Go to your TeXML Bin configuration page: https://portal.telnyx.com/#/call-control/texml-bin
   - Create a new TeXML Bin
   - In the "Name" field, provide a name
   - Leave the "URL" field blank
   - In the "Content" field, add the TeXML:

     ```xml
      <?xml version="1.0" encoding="UTF-8"?>
         <Response>
         <Connect>
            <Stream url="wss://your-url.ngrok.io/ws" bidirectionalMode="rtp"></Stream>
         </Connect>
         <Pause length="40"/>
      </Response>
     ```

4. Create a Telnyx TeXML application:

   - Go to your TeXML configuration page: https://portal.telnyx.com/#/call-control/texml
   - Create a new TeXML app, if one doesn't exist already:
     - Add an application name
     - Under Webhooks, select POST as the "Voice Method"
     - Select "TeXML Bin URL" under Webhook URL Method
     - Select the TeXML Bin you created in the previous step
     - Click "Create" to save
       Note: You'll see subsequent pages to set up SIP and Outbound, both are not required, so just skip.

5. Add TeXML application to your number:
   - Navigate to "Manage Numbers" (https://portal.telnyx.com/#/numbers/my-numbers)
   - Select the pencil icon next to the phone number of interest and select the TeXML application that you just created

### Run your bot

The bot.py file uses the Pipecat development runner, which runs a FastAPI server in order to receive connections.

1. To get started, we'll run our bot.py file:

   ```bash
   uv run bot.py --transport telnyx --proxy your_ngrok_url
   ```

   > Replace `your_ngrok_url` with your ngrok URL (e.g. your-subdomain.ngrok.io)

2. Call the number associated with your TeXML applicaiton. Your bot will answer and begin talking to you!

## Production deployment

To deploy your telnyx-chatbot for inbound calling, we'll use [Pipecat Cloud](https://pipecat.daily.co/).

### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

### Configure Telnyx for production

You'll want to purchase additional phone numbers for your production use. You can follow the steps above.

To run on Pipecat Cloud, the one change you'll have to make is to your TeXML Bin. You'll need to update the websocket URL to point to Pipecat Cloud. Your TeXML should look like this:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Response>
   <Connect>
      <Stream url="wss://api.pipecat.daily.co/ws/telnyx?serviceHost=AGENT_NAME.ORGANIZATION_NAME" bidirectionalMode="rtp"></Stream>
   </Connect>
   <Pause length="40"/>
</Response>
```

Replace:

- `AGENT_NAME` with the name of the agent you deployed in the previous stepyour deployed agent name
- `ORGANIZATION_NAME` with your Pipecat Cloud organization name

> If the bot is deployed to a region other than us-west (default), update the websocket url with region. For example, if deployed in `eu-central`, the url becomes `"wss://eu-central.api.pipecat.daily.co/ws/telnyx?serviceHost=AGENT_NAME.ORGANIZATION_NAME"`

### Call your Bot

Your bot file is now deployed to Pipecat Cloud and Telnyx is configured to receive calls. Dial the number associated with your bot to start a conversation!



================================================
FILE: telnyx-chatbot/inbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.telnyx import TelnyxFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in an audio call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    _, call_data = await parse_telephony_websocket(runner_args.websocket)
    from_number = call_data["from"]

    # Extract the from number from the call data, which allows you to identify the caller.
    # With this information, you can make a request to your API to get the user's information
    # and inject that information into your bot's configuration.
    logger.info(f"From number: {from_number}")

    serializer = TelnyxFrameSerializer(
        stream_id=call_data["stream_id"],
        outbound_encoding=call_data["outbound_encoding"],
        inbound_encoding="PCMU",
        call_control_id=call_data["call_control_id"],
        api_key=os.getenv("TELNYX_API_KEY"),
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: telnyx-chatbot/inbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: telnyx-chatbot/inbound/env.example
================================================
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=

# Telnyx API key (optional for hang-up functionality)
TELNYX_API_KEY=


================================================
FILE: telnyx-chatbot/inbound/pcc-deploy.toml
================================================
agent_name = "telnyx-chatbot-dial-in"
image = "your_username/telnyx-chatbot-dial-in:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "telnyx-chatbot"
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: telnyx-chatbot/inbound/pyproject.toml
================================================
[project]
name = "telnyx-chatbot-dial-in"
version = "0.1.0"
description = "Telnyx dial-in example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,openai,silero,deepgram,runner]>=0.0.90",
  "pipecatcloud>=0.2.7"
]



================================================
FILE: telnyx-chatbot/outbound/README.md
================================================
# Telnyx Chatbot: Outbound

This project is a Pipecat-based chatbot that integrates with Telnyx to make outbound calls with custom data injection. The project includes FastAPI endpoints for initiating outbound calls and handling WebSocket connections with custom data passed through the `body` parameter.

## How it works

1. The server receives a POST request with a phone number and optional custom data (`body`)
2. The server encodes the custom data as base64 JSON and includes it in the TeXML URL
3. The server uses Telnyx's REST API to initiate an outbound call
4. When the call is answered, Telnyx fetches TeXML from the server (with custom data in query parameters)
5. The TeXML connects the call to a WebSocket with custom data passed through URL parameters
6. The bot receives the decoded custom data and engages in personalized conversation

## Architecture

```
curl request (with body) → /start endpoint → Telnyx REST API → Call initiated →
TeXML fetched (with body in URL) → WebSocket connection (body decoded) → Bot conversation (with custom data)
```

## Prerequisites

### Telnyx

- A Telnyx account with:
  - API Key
  - A purchased phone number that supports voice calls

### AI Services

- OpenAI API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

```bash
cd outbound
uv sync
```

2. Get your Telnyx credentials:

- **API Key**: Found in your [Telnyx Mission Control Portal](https://portal.telnyx.com/)
- **Account SID**: Get your account_sid by running:

  ```bash
  curl -H "Authorization: Bearer YOUR_API_KEY" https://api.telnyx.com/v2/whoami
  ```

  The `organization_id` in the response is your `TELNYX_ACCOUNT_SID` which you'll add to your `.env` file.

- **Phone Number**: [Purchase a phone number](https://portal.telnyx.com/#/numbers/buy-numbers) that supports voice calls

3. Set up environment variables:

```bash
cp env.example .env
# Edit .env with your API keys
```

> Note: We'll create the TeXML application below which will provide the `TELNYX_APPLICATION_SID` for your `.env` file.

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server or ngrok URL for WebSocket connections
- Default configuration for development and testing
- WebSocket connections go directly to your running server

### Production (`ENV=production`)

- Uses Pipecat Cloud WebSocket URLs automatically
- Requires `AGENT_NAME` and `ORGANIZATION_NAME` from your Pipecat Cloud deployment
- Set these when deploying to production environments
- WebSocket connections route through Pipecat Cloud infrastructure

## Local Development

1. Start the outbound bot server:

   ```bash
   uv run server.py
   ```

The server will start on port 7860.

2. Using a new terminal, expose your server to the internet (for development)

   ```bash
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

   Copy the ngrok URL (e.g., `https://abc123.ngrok.io`)

3. **Configure your TeXML Application**

   - Go to your TeXML configuration page: https://portal.telnyx.com/#/call-control/texml
   - Create a new TeXML app, if one doesn't exist already:
     - Add an application name
     - Under Webhooks, select POST as the "Voice Method"
     - Select "Custom URL" under Webhook URL Method
     - Enter your ngrok URL in the "Webhook URL" field (e.g. https://your-name.ngrok.io)
     - Click "Create" to save
   - Find the **Application ID** on the page. Save this as your `TELNYX_APPLICATION_SID` in your `.env` file.
   - Navigate to "Manage Numbers" (https://portal.telnyx.com/#/numbers/my-numbers) and under SIP connection, select the pencil icon to edit and select the TeXML application that you just created.

## Making an Outbound Call

With the server running and exposed via ngrok, you can initiate an outbound call to a specified number:

### Basic Call (No Custom Data)

```bash
curl -X POST https://your-ngrok-url.ngrok.io/start \
  -H "Content-Type: application/json" \
  -d '{
    "phone_number": "+1234567890"
  }'
```

### Call with Custom Data

```bash
curl -X POST https://your-ngrok-url.ngrok.io/start \
  -H "Content-Type: application/json" \
  -d '{
    "phone_number": "+1234567890",
    "body": {
      "user": {
        "id": "user123",
        "firstName": "John",
        "lastName": "Doe",
        "accountType": "premium"
      }
    }
  }'
```

Replace:

- `your-ngrok-url.ngrok.io` with your actual ngrok URL
- `+1234567890` with the phone number you want to call
- Customize the `body` object with any data your bot needs for personalized conversation

### Custom Data Flow

The custom data in the `body` parameter:

1. **Gets encoded** as base64 JSON and added to the TeXML URL query parameters
2. **Passes through** Telnyx's TeXML system to the WebSocket URL
3. **Gets decoded** in the WebSocket endpoint and passed to your bot via `runner_args.body`
4. **Enables personalized** conversations based on user context, preferences, or business data

**Accessing custom data in your bot:**

```python
async def bot(runner_args: RunnerArguments):
    body_data = runner_args.body or {}
    first_name = body_data.get("user", {}).get("firstName", "there")
    # Use first_name to personalize greetings, prompts, etc.
```

> **Note**: The `body` parameter is optional. If not provided, the bot will engage in a standard conversation without custom context.

## Production Deployment

### 1. Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to Pipecat Cloud.

### 2. Configure Production Environment

Update your production `.env` file with the Pipecat Cloud details:

```bash
# Set to production mode
ENV=production

# Your Pipecat Cloud deployment details
AGENT_NAME=your-agent-name
ORGANIZATION_NAME=your-org-name

# Keep your existing Telnyx and AI service keys
TELNYX_API_KEY=your_key
OPENAI_API_KEY=your_key
# ... etc
```

### 3. Deploy the Server

The `server.py` handles outbound call initiation and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (initiates calls, serves TeXML)

When `ENV=production`, the server automatically routes WebSocket connections to your Pipecat Cloud bot.

> Alternatively, you can test your Pipecat Cloud deployment by running your server locally.

### Call your Bot

As you did before, initiate a call via `curl` command to trigger your bot to dial a number. You can include custom data in the `body` parameter for personalized conversations:

```bash
curl -X POST https://your-production-server.com/start \
  -H "Content-Type: application/json" \
  -d '{
    "phone_number": "+1234567890",
    "body": {
      "customer_id": "cust_789",
      "subscription_status": "active",
      "support_tier": "premium"
    }
  }'
```

The custom data will be automatically routed through Pipecat Cloud to your deployed bot for personalized interactions.



================================================
FILE: telnyx-chatbot/outbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.telnyx import TelnyxFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"), audio_passthrough=True)

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly assistant. "
                "Your responses will be read aloud, so keep them concise and conversational. "
                "Avoid special characters or formatting. "
                "Begin by saying: 'Hello! This is an automated call from our Telnyx chatbot demo.' "
            ),
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Kick off the outbound conversation, waiting for the user to speak first
        logger.info("Starting outbound call conversation")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Outbound call ended")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""

    transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
    logger.info(f"Auto-detected transport: {transport_type}")
    # Custom data from the body parameter can be accessed via the runner_args.body attribute
    logger.info(f"Runner args body: {runner_args.body}")

    serializer = TelnyxFrameSerializer(
        stream_id=call_data["stream_id"],
        outbound_encoding=call_data["outbound_encoding"],
        inbound_encoding="PCMU",
        call_control_id=call_data["call_control_id"],
        api_key=os.getenv("TELNYX_API_KEY"),
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)



================================================
FILE: telnyx-chatbot/outbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py



================================================
FILE: telnyx-chatbot/outbound/env.example
================================================
OPENAI_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=

# Telnyx API key (required)
TELNYX_API_KEY=

# Telnyx Account SID (required for TeXML API calls)
# Find it in your Telnyx Portal account settings
TELNYX_ACCOUNT_SID=

# Telnyx TeXML Application SID (required for TeXML calls)
# This is your TeXML Application ID, NOT a Call Control Application
# Find it at: https://portal.telnyx.com/#/call-control/texml
TELNYX_APPLICATION_SID=

# Your Telnyx phone number for outbound calls
TELNYX_PHONE_NUMBER=

# Environment configuration
ENV=local
AGENT_NAME=telnyx-chatbot-dial-out
ORGANIZATION_NAME=



================================================
FILE: telnyx-chatbot/outbound/pcc-deploy.toml
================================================
agent_name = "telnyx-chatbot-dial-out"
image = "your_username/telnyx-chatbot-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "telnyx-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: telnyx-chatbot/outbound/pyproject.toml
================================================
[project]
name = "telnyx-chatbot-dial-out"
version = "0.1.0"
description = "Telnyx dial-out example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,openai,silero,deepgram,runner]>=0.0.86",
  "pipecatcloud>=0.2.4"
]



================================================
FILE: telnyx-chatbot/outbound/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""server.py

Webhook server to handle outbound call requests, initiate calls via Telnyx API,
and handle subsequent WebSocket connections for Media Streams.
"""

import base64
import json
import os
import urllib.parse
from contextlib import asynccontextmanager

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, Query, Request, WebSocket
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, JSONResponse

load_dotenv(override=True)


# ----------------- HELPERS ----------------- #


async def make_telnyx_call(
    session: aiohttp.ClientSession, to_number: str, from_number: str, texml_url: str
):
    """Make an outbound call using Telnyx's TeXML API."""
    api_key = os.getenv("TELNYX_API_KEY")
    account_sid = os.getenv("TELNYX_ACCOUNT_SID")
    application_sid = os.getenv("TELNYX_APPLICATION_SID")  # This is your TeXML Application ID

    if not api_key:
        raise ValueError("Missing Telnyx API key (TELNYX_API_KEY)")

    if not account_sid:
        raise ValueError(
            "Missing Telnyx Account SID (TELNYX_ACCOUNT_SID) - required for TeXML calls"
        )

    if not application_sid:
        raise ValueError(
            "Missing Telnyx TeXML Application SID (TELNYX_APPLICATION_SID) - required for TeXML calls"
        )

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
        "Accept": "application/json",
    }

    data = {
        "ApplicationSid": application_sid,
        "To": to_number,
        "From": from_number,
        "Url": texml_url,
    }

    url = f"https://api.telnyx.com/v2/texml/Accounts/{account_sid}/Calls"

    async with session.post(url, headers=headers, json=data) as response:
        if response.status != 200:
            error_text = await response.text()
            raise Exception(f"Telnyx API error ({response.status}): {error_text}")

        result = await response.json()
        return result


def get_websocket_url(host: str):
    """Construct base WebSocket URL (without query parameters)."""
    env = os.getenv("ENV", "local").lower()

    if env == "production":
        print("If deployed in a region other than us-west (default), update websocket url!")

        ws_url = "wss://api.pipecat.daily.co/ws/telnyx"
        # uncomment appropriate region url:
        # ws_url = wss://us-east.api.pipecat.daily.co/ws/telnyx
        # ws_url = wss://eu-central.api.pipecat.daily.co/ws/telnyx
        # ws_url = wss://ap-south.api.pipecat.daily.co/ws/telnyx

        return ws_url
    else:
        return f"wss://{host}/ws"


# ----------------- API ----------------- #


@asynccontextmanager
async def lifespan(app: FastAPI):
    # Create aiohttp session for Telnyx API calls
    app.state.session = aiohttp.ClientSession()
    yield
    # Close session when shutting down
    await app.state.session.close()


app = FastAPI(lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins for testing
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.post("/start")
async def initiate_outbound_call(request: Request) -> JSONResponse:
    """Handle outbound call request and initiate call via Telnyx."""
    print("Received outbound call request")

    try:
        data = await request.json()

        # Validate request data
        if not data.get("phone_number"):
            raise HTTPException(
                status_code=400, detail="Missing 'phone_number' in the request body"
            )

        # Extract the phone number to dial
        phone_number = str(data["phone_number"])
        print(f"Processing outbound call to {phone_number}")

        # Extract body data if provided (for custom data injection)
        body = data.get("body", {})

        # Get server URL for TeXML webhook
        host = request.headers.get("host")
        if not host:
            raise HTTPException(status_code=400, detail="Unable to determine server host")

        # Use https for production, http for localhost
        protocol = (
            "https"
            if not host.startswith("localhost") and not host.startswith("127.0.0.1")
            else "http"
        )

        # Add body as base64-encoded parameter to TeXML URL
        texml_url = f"{protocol}://{host}/answer"
        if body:
            # Encode body as base64 JSON
            body_json = json.dumps(body)
            body_b64 = base64.b64encode(body_json.encode("utf-8")).decode("utf-8")

            # URL encode the base64 string to handle special characters like +, /, =
            encoded_body = urllib.parse.quote(body_b64, safe="")
            texml_url = f"{texml_url}?body={encoded_body}"
            print(f"TeXML URL with body param: {texml_url}")
            print(f"Encoded body length: {len(body_b64)}")

        # Initiate outbound call via Telnyx
        try:
            call_result = await make_telnyx_call(
                session=request.app.state.session,
                to_number=phone_number,
                from_number=os.getenv("TELNYX_PHONE_NUMBER"),
                texml_url=texml_url,
            )

            # Extract call ID from response
            if "data" in call_result:
                call_sid = call_result["data"].get("call_control_id") or call_result["data"].get(
                    "sid"
                )
            else:
                call_sid = call_result.get("sid") or call_result.get("call_control_id")

            if not call_sid:
                call_sid = "unknown"

        except Exception as e:
            print(f"Error initiating Telnyx call: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to initiate call: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        print(f"Unexpected error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Server error: {str(e)}")

    return JSONResponse(
        {
            "call_control_id": call_sid,
            "status": "call_initiated",
            "phone_number": phone_number,
        }
    )


@app.post("/answer")
async def get_answer_xml(request: Request) -> HTMLResponse:
    """Return TeXML instructions for connecting call to WebSocket."""
    print("Serving TeXML for outbound call")

    try:
        # Get the server host to construct WebSocket URL
        host = request.headers.get("host")
        if not host:
            raise HTTPException(status_code=400, detail="Unable to determine server host")

        # Get dynamic WebSocket URL based on environment
        ws_url = get_websocket_url(host)

        # Add query parameters to WebSocket URL
        query_parts = []

        # Add serviceHost for production environments
        env = os.getenv("ENV", "local").lower()
        if env == "production":
            agent_name = os.getenv("AGENT_NAME")
            org_name = os.getenv("ORGANIZATION_NAME")
            if agent_name and org_name:
                query_parts.append(f"serviceHost={agent_name}.{org_name}")

        # Add body parameter if present
        if request.query_params and "body" in request.query_params:
            body_param = request.query_params["body"]
            query_parts.append(f"body={body_param}")
            print(f"Added body param to WebSocket URL")

        # Construct WebSocket URL with proper &amp; encoding for multiple params
        if query_parts:
            query_string = "&amp;".join(query_parts)
            ws_url = f"{ws_url}?{query_string}"
            print(f"WebSocket URL with query params: {ws_url}")

        # Generate TeXML response
        texml_content = f'''<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Connect>
        <Stream url="{ws_url}" bidirectionalMode="rtp"></Stream>
    </Connect>
    <Pause length="40"/>
</Response>'''

        return HTMLResponse(content=texml_content, media_type="application/xml")

    except Exception as e:
        print(f"Error generating TeXML: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to generate TeXML: {str(e)}")


@app.websocket("/ws")
async def websocket_endpoint(
    websocket: WebSocket,
    body: str = Query(None),
    serviceHost: str = Query(None),
):
    """Handle WebSocket connection from Telnyx Media Streams."""
    await websocket.accept()
    print("WebSocket connection accepted for outbound call")

    print(f"Received query params - body: {body}, serviceHost: {serviceHost}")

    # Decode body parameter if provided
    body_data = {}
    if body:
        try:
            # URL decode first, then base64 decode
            url_decoded = urllib.parse.unquote(body)
            decoded_json = base64.b64decode(url_decoded).decode("utf-8")
            body_data = json.loads(decoded_json)
            print(f"Decoded body data: {body_data}")
        except Exception as e:
            print(f"Error decoding body parameter: {e}")
    else:
        print("No body parameter received")

    try:
        # Import the bot function from the bot module
        from bot import bot
        from pipecat.runner.types import WebSocketRunnerArguments

        # Create runner arguments with body data
        runner_args = WebSocketRunnerArguments(websocket=websocket, body=body_data)

        await bot(runner_args)

    except Exception as e:
        print(f"Error in WebSocket endpoint: {e}")
        await websocket.close()


# ----------------- Main ----------------- #


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)



================================================
FILE: translation-chatbot/README.md
================================================
# Translation Chatbot

<img src="image.png" width="420px">

This app listens for user speech, then translates that speech to Spanish and speaks the translation back to the user using text-to-speech. It's probably most useful with multiple users talking to each other, along with some manual track subscription management in the Daily call.

See a quick video walkthrough of the code here: https://www.loom.com/share/59fdddf129534dc2be4dde3cc6ebe8de

## Get started

```python
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

cp env.example .env # and add your credentials

```

## Run the server

```bash
python server.py
```

Then, visit `http://localhost:7860/` in your browser to start a translatorbot session.

## Build and test the Docker image

```
docker build -t chatbot .
docker run --env-file .env -p 7860:7860 chatbot
```



================================================
FILE: translation-chatbot/bot.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
from typing import List

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    Frame,
    LLMMessagesUpdateFrame,
    TranscriptionFrame,
    TranscriptionMessage,
    TranscriptionUpdateFrame,
)
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.filters.stt_mute_filter import STTMuteConfig, STTMuteFilter, STTMuteStrategy
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor
from pipecat.processors.frameworks.rtvi import RTVIObserver, RTVIProcessor
from pipecat.processors.transcript_processor import TranscriptProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams
from pipecat.transports.websocket.fastapi import FastAPIWebsocketParams

load_dotenv(override=True)

"""
This example looks a bit different than the chatbot example, because it isn't waiting on the user to stop talking to start translating.
It also isn't saving what the user or bot says into the context object for use in subsequent interactions.
"""


# We need to use a custom service here to yield LLM frames without saving
# any context
class TranslationProcessor(FrameProcessor):
    """A processor that translates text frames from a source language to a target language."""

    def __init__(self, in_language, out_language):
        """Initialize the TranslationProcessor with source and target languages.

        Args:
            in_language (str): The language of the input text.
            out_language (str): The language to translate the text into.
        """
        super().__init__()
        self._out_language = out_language
        self._in_language = in_language

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        """Process a frame and translate text frames.

        Args:
            frame (Frame): The frame to process.
            direction (FrameDirection): The direction of the frame.
        """
        await super().process_frame(frame, direction)

        if isinstance(frame, TranscriptionFrame):
            logger.debug(f"Translating {self._in_language}: {frame.text} to {self._out_language}")
            messages = [
                {
                    "role": "system",
                    "content": f"You will be provided with a sentence in {self._in_language}, and your task is to only translate it into {self._out_language}.",
                },
                {"role": "user", "content": frame.text},
            ]
            await self.push_frame(LLMMessagesUpdateFrame(messages, run_llm=True))
        else:
            await self.push_frame(frame)


class TranscriptHandler:
    """Simple handler to demonstrate transcript processing.

    Maintains a list of conversation messages and logs them with timestamps.
    """

    def __init__(self, in_language="English", out_language="Spanish"):
        """Initialize the TranscriptHandler with an empty list of messages."""
        self.messages: List[TranscriptionMessage] = []
        self.in_language = in_language
        self.out_language = out_language

    async def on_transcript_update(
        self, processor: TranscriptProcessor, frame: TranscriptionUpdateFrame
    ):
        """Handle new transcript messages.

        Args:
            processor: The TranscriptProcessor that emitted the update
            frame: TranscriptionUpdateFrame containing new messages
        """
        self.messages.extend(frame.messages)

        # Log the new messages
        logger.info("New transcript messages:")
        for msg in frame.messages:
            timestamp = f"[{msg.timestamp}] " if msg.timestamp else ""
            message = {
                "event": "translation",
                "timestamp": msg.timestamp,
                "role": msg.role,
                "language": self.out_language if msg.role == "assistant" else self.in_language,
                "text": msg.content,
            }
            logger.info(f"{timestamp}{msg.role}: {msg.content}")


# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
        turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
    ),
}


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.info(f"Starting bot")

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    stt_mute_processor = STTMuteFilter(
        config=STTMuteConfig(
            strategies={
                STTMuteStrategy.ALWAYS,
            }
        ),
    )

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="34dbb662-8e98-413c-a1ef-1a3407675fe7",  # Spanish Narrator Man
        model="sonic-2",
    )

    in_language = "English"
    out_language = "Spanish"

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))
    context = LLMContext()
    context_aggregator = LLMContextAggregatorPair(context)

    tp = TranslationProcessor(in_language=in_language, out_language=out_language)

    transcript = TranscriptProcessor()
    transcript_handler = TranscriptHandler(in_language=in_language, out_language=out_language)

    rtvi = RTVIProcessor()

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt_mute_processor,  # We don't want to interrupt the translator bot
            stt,
            transcript.user(),  # User transcripts
            tp,
            llm,
            tts,
            transport.output(),
            transcript.assistant(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @transport.event_handler("on_first_participant_joined")
    async def on_first_participant_joined(transport, participant):
        logger.info("First participant joined")

    @transport.event_handler("on_participant_left")
    async def on_participant_left(transport, participant, reason):
        await task.cancel()

    # Register event handler for transcript updates
    @transcript.event_handler("on_transcript_update")
    async def on_transcript_update(processor, frame):
        await transcript_handler.on_transcript_update(processor, frame)

    runner = PipelineRunner()

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: translation-chatbot/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: translation-chatbot/env.example
================================================
DAILY_API_KEY=7df...
OPENAI_API_KEY=sk-PL...
DEEPGRAM_API_KEY=...
CARTESIA_API_KEY=...


================================================
FILE: translation-chatbot/pcc-deploy.toml
================================================
agent_name = "translation-chatbot"
image = "your_username/translation-chatbot:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "translation-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: translation-chatbot/pyproject.toml
================================================
[project]
name = "translation-chatbot"
version = "0.1.0"
description = "Translation chatbot"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[daily,webrtc,websocket,cartesia,google,silero,deepgram,runner,local-smart-turn-v3]>=0.0.91",
  "pipecatcloud>=0.2.7",
]



================================================
FILE: travel-companion/README.md
================================================
# Pipecat Travel Companion

Pipecat Travel Companion is a smart travel assistant powered by the `GeminiLiveLLMService`.
It offers personalized recommendations and services like checking the weather, suggesting nearby restaurants,
and providing recent news based on your current location. 

---

## Features
- **Location Sharing**:
  - Retrieves your current location using the `get_my_current_location` RTVI function calling.
  - Shares selected restaurant locations using the `set_restaurant_location` RTVI function calling, which opens Google Maps on iOS.
- **Weather Updates**: Uses `google_search` to check and share the current weather.
- **Restaurant Recommendations**: Suggests restaurants near your current location using `google_search`.
- **Local News**: Provides relevant and recent news from your location using `google_search`.
---

## Getting Started

Follow these steps to set up and run the Pipecat Travel Companion server.

### 1. Setup Virtual Environment (Recommended)

Navigate to the `server` directory and set up a virtual environment:

```bash
cd server
python3 -m venv venv
source venv/bin/activate
```

### 2. Installation

Install the required dependencies in development mode:

```bash
pip install -r requirements.txt
```

### 3. Configuration

1. Copy the example environment configuration file:
   
```bash
cp env.example .env
```

2. Open `.env` and add your API keys and configuration details.

### 4. Running the Server

Start the server with the following command:

```bash
cd server
python src/server.py --host YOUR_IP
```
Replace `YOUR_IP` with your desired host IP address.

---

## Client APP

This project is designed to work with a companion iOS app. The app:
- Uses RTVI function calls to share the user's current location with the LLM.
- Receives restaurant location suggestions from the LLM and opens Google Maps to display the location.

For detailed instructions on setting up and running the iOS app, refer to [this link](./client/ios/README.md).

---

## Additional Notes

- Ensure all required API keys are defined.

---

Happy travels with Pipecat! 🌍




================================================
FILE: travel-companion/client/ios/README.md
================================================
# iOS Implementation

This guide provides instructions for setting up and running the iOS implementation of the Pipecat Travel Companion app. 
It utilizes the [Pipecat iOS SDK](https://docs.pipecat.ai/client/ios/introduction) to integrate location-based features and server interactions.

---

## Prerequisites

1. **Run the Bot Server**
   - Make sure the Pipecat Travel Companion server is running. Refer to the [server README](../../README) for setup instructions.

2. **Install Xcode**
   - Download and install [Xcode 15](https://developer.apple.com/xcode/).
   - Set up your device to run custom applications by following the guide on [distributing your app to registered devices](https://developer.apple.com/documentation/xcode/distributing-your-app-to-registered-devices).

---

## Running Locally

1. **Open the Project in Xcode**
   - Navigate to the project directory and open `TravelCompanion.xcodeproj` in Xcode.

2. **Build the Project**
   - Build the project by selecting your target device and clicking the "Build" button.

3. **Run on Your Device**
   - Connect your device to your computer.
   - Run the app by clicking the "Run" button.

4. **Connect to the Server**
   - Ensure the app is pointing to the correct server URL for testing. Update the configuration if needed.

---

## Troubleshooting

- **Build Errors**: Check if all dependencies are resolved and updated. Verify your Xcode version is 15 or above.
- **Connection Issues**: Ensure that the server is running and accessible from your device. Confirm the URL is correct.
- **Device Setup**: Make sure your device is properly registered and set up for development in Xcode.

---

## Additional Resources

- [Pipecat iOS SDK Documentation](https://docs.pipecat.ai/client/ios/introduction)
- [Xcode Documentation](https://developer.apple.com/documentation/xcode/)

---

Happy coding with Pipecat! 🚀




================================================
FILE: travel-companion/client/ios/TravelCompanion/Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>NSAppTransportSecurity</key>
	<dict>
		<key>NSAllowsLocalNetworking</key>
		<true/>
	</dict>
	<key>UIBackgroundModes</key>
	<array>
		<string>voip</string>
	</array>
	<key>NSLocationWhenInUseUsageDescription</key>
    <string>We need your location to provide a better experience.</string>
    <key>NSLocationAlwaysAndWhenInUseUsageDescription</key>
    <string>We need your location to provide location-based features.</string>
    <key>NSLocationAlwaysUsageDescription</key>
    <string>We need your location to provide continuous location updates.</string>
</dict>
</plist>



================================================
FILE: travel-companion/client/ios/TravelCompanion/TravelCompanionApp.swift
================================================
import SwiftUI

@main
struct TravelCompanionApp: App {

    @StateObject var callContainerModel = CallContainerModel()

    var body: some Scene {
        WindowGroup {
            if (!callContainerModel.isInCall) {
                PreJoinView().environmentObject(callContainerModel)
            } else {
                MeetingView().environmentObject(callContainerModel)
            }
        }
    }

}



================================================
FILE: travel-companion/client/ios/TravelCompanion/Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "appstore.png",
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/Assets.xcassets/pipecat.imageset/Contents.json
================================================
{
  "images" : [
    {
      "filename" : "Square Black.svg",
      "idiom" : "universal",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "scale" : "3x"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/model/CallContainerModel.swift
================================================
import SwiftUI

import PipecatClientIOSDaily
import PipecatClientIOS
import CoreLocation

class CallContainerModel: ObservableObject {
    
    @Published var voiceClientStatus: String = TransportState.disconnected.description
    @Published var isInCall: Bool = false
    @Published var isBotReady: Bool = false
    @Published var timerCount = 0
    
    @Published var isMicEnabled: Bool = false
    
    @Published var toastMessage: String? = nil
    @Published var showToast: Bool = false
    
    @Published
    var remoteAudioLevel: Float = 0
    @Published
    var localAudioLevel: Float = 0
    
    private var meetingTimer: Timer?
    
    var pipecatClientIOS: PipecatClient?
    let locationManager = LocationManager()
    
    init() {
        // Changing the log level
        PipecatClientIOS.setLogLevel(.warn)
        self.locationManager.requestLocationPermission()
    }
    
    @MainActor
    func connect(backendURL: String) {
        let baseUrl = backendURL.trimmingCharacters(in: .whitespacesAndNewlines)
        if(baseUrl.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty){
            self.showError(message: "Need to fill the backendURL. For more info visit: https://bots.daily.co")
            return
        }
        
        let currentSettings = SettingsManager.getSettings()
        let pipecatClientOptions = PipecatClientOptions.init(
            transport: DailyTransport.init(),
            enableMic: currentSettings.enableMic,
            enableCam: false,
        )
        self.pipecatClientIOS = PipecatClient.init(
            options: pipecatClientOptions
        )
        
        self.pipecatClientIOS?.delegate = self
        let startBotParams = APIRequest.init(endpoint: URL(string: baseUrl + "/connect")!)
        
        // Handling the function calls
        self.pipecatClientIOS?.registerFunctionCallHandler(functionName: ToolsFunctions.getMyCurrentLocation.rawValue) { functionCallData, onResult  in
            let location = await self.handleGetCurrentLocation()
            await onResult(location)
        }
        self.pipecatClientIOS?.registerFunctionCallHandler(functionName: ToolsFunctions.setRestaurantLocation.rawValue) { functionCallData, onResult  in
            self.handleRestaurantLocation(restaurantInfo: functionCallData.args)
            await onResult(.string("success"))
        }
        
        self.pipecatClientIOS?.startBotAndConnect(startBotParams: startBotParams) { (result: Result<DailyTransportConnectionParams, AsyncExecutionError>) in
            if case .failure(let error) = result {
                self.showError(message: error.localizedDescription)
                self.pipecatClientIOS = nil
            }
        }
        // Selecting the mic based on the preferences
        if let selectedMic = currentSettings.selectedMic {
            self.pipecatClientIOS?.updateMic(micId: MediaDeviceId(id:selectedMic), completion: nil)
        }
        self.saveCredentials(backendURL: baseUrl)
    }
    
    @MainActor
    func disconnect() {
        self.pipecatClientIOS?.unregisterAllFunctionCallHandlers()
        self.pipecatClientIOS?.disconnect(completion: nil)
    }
    
    func showError(message: String) {
        self.toastMessage = message
        self.showToast = true
        // Hide the toast after 5 seconds
        DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
            self.showToast = false
            self.toastMessage = nil
        }
    }
    
    @MainActor
    func toggleMicInput() {
        self.pipecatClientIOS?.enableMic(enable: !self.isMicEnabled) { result in
            switch result {
            case .success():
                self.isMicEnabled = self.pipecatClientIOS?.isMicEnabled ?? false
            case .failure(let error):
                self.showError(message: error.localizedDescription)
            }
        }
    }
    
    private func startTimer() {
        self.timerCount = 0
        self.meetingTimer = Timer.scheduledTimer(withTimeInterval: 1.0, repeats: true) { timer in
            DispatchQueue.main.async {
                self.timerCount += 1
            }
        }
    }
    
    private func stopTimer() {
        self.meetingTimer?.invalidate()
        self.meetingTimer = nil
        self.timerCount = 0
    }
    
    func saveCredentials(backendURL: String) {
        var currentSettings = SettingsManager.getSettings()
        currentSettings.backendURL = backendURL
        // Saving the settings
        SettingsManager.updateSettings(settings: currentSettings)
    }
    
}

extension CallContainerModel:PipecatClientDelegate {
    
    private func handleEvent(eventName: String, eventValue: Any? = nil) {
        if let value = eventValue {
            print("Travel Companion Demo, received event:\(eventName), value:\(value)")
        } else {
            print("Travel Companion Demo, received event: \(eventName)")
        }
    }
    
    func onTransportStateChanged(state: TransportState) {
        self.handleEvent(eventName: "onTransportStateChanged", eventValue: state)
        self.voiceClientStatus = state.description
        self.isInCall = ( state == .connecting || state == .connected || state == .ready || state == .authenticating )
    }
    
    func onBotReady(botReadyData: BotReadyData) {
        Task { @MainActor in
            self.handleEvent(eventName: "onBotReady.")
            self.isBotReady = true
            self.startTimer()
            // Only to show how to send custom messages or requests
            try self.pipecatClientIOS?.sendClientMessage(msgType: "ios-app-name", data: .string("Travel Companion!"))
            let llmVendor = try await self.pipecatClientIOS?.sendClientRequest(msgType: "get-llm-vendor").d?.asString
            print("Travel Companion Demo, LLM Vendor: \(llmVendor ?? "")")
        }
    }
    
    func onConnected() {
        Task { @MainActor in
            self.isMicEnabled = self.pipecatClientIOS?.isMicEnabled ?? false
        }
    }
    
    func onDisconnected() {
        self.stopTimer()
        self.isBotReady = false
    }
    
    func onRemoteAudioLevel(level: Float, participant: Participant) {
        Task { @MainActor in
            self.remoteAudioLevel = level
        }
    }
    
    func onLocalAudioLevel(level: Float) {
        Task { @MainActor in
            self.localAudioLevel = level
        }
    }
    
    func onUserTranscript(data: Transcript) {
        Task { @MainActor in
            if (data.final ?? false) {
                self.handleEvent(eventName: "onUserTranscript", eventValue: data.text)
            }
        }
    }
    
    func onBotTranscript(data: BotLLMText) {
        Task { @MainActor in
            self.handleEvent(eventName: "onBotTranscript", eventValue: data)
        }
    }
    
    func onError(message: RTVIMessageInbound) {
        Task { @MainActor in
            self.handleEvent(eventName: "onError", eventValue: message)
            self.showError(message: message.data ?? "")
        }
    }
    
    func onTrackStarted(track: MediaStreamTrack, participant: Participant?) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTrackStarted", eventValue: track)
        }
    }

    func onTrackStopped(track: MediaStreamTrack, participant: Participant?) {
        Task { @MainActor in
            self.handleEvent(eventName: "onTrackStopped", eventValue: track)
        }
    }
    
    private func openGoogleMaps(fullAddress: String) {
        DispatchQueue.main.async {
            // Not using the latitude and longitude that we are receiving, they don't see to be matching the address
            // &center=\(latitude),\(longitude)
            let googleMapsURL = "comgooglemaps://?q=\(fullAddress)&zoom=14"
            print("googleMapsURL \(googleMapsURL)")
            if let url = URL(string: googleMapsURL) {
                if UIApplication.shared.canOpenURL(url) {
                    UIApplication.shared.open(url, options: [:], completionHandler: nil)
                } else {
                    // Google Maps not installed, fallback to web
                    if let webURL = URL(string: "https://www.google.com/maps/search/?api=1&query=\(fullAddress)") {
                        UIApplication.shared.open(webURL, options: [:], completionHandler: nil)
                    }
                }
            }
        }
    }
    
    private func handleRestaurantLocation(restaurantInfo:Value) {
        var restaurantName: String? = nil
        var longitude: Double? = nil
        var latitude: Double? = nil
        var fullAddress: String = ""
        
        if case .object(let dictionary) = restaurantInfo {
            if let restaurantValue = dictionary["restaurant"],
               case .string(let name) = restaurantValue {
                restaurantName = name
            }
            if let lonValue = dictionary["lon"],
               case .string(let lon) = lonValue {
                longitude = Double(lon)
            }
            if let latValue = dictionary["lat"],
               case .string(let lat) = latValue {
                latitude = Double(lat)
            }
            if let addressValue = dictionary["address"],
               case .string(let address) = addressValue {
                fullAddress = address
            }
        }
        
        if let restaurant = restaurantName,
           let lon = longitude,
           let lat = latitude {
            print("Restaurant: \(restaurant), Longitude: \(lon), Latitude: \(lat)")
            print("Restaurant: \(fullAddress)")
            self.openGoogleMaps(fullAddress: fullAddress)
        } else {
            print("One or more properties are missing for the restaurant location.")
        }
    }
    
    private func handleGetCurrentLocation() async -> Value {
        do {
            let location = try await locationManager.fetchLocation()
            print("Location: \(location.coordinate.latitude), \(location.coordinate.longitude)")
            return Value.object([
                "lat": .string(String(location.coordinate.latitude)),
                "lon": .string(String(location.coordinate.longitude))
            ])
        } catch {
            return Value.string("Failed to get current location!")
        }
    }
}

enum ToolsFunctions: String {
    case getMyCurrentLocation = "get_my_current_location"
    case setRestaurantLocation = "set_restaurant_location"
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/model/LocationManager.swift
================================================
import CoreLocation

class LocationManager: NSObject, CLLocationManagerDelegate {
    private let locationManager = CLLocationManager()
    private var locationContinuation: CheckedContinuation<CLLocation, Error>?

    override init() {
        super.init()
        locationManager.delegate = self
        locationManager.desiredAccuracy = kCLLocationAccuracyBest
    }

    func requestLocationPermission() {
        locationManager.requestWhenInUseAuthorization()
    }

    func fetchLocation() async throws -> CLLocation {
        // Check authorization status
        let status = locationManager.authorizationStatus
        guard status == .authorizedWhenInUse || status == .authorizedAlways else {
            throw NSError(domain: "LocationError", code: 1, userInfo: [NSLocalizedDescriptionKey: "Location permission not granted"])
        }

        return try await withCheckedThrowingContinuation { continuation in
            locationContinuation = continuation
            locationManager.requestLocation()
        }
    }

    // CLLocationManagerDelegate
    func locationManager(_ manager: CLLocationManager, didUpdateLocations locations: [CLLocation]) {
        guard let location = locations.last else { return }
        locationContinuation?.resume(returning: location)
        locationContinuation = nil // Clear continuation after use
    }

    func locationManager(_ manager: CLLocationManager, didFailWithError error: Error) {
        locationContinuation?.resume(throwing: error)
        locationContinuation = nil // Clear continuation after use
    }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/model/MockCallContainerModel.swift
================================================
import SwiftUI
import PipecatClientIOS

class MockCallContainerModel: CallContainerModel {

    override init() {
    }

    override func connect(backendURL: String) {
        print("connect")
    }

    override func disconnect() {
        print("disconnect")
    }

    override func showError(message: String) {
        self.toastMessage = message
        self.showToast = true
        // Hide the toast after 5 seconds
        DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
            self.showToast = false
            self.toastMessage = nil
        }
    }

    func startAudioLevelSimulation() {
        // Simulate audio level changes
        Timer.scheduledTimer(withTimeInterval: 0.1, repeats: true) { _ in
            let newLevel = Float.random(in: 0...1)
            self.remoteAudioLevel = newLevel
            self.localAudioLevel = newLevel
        }
    }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/Preview Content/Preview Assets.xcassets/Contents.json
================================================
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/MeetingView.swift
================================================
import SwiftUI

struct MeetingView: View {
    
    @State private var showingSettings = false
    @EnvironmentObject private var model: CallContainerModel
    
    var body: some View {
        VStack {
            // Header Toolbar
            HStack {
                Image("dailyBot")
                    .resizable()
                    .frame(width: 48, height: 48)
                Spacer()
                HStack {
                    Image(systemName: "stopwatch")
                        .resizable()
                        .frame(width: 24, height: 24)
                    Text(timerString(from: self.model.timerCount))
                        .font(.headline)
                }.padding()
                    .background(Color.timer)
                    .cornerRadius(12)
            }
            .padding()
            
            // Main Panel
            VStack {
                VStack {
                    WaveformView(audioLevel: model.remoteAudioLevel, isBotReady: model.isBotReady, voiceClientStatus: model.voiceClientStatus)
                }
                .frame(maxHeight: .infinity)
                VStack {
                    HStack {
                        MicrophoneView(audioLevel: model.localAudioLevel, isMuted: !self.model.isMicEnabled)
                            .frame(width: 160, height: 160)
                            .onTapGesture {
                                self.model.toggleMicInput()
                            }
                    }
                }
                .frame(height: 120)
            }
            .frame(maxHeight: .infinity)
            .padding()
            
            // Bottom Panel
            VStack {
                HStack {
                    Button(action: {
                        self.showingSettings = true
                    }) {
                        HStack {
                            Image(systemName: "gearshape")
                                .resizable()
                                .frame(width: 24, height: 24)
                            Text("Settings")
                        }
                        .frame(maxWidth: .infinity)
                        .padding()
                        .sheet(isPresented: $showingSettings) {
                            SettingsView(showingSettings: $showingSettings).environmentObject(self.model)
                        }
                    }
                    .border(Color.buttonsBorder, width: 1)
                    .cornerRadius(12)
                }
                .foregroundColor(.black)
                .padding([.top, .horizontal])
                
                Button(action: {
                    self.model.disconnect()
                }) {
                    HStack {
                        Image(systemName: "rectangle.portrait.and.arrow.right")
                            .resizable()
                            .frame(width: 24, height: 24)
                        Text("End")
                    }
                    .frame(maxWidth: .infinity)
                    .padding()
                }
                .foregroundColor(.white)
                .background(Color.black)
                .cornerRadius(12)
                .padding([.bottom, .horizontal])
            }
        }
        .background(Color.backgroundApp)
        .toast(message: model.toastMessage, isShowing: model.showToast)
    }
    
    func timerString(from count: Int) -> String {
        let hours = count / 3600
        let minutes = (count % 3600) / 60
        let seconds = count % 60
        return String(format: "%02d:%02d:%02d", hours, minutes, seconds)
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = MeetingView().environmentObject(mockModel as CallContainerModel)
    mockModel.startAudioLevelSimulation()
    return result
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/PreJoinView.swift
================================================
import SwiftUI

struct PreJoinView: View {
    
    @State var backendURL: String

    @EnvironmentObject private var model: CallContainerModel
    
    init() {
        let currentSettings = SettingsManager.getSettings()
        self.backendURL = currentSettings.backendURL
    }

    var body: some View {
        VStack(spacing: 20) {
            Image("pipecat")
                .resizable()
                .frame(width: 80, height: 80)
            Text("Pipecat Client iOS.")
                .font(.headline)
            TextField("Server URL", text: $backendURL)
                .textFieldStyle(RoundedBorderTextFieldStyle())
                .frame(maxWidth: .infinity)
                .padding([.bottom, .horizontal])
            Button("Connect") {
                self.model.connect(backendURL: self.backendURL)
            }
            .padding()
            .background(Color.black)
            .foregroundColor(.white)
            .cornerRadius(8)
        }
        .padding()
        .frame(maxHeight: .infinity)
        .background(Color.backgroundApp)
        .toast(message: model.toastMessage, isShowing: model.showToast)
    }
}

#Preview {
    PreJoinView().environmentObject(MockCallContainerModel() as CallContainerModel)
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/components/MicrophoneView.swift
================================================
import SwiftUI

struct MicrophoneView: View {
    var audioLevel: Float // Current audio level
    var isMuted: Bool // Muted state

    var body: some View {
        GeometryReader { geometry in
            let width = geometry.size.width
            let circleSize = width * 0.9
            let innerCircleSize = width * 0.82
            let audioCircleSize = CGFloat(audioLevel) * (width * 0.95)

            ZStack {
                Circle()
                    .stroke(Color.gray, lineWidth: 1)
                    .frame(width: circleSize)

                Circle()
                    .fill(isMuted ? Color.disabledMic : Color.backgroundCircle)
                    .frame(width: innerCircleSize)

                if !isMuted {
                    Circle()
                        .fill(Color.micVolume)
                        .opacity(0.5)
                        .frame(width: audioCircleSize)
                        .animation(.easeInOut(duration: 0.2), value: audioLevel)
                }

                Image(systemName: isMuted ? "mic.slash.fill" : "mic.fill")
                    .resizable()
                    .scaledToFit()
                    .frame(width: width * 0.2)
                    .foregroundColor(.white)
            }
            .frame(maxWidth: .infinity, maxHeight: .infinity) // Ensures the ZStack is centered
        }
    }
}

#Preview {
    MicrophoneView(audioLevel: 1, isMuted: false)
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/components/ToastModifier.swift
================================================
import SwiftUI

struct ToastModifier: ViewModifier {
    var message: String?
    var isShowing: Bool
    
    func body(content: Content) -> some View {
        ZStack {
            content
            if isShowing, let message = message {
                VStack {
                    Text(message)
                        .padding()
                        .background(Color.black.opacity(0.7))
                        .foregroundColor(.white)
                        .cornerRadius(8)
                        .transition(.slide)
                        .padding(.top, 50)
                    Spacer()
                }
                .animation(.easeInOut(duration: 0.5), value: isShowing)
            }
        }
    }
}

extension View {
    func toast(message: String?, isShowing: Bool) -> some View {
        self.modifier(ToastModifier(message: message, isShowing: isShowing))
    }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/components/WaveformView.swift
================================================
import SwiftUI

struct WaveformView: View {
    
    var audioLevel: Float
    var isBotReady: Bool
    var voiceClientStatus: String
    
    @State
    private var audioLevels: [Float] = Array(repeating: 0, count: 5)
    private let dotCount = 5
    
    var body: some View {
        GeometryReader { geometry in
            VStack {
                Spacer()
                HStack {
                    Spacer()
                    ZStack {
                        // Outer gray border
                        Circle()
                            .stroke(Color.gray, lineWidth: 1)
                            .frame(width: geometry.size.width * 0.9, height: geometry.size.width * 0.9)
                        
                        // Gray middle
                        Circle()
                            .fill(isBotReady ? Color.backgroundCircle : Color.backgroundCircleNotConnected)
                            .frame(width: geometry.size.width * 0.82, height: geometry.size.width * 0.82)
                        
                        if isBotReady {
                            if audioLevel > 0 {
                                // Waveform bars inside the circle
                                HStack(spacing: 10) {
                                    ForEach(0..<dotCount, id: \.self) { index in
                                        Rectangle()
                                            .fill(Color.white)
                                            .frame(height: CGFloat(audioLevels[index]) * (geometry.size.height))
                                            .cornerRadius(12)
                                            .animation(.easeInOut(duration: 0.2), value: audioLevels[index])
                                    }
                                    .frame(maxWidth: .infinity)
                                    .padding(.horizontal, 5)
                                }
                                .frame(width: geometry.size.width * 0.5, height: geometry.size.width * 0.5)
                                .mask(Circle().frame(width: geometry.size.width * 0.82, height: geometry.size.width * 0.82))
                            } else {
                                // Dots inside the circle
                                HStack(spacing: 10) {
                                    ForEach(0..<dotCount, id: \.self) { _ in
                                        Circle()
                                            .fill(Color.white)
                                            .frame(maxWidth: .infinity)
                                    }
                                    .frame(maxWidth: .infinity)
                                }
                                .frame(width: geometry.size.width * 0.5, height: geometry.size.height * 0.5)
                            }
                        } else {
                            // Gray circle with loading icon when not connected
                            VStack {
                                ProgressView()
                                    .progressViewStyle(CircularProgressViewStyle(tint: .white))
                                    .scaleEffect(2) // Adjust size of the loading spinner
                                    .padding()
                                Text(voiceClientStatus)
                                    .foregroundColor(.white)
                                    .font(.headline)
                            }
                        }
                    }
                    Spacer()
                }
                Spacer()
            }
        }
        .onChange(of: audioLevel) { oldLevel, newLevel in
            // The audio level that we receive from the bot is usually too low
            // so just increasing it so we can see a better graph but
            // making sure that it is not higher than the maximum 1
            var audioLevel = audioLevel + 0.4
            if(audioLevel > 1) {
                audioLevel = 1
            }
            // Update the array and shift values
            audioLevels.removeFirst()
            audioLevels.append(newLevel)
        }
    }
}

#Preview {
    WaveformView(audioLevel: 0, isBotReady: false, voiceClientStatus: "idle")
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/extensions/CustomColors.swift
================================================
import SwiftUI

public extension Color {
    
    static let backgroundCircle = Color(hex: "#374151")
    static let backgroundCircleNotConnected = Color(hex: "#D1D5DB")
    static let backgroundApp = Color(hex: "#F9FAFB")
    static let buttonsBorder = Color(hex: "#E5E7EB")
    static let micVolume = Color(hex: "#86EFAC")
    static let timer = Color(hex: "#E5E7EB")
    static let disabledMic = Color(hex: "#ee6b6e")
    static let disabledVision = Color(hex: "#BBF7D0")
        
    init(hex: String) {
        let scanner = Scanner(string: hex)
        _ = scanner.scanString("#")
        
        var rgb: UInt64 = 0
        scanner.scanHexInt64(&rgb)
        
        let red = Double((rgb >> 16) & 0xFF) / 255.0
        let green = Double((rgb >> 8) & 0xFF) / 255.0
        let blue = Double(rgb & 0xFF) / 255.0
        
        self.init(red: red, green: green, blue: blue)
    }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/settings/SettingsManager.swift
================================================
import Foundation

class SettingsManager {
    private static let preferencesKey = "settingsPreference"

    static func getSettings() -> SettingsPreference {
        if let data = UserDefaults.standard.data(forKey: preferencesKey),
           let settings = try? JSONDecoder().decode(SettingsPreference.self, from: data) {
            return settings
        } else {
            // default values in case we don't have any settings
            return SettingsPreference(enableMic: true, backendURL: "http://YOUR_IP:7860")
        }
    }
    
    static func updateSettings(settings: SettingsPreference) {
        if let data = try? JSONEncoder().encode(settings) {
            UserDefaults.standard.set(data, forKey: preferencesKey)
        }
    }
}



================================================
FILE: travel-companion/client/ios/TravelCompanion/views/settings/SettingsPreference.swift
================================================
import Foundation

struct SettingsPreference: Codable {
    var selectedMic: String?
    var enableMic: Bool
    var backendURL: String
}




================================================
FILE: travel-companion/client/ios/TravelCompanion/views/settings/SettingsView.swift
================================================
import SwiftUI
import PipecatClientIOS

struct SettingsView: View {
    
    @EnvironmentObject private var model: CallContainerModel
    
    @Binding var showingSettings: Bool
    
    @State private var selectedMic: MediaDeviceId? = nil
    @State private var isMicEnabled: Bool = true
    @State private var backendURL: String = ""
    
    var body: some View {
        let microphones = self.model.pipecatClientIOS?.getAllMics() ?? []
        NavigationView {
            Form {
                Section(header: Text("Audio Settings")) {
                    List(microphones, id: \.self.id.id) { mic in
                        Button(action: {
                            self.selectMic(mic.id)
                        }) {
                            HStack {
                                Text(mic.name)
                                Spacer()
                                if mic.id == self.selectedMic {
                                    Image(systemName: "checkmark")
                                }
                            }
                        }
                    }
                }
                Section(header: Text("Start options")) {
                    Toggle("Enable Microphone", isOn: $isMicEnabled)
                }
                Section(header: Text("Server")) {
                    TextField("Backend URL", text: $backendURL)
                        .keyboardType(.URL)
                }
            }
            .navigationTitle("Settings")
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Close") {
                        self.saveSettings()
                        self.showingSettings = false
                    }
                }
            }
            .onAppear {
                self.loadSettings()
            }
        }
    }
    
    private func selectMic(_ mic: MediaDeviceId) {
        self.selectedMic = mic
        self.model.pipecatClientIOS?.updateMic(micId: mic, completion: nil)
    }
    
    private func saveSettings() {
        let newSettings = SettingsPreference(
            selectedMic: selectedMic?.id,
            enableMic: isMicEnabled,
            backendURL: backendURL
        )
        SettingsManager.updateSettings(settings: newSettings)
    }
    
    private func loadSettings() {
        let savedSettings = SettingsManager.getSettings()
        if let selectedMic = savedSettings.selectedMic {
            self.selectedMic = MediaDeviceId(id: selectedMic)
        } else {
            self.selectedMic = nil
        }
        self.isMicEnabled = savedSettings.enableMic
        self.backendURL = savedSettings.backendURL
    }
}

#Preview {
    let mockModel = MockCallContainerModel()
    let result = SettingsView(showingSettings: .constant(true)).environmentObject(mockModel as CallContainerModel)
    mockModel.startAudioLevelSimulation()
    return result
}



================================================
FILE: travel-companion/client/ios/TravelCompanionTests/TravelCompanionTests.swift
================================================
import XCTest
@testable import TravelCompanion

final class TravelCompanionTests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        // This is an example of a functional test case.
        // Use XCTAssert and related functions to verify your tests produce the correct results.
        // Any test you write for XCTest can be annotated as throws and async.
        // Mark your test throws to produce an unexpected failure when your test encounters an uncaught error.
        // Mark your test async to allow awaiting for asynchronous code to complete. Check the results with assertions afterwards.
    }

    func testPerformanceExample() throws {
        // This is an example of a performance test case.
        self.measure {
            // Put the code you want to measure the time of here.
        }
    }

}



================================================
FILE: travel-companion/client/ios/TravelCompanionUITests/TravelCompanionUITests.swift
================================================
import XCTest

final class TravelCompanionUITests: XCTestCase {

    override func setUpWithError() throws {
        // Put setup code here. This method is called before the invocation of each test method in the class.

        // In UI tests it is usually best to stop immediately when a failure occurs.
        continueAfterFailure = false

        // In UI tests it’s important to set the initial state - such as interface orientation - required for your tests before they run. The setUp method is a good place to do this.
    }

    override func tearDownWithError() throws {
        // Put teardown code here. This method is called after the invocation of each test method in the class.
    }

    func testExample() throws {
        // UI tests must launch the application that they test.
        let app = XCUIApplication()
        app.launch()

        // Use XCTAssert and related functions to verify your tests produce the correct results.
    }

    func testLaunchPerformance() throws {
        if #available(macOS 10.15, iOS 13.0, tvOS 13.0, watchOS 7.0, *) {
            // This measures how long it takes to launch your application.
            measure(metrics: [XCTApplicationLaunchMetric()]) {
                XCUIApplication().launch()
            }
        }
    }
}



================================================
FILE: travel-companion/client/ios/TravelCompanionUITests/TravelCompanionUITestsLaunchTests.swift
================================================
import XCTest

final class TravelCompanionUITestsLaunchTests: XCTestCase {

    override class var runsForEachTargetApplicationUIConfiguration: Bool {
        true
    }

    override func setUpWithError() throws {
        continueAfterFailure = false
    }

    func testLaunch() throws {
        let app = XCUIApplication()
        app.launch()

        // Insert steps here to perform after app launch but before taking a screenshot,
        // such as logging into a test account or navigating somewhere in the app

        let attachment = XCTAttachment(screenshot: app.screenshot())
        attachment.name = "Launch Screen"
        attachment.lifetime = .keepAlways
        add(attachment)
    }
}



================================================
FILE: travel-companion/server/bot-gemini.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Gemini Travel Companion

This module implements a chatbot using Google's Gemini Live model.
It includes:
- Real-time audio interaction through Daily
- Function calling
- Google search

The bot runs as part of a pipeline that processes audio frames and manages
the conversation flow using Gemini's streaming capabilities.
"""

import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.adapters.schemas.function_schema import FunctionSchema
from pipecat.adapters.schemas.tools_schema import AdapterType, ToolsSchema
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.daily.transport import DailyParams

load_dotenv(override=True)

# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        video_in_enabled=True,
        # set stop_secs to something roughly similar to the internal setting
        # of the Gemini Live api, just to align events. This doesn't really
        # matter because we can only use the Gemini Live API's phrase
        # endpointing, for now.
        vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.5)),
    ),
}

# Search tool can only be used together with other tools when using the Gemini Live API
# Otherwise it should be used alone.
# We are registering the tools here, but who are handling them is the RTVI client
get_location_function = FunctionSchema(
    name="get_my_current_location",
    description="Retrieves the user current location",
    properties={},
    required=[],
)

set_restaurant_function = FunctionSchema(
    name="set_restaurant_location",
    description="Sets the location of the chosen restaurant",
    properties={
        "restaurant": {
            "type": "string",
            "description": "Restaurant name",
        },
        "lat": {
            "type": "string",
            "description": "Latitude of the location",
        },
        "lon": {
            "type": "string",
            "description": "Longitude of the location",
        },
        "address": {
            "type": "string",
            "description": "Complete address of the location in this format: {street, number, city}",
        },
    },
    required=["restaurant", "lat", "lon", "address"],
)

search_tool = {"google_search": {}}
tools = ToolsSchema(
    standard_tools=[get_location_function, set_restaurant_function],
    custom_tools={AdapterType.GEMINI: [search_tool]},
)


system_instruction = """
You are a travel companion, and your responses will be converted to audio, so keep them simple and avoid special characters or complex formatting.

You can:
- Use get_my_current_location to determine the user's current location. Once retrieved, inform the user of the city they are in, rather than providing coordinates.
- Use google_search to check the weather and share it with the user. Describe the temperature in Celsius and Fahrenheit.
- Use google_search to recommend restaurants that are nearby to the user's location, less than 10km. 
- Use set_restaurant_location to share the location of a selected restaurant with the user. Also check on google_search first for the precise location.
- Use google_search to provide recent and relevant news from the user's current location.

Answer any user questions with accurate, concise, and conversational responses.
"""


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    """Main bot execution function.

    Sets up and runs the bot pipeline including:
    - Gemini Live model integration
    - Voice activity detection
    - Animation processing
    - RTVI event handling
    """

    # Initialize the Gemini Live model
    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        system_instruction=system_instruction,
        tools=tools,
    )

    messages = [
        {
            "role": "user",
            "content": "Start by briefly introduction yourself and tell me what you can do.",
        },
    ]
    # Set up conversation context and management
    # The context_aggregator will automatically collect conversation context
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    #
    # RTVI events for Pipecat client UI
    #
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    # Registering the functions to be invoked by RTVI
    llm.register_function(None, rtvi.handle_function_call)

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            context_aggregator.user(),
            llm,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    @rtvi.event_handler("on_client_message")
    async def on_client_message(rtvi, msg):
        print("RTVI client message:", msg.type, msg.data)
        # Sample message to show how it works
        if msg.type == "get-llm-vendor":
            await rtvi.send_server_response(msg, "Google")

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: travel-companion/server/env.example
================================================
GOOGLE_API_KEY=
DAILY_API_KEY=
DAILY_SAMPLE_ROOM_URL=


================================================
FILE: travel-companion/server/requirements.txt
================================================
pipecat-ai[daily,openai,silero,google,runner]>=0.0.82


================================================
FILE: twilio-chatbot/README.md
================================================
# Twilio Voice Bot Examples

This repository contains examples of voice bots that integrate with Twilio's Programmable Voice API using Pipecat. The examples demonstrate both inbound and outbound calling scenarios using Twilio Media Streams for real-time audio processing.

## Examples

### 🔽 [Inbound Calling](./inbound/)

Demonstrates how to handle incoming phone calls where users call your Twilio number and interact with a voice bot.

### 🔼 [Outbound Calling](./outbound/)

Shows how to initiate outbound phone calls programmatically where your bot calls users.

## Architecture

Both examples use the same core architecture:

```
Phone Call ↔ Twilio ↔ Media Streams (WebSocket) ↔ Pipecat ↔ AI Services
```

**Components:**

- **Twilio**: Handles phone call routing and audio transport
- **Media Streams**: Real-time bidirectional audio over WebSocket
- **Pipecat**: Audio processing pipeline and AI service orchestration
- **AI Services**: OpenAI (LLM), Deepgram (STT), Cartesia (TTS)

## Getting Help

- **Detailed Setup**: See individual README files in `inbound/` and `outbound/` directories
- **Pipecat Documentation**: [docs.pipecat.ai](https://docs.pipecat.ai)
- **Twilio Documentation**: [twilio.com/docs](https://www.twilio.com/docs)



================================================
FILE: twilio-chatbot/inbound/README.md
================================================
# Twilio Chatbot: Inbound

This project is a Pipecat-based chatbot that integrates with Twilio to handle inbound phone calls via WebSocket connections and provide real-time voice conversations.

## Table of Contents

- [How It Works](#how-it-works)
- [Prerequisites](#prerequisites)
- [Setup](#setup)
- [Environment Configuration](#environment-configuration)
- [Local Development](#local-development)
- [Production Deployment](#production-deployment)
- [Customizing your Bot](#customizing-your-bot)
- [Testing](#testing)

## How It Works

When someone calls your Twilio number:

1. **Twilio sends WebSocket messages**: Twilio processes the associated TwiML Bin and starts a WebSocket stream to your bot (local or Pipecat Cloud)
2. **Parse the WebSocket messages**: Your bot parses the WebSocket connection messages to set up the corresponding Pipecat transport
3. **(Optional) Look up the caller**: Optionally, look up the caller using Twilio's REST API to retrieve custom information about the call and personalize your bot's behavior
4. **Bot starts responding**: Once the pipeline is started, your bot will initiate the conversation

## Prerequisites

### Twilio

- A Twilio account with:
  - Account SID and Auth Token
  - A purchased phone number that supports voice calls

### AI Services

- Google API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

   ```sh
   cd inbound
   uv sync
   ```

2. Create an .env file and add API keys:

   ```sh
   cp env.example .env
   ```

## Local Development

### Configure Twilio

1. Start ngrok:
   In a new terminal, start ngrok to tunnel the local server:

   ```sh
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

2. Create a TwiML Bin:

   - Go to your Twilio Console: https://console.twilio.com/
   - Navigate to TwiML Bins > My TwiML Bins
   - Click the `+` to create a new TwiML Bin
   - Name your bin and add the TwiML containing your ngrok URL:

     ```xml
     <?xml version="1.0" encoding="UTF-8"?>
     <Response>
     <Connect>
        <Stream url="wss://your-url.ngrok.io/ws" />
     </Connect>
     </Response>
     ```

   - Click "Save"

3. Assign the TwiML Bin to your number:

   - Navigate to Phone Numbers > Manage > Active numbers
   - Click on your Twilio phone number
   - In the "Voice Configuration" section:
     - Set "A call comes in" to "TwiML Bin"
     - Select the name of your TwiML Bin from step 2
   - Click "Save configuration"

### Run your Bot

Run your bot by passing in the `twilio` command line arg

```bash
uv run bot.py --transport twilio
```

> Note: This bot uses [Pipecat's development runner](https://docs.pipecat.ai/server/utilities/runner/guide), which runs a FastAPI server that handles and routes incoming WebSocket messages to your bot.

### Call your Bot

Place a call to the number associated with your bot. The bot will answer and start the conversation.

## Production Deployment

To deploy your twilio-chatbot for inbound calling, we'll use [Pipecat Cloud](https://pipecat.daily.co/).

### Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) for tips on how to create secrets, build and push a docker image, and deploy your agent to Pipecat Cloud.

### Update TwiML for Production

Update your TwiML Bin to point directly to Pipecat Cloud's WebSocket endpoint.

In your Twilio Console, update the TwiML Bin to include your Pipecat Cloud agent and organization name:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Connect>
    <Stream url="wss://api.pipecat.daily.co/ws/twilio">
      <Parameter name="_pipecatCloudServiceHost"
         value="AGENT_NAME.ORGANIZATION_NAME"/>
    </Stream>
  </Connect>
</Response>
```

where:

- `AGENT_NAME` is the name of the agent that you deployed to Pipecat Cloud
- `ORGANIZATION_NAME` is the name of your Pipecat Cloud organization

> If the bot is deployed to a region other than us-west (default), update the websocket url with region. For example, if deployed in `eu-central`, the url becomes `"wss://eu-central.api.pipecat.daily.co/ws/twilio"`

### Call your Bot

Place a call to the number associated with your bot. The bot will answer and start the conversation.

## Customizing your Bot

The `bot.py` example file is configured to look up the caller's phone number by calling Twilio's REST API using the Call SID. With this information, you can:

- Perform a lookup in your own database to retrieve customer information
- Personalize the bot's greeting and behavior based on the caller

## Testing

It is also possible to test the server without making phone calls by using one of these clients:

- [python](client/python/README.md): This Python client enables automated testing of the server via WebSocket without the need to make actual phone calls.
- [typescript](client/typescript/README.md): This typescript client enables manual testing of the server via WebSocket without the need to make actual phone calls.



================================================
FILE: twilio-chatbot/inbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import datetime
import io
import os
import wave
from typing import Optional

import aiofiles
import aiohttp
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.audio.audio_buffer_processor import AudioBufferProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.twilio import TwilioFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)


async def get_call_info(call_sid: str) -> dict:
    """Fetch call information from Twilio REST API using aiohttp.

    Args:
        call_sid: The Twilio call SID

    Returns:
        Dictionary containing call information including from_number, to_number, status, etc.
    """
    account_sid = os.getenv("TWILIO_ACCOUNT_SID")
    auth_token = os.getenv("TWILIO_AUTH_TOKEN")

    if not account_sid or not auth_token:
        logger.warning("Missing Twilio credentials, cannot fetch call info")
        return {}

    url = f"https://api.twilio.com/2010-04-01/Accounts/{account_sid}/Calls/{call_sid}.json"

    try:
        # Use HTTP Basic Auth with aiohttp
        auth = aiohttp.BasicAuth(account_sid, auth_token)

        async with aiohttp.ClientSession() as session:
            async with session.get(url, auth=auth) as response:
                if response.status != 200:
                    error_text = await response.text()
                    logger.error(f"Twilio API error ({response.status}): {error_text}")
                    return {}

                data = await response.json()

                call_info = {
                    "from_number": data.get("from"),
                    "to_number": data.get("to"),
                }

                return call_info

    except Exception as e:
        logger.error(f"Error fetching call info from Twilio: {e}")
        return {}


async def save_audio(audio: bytes, sample_rate: int, num_channels: int):
    if len(audio) > 0:
        filename = f"recording_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
        with io.BytesIO() as buffer:
            with wave.open(buffer, "wb") as wf:
                wf.setsampwidth(2)
                wf.setnchannels(num_channels)
                wf.setframerate(sample_rate)
                wf.writeframes(audio)
            async with aiofiles.open(filename, "wb") as file:
                await file.write(buffer.getvalue())
        logger.info(f"Merged audio saved to {filename}")
    else:
        logger.info("No audio data to save")


async def run_bot(transport: BaseTransport, handle_sigint: bool, testing: bool):
    llm = GoogleLLMService(api_key=os.getenv("GOOGLE_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
        push_silence_after_stop=testing,
    )

    messages = [
        {
            "role": "system",
            "content": "You are an elementary teacher in an audio call. Your output will be converted to audio so don't include special characters in your answers. Respond to what the student said in a short short sentence.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # NOTE: Watch out! This will save all the conversation in memory. You can
    # pass `buffer_size` to get periodic callbacks.
    audiobuffer = AudioBufferProcessor()

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            audiobuffer,  # Used to buffer the audio in the pipeline
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Start recording.
        await audiobuffer.start_recording()
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        await task.cancel()

    @audiobuffer.event_handler("on_audio_data")
    async def on_audio_data(buffer, audio, sample_rate, num_channels):
        await save_audio(audio, sample_rate, num_channels)

    # We use `handle_sigint=False` because `uvicorn` is controlling keyboard
    # interruptions. We use `force_gc=True` to force garbage collection after
    # the runner finishes running a task which could be useful for long running
    # applications with multiple clients connecting.
    runner = PipelineRunner(handle_sigint=handle_sigint, force_gc=True)

    await runner.run(task)


async def bot(runner_args: RunnerArguments, testing: Optional[bool] = False):
    """Main bot entry point compatible with Pipecat Cloud."""

    _, call_data = await parse_telephony_websocket(runner_args.websocket)

    # Fetch call information from Twilio REST API
    # With the call information, you can make a request to your API to get the user's information
    # and inject that information into your bot's configuration.
    call_info = await get_call_info(call_data["call_id"])
    if call_info:
        logger.info(f"Call from: {call_info.get('from_number')} to: {call_info.get('to_number')}")

    serializer = TwilioFrameSerializer(
        stream_sid=call_data["stream_id"],
        call_sid=call_data["call_id"],
        account_sid=os.getenv("TWILIO_ACCOUNT_SID", ""),
        auth_token=os.getenv("TWILIO_AUTH_TOKEN", ""),
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    await run_bot(transport, runner_args.handle_sigint, testing)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: twilio-chatbot/inbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: twilio-chatbot/inbound/env.example
================================================
GOOGLE_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=



================================================
FILE: twilio-chatbot/inbound/pcc-deploy.toml
================================================
agent_name = "twilio-chatbot-dial-in"
image = "your_username/twilio-chatbot-dial-in:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "twilio-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: twilio-chatbot/inbound/pyproject.toml
================================================
[project]
name = "twilio-chatbot-dial-in"
version = "0.1.0"
description = "Twilio dial-in example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,google,silero,deepgram,runner]>=0.0.91",
  "pipecatcloud>=0.2.7",
]



================================================
FILE: twilio-chatbot/inbound/client/python/README.md
================================================
# Python Client for Server Testing

This Python client enables automated testing of the server via WebSocket without the need to make actual phone calls.

## Setup Instructions

### 1. Configure the Stream Template

Edit the `templates/streams.xml` file to point to your server’s WebSocket endpoint. For example:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<Response>
  <Connect>
    <Stream url="ws://localhost:7860/ws" />
  </Connect>
  <Pause length="40"/>
</Response>
```

### 2. Start the Server in Test Mode

Run the server with the `-t` flag to indicate test mode:

```sh
# Ensure you're in the project directory and your virtual environment is activated
python server.py -t
```

### 3. Run the Client

Start the client and point it to the server URL:

```sh
python client.py -u http://localhost:7860 -c 2
```

- `-u`: Server URL (default is `http://localhost:7860`)
- `-c`: Number of concurrent client connections (e.g., 2)



================================================
FILE: twilio-chatbot/inbound/client/python/client.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import asyncio
import datetime
import io
import os
import sys
import wave
import xml.etree.ElementTree as ET
from uuid import uuid4

import aiofiles
import aiohttp
from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import EndFrame, TransportMessageUrgentFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.audio.audio_buffer_processor import AudioBufferProcessor
from pipecat.serializers.twilio import TwilioFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.openai.llm import OpenAILLMService
from pipecat.transports.websocket.client import (
    WebsocketClientParams,
    WebsocketClientTransport,
)

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


DEFAULT_CLIENT_DURATION = 30


async def download_twiml(server_url: str) -> str:
    # TODO(aleix): add error checking.
    async with aiohttp.ClientSession() as session:
        async with session.post(server_url) as response:
            return await response.text()


def get_stream_url_from_twiml(twiml: str) -> str:
    root = ET.fromstring(twiml)
    # TODO(aleix): add error checking.
    stream_element = root.find(".//Stream")  # Finds the first <Stream> element
    url = stream_element.get("url")
    return url


async def save_audio(client_name: str, audio: bytes, sample_rate: int, num_channels: int):
    if len(audio) > 0:
        filename = (
            f"{client_name}_recording_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.wav"
        )
        with io.BytesIO() as buffer:
            with wave.open(buffer, "wb") as wf:
                wf.setsampwidth(2)
                wf.setnchannels(num_channels)
                wf.setframerate(sample_rate)
                wf.writeframes(audio)
            async with aiofiles.open(filename, "wb") as file:
                await file.write(buffer.getvalue())
        logger.info(f"Merged audio saved to {filename}")
    else:
        logger.info("No audio data to save")


async def run_client(client_name: str, server_url: str, duration_secs: int):
    twiml = await download_twiml(server_url)

    stream_url = get_stream_url_from_twiml(twiml)

    stream_sid = str(uuid4())
    call_sid = str(uuid4())

    transport = WebsocketClientTransport(
        uri=stream_url,
        params=WebsocketClientParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            serializer=TwilioFrameSerializer(stream_sid=stream_sid, call_sid=call_sid),
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=1.0)),
        ),
    )

    llm = OpenAILLMService(api_key=os.getenv("OPENAI_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="e13cae5c-ec59-4f71-b0a6-266df3c9bb8e",  # Madame Mischief
        push_silence_after_stop=True,
    )

    messages = [
        {
            "role": "system",
            "content": "You are an 8 year old child. A teacher will explain you new concepts you want to know about. Feel free to change topics whnever you want. Once you are taught something you need to keep asking for clarifications if you think someone your age would not understand what you are being taught.",
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # NOTE: Watch out! This will save all the conversation in memory. You can
    # pass `buffer_size` to get periodic callbacks.
    audiobuffer = AudioBufferProcessor()

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from server
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to server
            audiobuffer,  # Used to buffer the audio in the pipeline
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_connected")
    async def on_connected(transport: WebsocketClientTransport, client):
        # Start recording.
        await audiobuffer.start_recording()

        message = TransportMessageUrgentFrame(
            message={"event": "connected", "protocol": "Call", "version": "1.0.0"}
        )
        await transport.output().send_message(message)

        message = TransportMessageUrgentFrame(
            message={
                "event": "start",
                "streamSid": stream_sid,
                "callSid": call_sid,
                "start": {"streamSid": stream_sid, "callSid": call_sid},
            }
        )
        await transport.output().send_message(message)

    @audiobuffer.event_handler("on_audio_data")
    async def on_audio_data(buffer, audio, sample_rate, num_channels):
        await save_audio(client_name, audio, sample_rate, num_channels)

    async def end_call():
        await asyncio.sleep(duration_secs)
        logger.info(f"Client {client_name} finished after {duration_secs} seconds.")
        await task.queue_frame(EndFrame())

    runner = PipelineRunner()

    await asyncio.gather(runner.run(task), end_call())


async def main():
    parser = argparse.ArgumentParser(description="Pipecat Twilio Chatbot Client")
    parser.add_argument("-u", "--url", type=str, required=True, help="specify the server URL")
    parser.add_argument(
        "-c", "--clients", type=int, required=True, help="number of concurrent clients"
    )
    parser.add_argument(
        "-d",
        "--duration",
        type=int,
        default=DEFAULT_CLIENT_DURATION,
        help=f"duration of each client in seconds (default: {DEFAULT_CLIENT_DURATION})",
    )
    args, _ = parser.parse_known_args()

    clients = []
    for i in range(args.clients):
        clients.append(asyncio.create_task(run_client(f"client_{i}", args.url, args.duration)))
    await asyncio.gather(*clients)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: twilio-chatbot/inbound/client/typescript/README.md
================================================
# Typescript Client for Server Testing

This typescript client enables manual testing of the server via WebSocket without the need to make actual phone calls.

## Setup

1. Run the bot server. See the [server README](../../README).

2. Navigate to the `client/typescript` directory:

```bash
cd client/typescript
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: twilio-chatbot/inbound/client/typescript/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
<div class="container">
  <div class="status-bar">
    <div class="status">
      Transport: <span id="connection-status">Disconnected</span>
    </div>
    <div class="controls">
      <button id="connect-btn">Connect</button>
      <button id="disconnect-btn" disabled>Disconnect</button>
    </div>
  </div>

  <audio id="bot-audio" autoplay></audio>

  <div class="debug-panel">
    <h3>Debug Info</h3>
    <div id="debug-log"></div>
  </div>
</div>

<script type="module" src="/src/app.ts"></script>
<link rel="stylesheet" href="/src/style.css">
</body>

</html>



================================================
FILE: twilio-chatbot/inbound/client/typescript/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.13.1",
    "@types/protobufjs": "^6.0.0",
    "@vitejs/plugin-react-swc": "^3.7.2",
    "typescript": "^5.7.3",
    "vite": "^6.0.2"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/websocket-transport": "^1.2.0"
  }
}



================================================
FILE: twilio-chatbot/inbound/client/typescript/tsconfig.json
================================================
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */

    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */

    /* Language and Environment */
    "target": "es2016",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for legacy experimental decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */

    /* Modules */
    "module": "commonjs",                                /* Specify what module code is generated. */
    // "rootDir": "./",                                  /* Specify the root folder within your source files. */
    // "moduleResolution": "node10",                     /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "allowImportingTsExtensions": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */
    // "rewriteRelativeImportExtensions": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */
    // "resolvePackageJsonExports": true,                /* Use the package.json 'exports' field when resolving package imports. */
    // "resolvePackageJsonImports": true,                /* Use the package.json 'imports' field when resolving imports. */
    // "customConditions": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */
    // "noUncheckedSideEffectImports": true,             /* Check side effect imports. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "allowArbitraryExtensions": true,                 /* Enable importing files with any extension, provided a declaration file is present. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */

    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */

    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    // "outDir": "./",                                   /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */

    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "verbatimModuleSyntax": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */
    // "isolatedDeclarations": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true,            /* Ensure that casing is correct in imports. */

    /* Type Checking */
    "strict": true,                                      /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "strictBuiltinIteratorReturn": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */

    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true                                 /* Skip type checking all .d.ts files. */
  }
}



================================================
FILE: twilio-chatbot/inbound/client/typescript/vite.config.js
================================================
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

export default defineConfig({
  base: './', //Use relative paths so it works at any mount path
  plugins: [react()],
  server: {
    proxy: {
      '/ws': {
        target: 'ws://0.0.0.0:7860', // Replace with your backend URL
        changeOrigin: true,
      },
    },
  },
});



================================================
FILE: twilio-chatbot/inbound/client/typescript/src/app.ts
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

import {
  BotLLMTextData,
  Participant,
  PipecatClient,
  PipecatClientOptions,
  RTVIEvent,
  RTVIMessage,
  TranscriptData,
} from '@pipecat-ai/client-js';
import {
  WebSocketTransport,
  TwilioSerializer,
} from '@pipecat-ai/websocket-transport';

class WebsocketClientApp {
  private static STREAM_SID = 'ws_mock_stream_sid';
  private static CALL_SID = 'ws_mock_call_sid';

  private rtviClient: PipecatClient | null = null;
  private connectBtn: HTMLButtonElement | null = null;
  private disconnectBtn: HTMLButtonElement | null = null;
  private statusSpan: HTMLElement | null = null;
  private debugLog: HTMLElement | null = null;
  private botAudio: HTMLAudioElement;

  constructor() {
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    document.body.appendChild(this.botAudio);
    this.setupDOMElements();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      'connect-btn'
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      'disconnect-btn'
    ) as HTMLButtonElement;
    this.statusSpan = document.getElementById('connection-status');
    this.debugLog = document.getElementById('debug-log');
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  private setupEventListeners(): void {
    this.connectBtn?.addEventListener('click', () => this.connect());
    this.disconnectBtn?.addEventListener('click', () => this.disconnect());
  }

  /**
   * Add a timestamped message to the debug log
   */
  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3';
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50';
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  private async emulateTwilioMessages() {
    const connectedMessage = {
      event: 'connected',
      protocol: 'Call',
      version: '1.0.0',
    };

    const websocketTransport = this.rtviClient?.transport as WebSocketTransport;
    void websocketTransport?.sendRawMessage(connectedMessage);

    const startMessage = {
      event: 'start',
      start: {
        streamSid: WebsocketClientApp.STREAM_SID,
        callSid: WebsocketClientApp.CALL_SID,
      },
    };
    void websocketTransport?.sendRawMessage(startMessage);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.rtviClient) return;
    const tracks = this.rtviClient.tracks();
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.rtviClient) return;

    // Listen for new tracks starting
    this.rtviClient.on(
      RTVIEvent.TrackStarted,
      (track: MediaStreamTrack, participant?: Participant) => {
        // Only handle non-local (bot) tracks
        if (!participant?.local && track.kind === 'audio') {
          this.setupAudioTrack(track);
        }
      }
    );

    // Listen for tracks stopping
    this.rtviClient.on(
      RTVIEvent.TrackStopped,
      (track: MediaStreamTrack, participant?: Participant) => {
        this.log(
          `Track stopped: ${track.kind} from ${participant?.name || 'unknown'}`
        );
      }
    );
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  private setupAudioTrack(track: MediaStreamTrack): void {
    this.log('Setting up audio track');
    if (
      this.botAudio.srcObject &&
      'getAudioTracks' in this.botAudio.srcObject
    ) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the RTVI client, initializes devices, and establishes the connection
   */
  public async connect(): Promise<void> {
    try {
      const startTime = Date.now();

      const ws_opts = {
        serializer: new TwilioSerializer(),
        recorderSampleRate: 8000,
        playerSampleRate: 8000,
        wsUrl: 'http://localhost:7860/ws',
      };
      const pcConfig: PipecatClientOptions = {
        transport: new WebSocketTransport(ws_opts),
        enableMic: true,
        enableCam: false,
        callbacks: {
          onConnected: () => {
            this.emulateTwilioMessages();
            this.updateStatus('Connected');
            if (this.connectBtn) this.connectBtn.disabled = true;
            if (this.disconnectBtn) this.disconnectBtn.disabled = false;
          },
          onDisconnected: () => {
            this.updateStatus('Disconnected');
            if (this.connectBtn) this.connectBtn.disabled = false;
            if (this.disconnectBtn) this.disconnectBtn.disabled = true;
            this.log('Client disconnected');
          },
          onBotReady: (data: any) => {
            this.log(`Bot ready: ${JSON.stringify(data)}`);
            this.setupMediaTracks();
          },
          onUserTranscript: (data: TranscriptData) => {
            if (data.final) {
              this.log(`User: ${data.text}`);
            }
          },
          onBotTranscript: (data: BotLLMTextData) =>
            this.log(`Bot: ${data.text}`),
          onMessageError: (error: RTVIMessage) =>
            console.error('Message error:', error),
          onError: (error: RTVIMessage) => console.error('Error:', error),
        },
      };
      this.rtviClient = new PipecatClient(pcConfig);
      this.setupTrackListeners();

      this.log('Initializing devices...');
      await this.rtviClient.initDevices();

      this.log('Connecting to bot...');
      await this.rtviClient.connect();

      const timeTaken = Date.now() - startTime;
      this.log(`Connection complete, timeTaken: ${timeTaken}`);
    } catch (error) {
      this.log(`Error connecting: ${(error as Error).message}`);
      this.updateStatus('Error');
      // Clean up if there's an error
      if (this.rtviClient) {
        try {
          await this.rtviClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  public async disconnect(): Promise<void> {
    if (this.rtviClient) {
      try {
        await this.rtviClient.disconnect();
        this.rtviClient = null;
        if (
          this.botAudio.srcObject &&
          'getAudioTracks' in this.botAudio.srcObject
        ) {
          this.botAudio.srcObject
            .getAudioTracks()
            .forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }
      } catch (error) {
        this.log(`Error disconnecting: ${(error as Error).message}`);
      }
    }
  }
}

declare global {
  interface Window {
    WebsocketClientApp: typeof WebsocketClientApp;
  }
}

window.addEventListener('DOMContentLoaded', () => {
  window.WebsocketClientApp = WebsocketClientApp;
  new WebsocketClientApp();
});



================================================
FILE: twilio-chatbot/inbound/client/typescript/src/style.css
================================================
body {
    margin: 0;
    padding: 20px;
    font-family: Arial, sans-serif;
    background-color: #f0f0f0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
}

.status-bar {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px;
    background-color: #fff;
    border-radius: 8px;
    margin-bottom: 20px;
}

.controls button {
    padding: 8px 16px;
    margin-left: 10px;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}

#connect-btn {
    background-color: #4caf50;
    color: white;
}

#disconnect-btn {
    background-color: #f44336;
    color: white;
}

button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.main-content {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.bot-container {
    display: flex;
    flex-direction: column;
    align-items: center;
}

#bot-video-container {
    width: 640px;
    height: 360px;
    background-color: #e0e0e0;
    border-radius: 8px;
    margin: 20px auto;
    overflow: hidden;
    display: flex;
    align-items: center;
    justify-content: center;
}

#bot-video-container video {
    width: 100%;
    height: 100%;
    object-fit: cover;
}

.debug-panel {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
}

.debug-panel h3 {
    margin: 0 0 10px 0;
    font-size: 16px;
    font-weight: bold;
}

#debug-log {
    height: 500px;
    overflow-y: auto;
    background-color: #f8f8f8;
    padding: 10px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 12px;
    line-height: 1.4;
}



================================================
FILE: twilio-chatbot/outbound/README.md
================================================
# Twilio Chatbot: Outbound

This project is a Pipecat-based chatbot that integrates with Twilio to make outbound calls with personalized call information. The project includes FastAPI endpoints for initiating outbound calls and handling WebSocket connections with call context.

## How It Works

When you want to make an outbound call:

1. **Send POST request**: `POST /dialout` with a phone number to call
2. **Server initiates call**: Uses Twilio's REST API to make the outbound call
3. **Call answered**: When answered, Twilio fetches TwiML from your server's `/twiml` endpoint
4. **Server returns TwiML**: Tells Twilio to start a WebSocket stream to your bot
5. **WebSocket connection**: Audio streams between the called person and your bot
6. **Call information**: Phone numbers are passed via TwiML Parameters to your bot

## Architecture

```
curl request → /dialout endpoint → Twilio REST API → Call initiated →
TwiML fetched → WebSocket connection → Bot conversation
```

## Prerequisites

### Twilio

- A Twilio account with:
  - Account SID and Auth Token
  - A purchased phone number that supports voice calls

### AI Services

- Google API key for the LLM inference
- Deepgram API key for speech-to-text
- Cartesia API key for text-to-speech

### System

- Python 3.10+
- `uv` package manager
- ngrok (for local development)
- Docker (for production deployment)

## Setup

1. Set up a virtual environment and install dependencies:

```bash
cd outbound
uv sync
```

2. Get your Twilio credentials:

- **Account SID & Auth Token**: Found in your [Twilio Console Dashboard](https://console.twilio.com/)
- **Phone Number**: [Purchase a phone number](https://console.twilio.com/us1/develop/phone-numbers/manage/search) that supports voice calls

3. Set up environment variables:

```bash
cp env.example .env
# Edit .env with your API keys
```

## Environment Configuration

The bot supports two deployment modes controlled by the `ENV` variable:

### Local Development (`ENV=local`)

- Uses your local server or ngrok URL for WebSocket connections
- Default configuration for development and testing
- WebSocket connections go directly to your running server

### Production (`ENV=production`)

- Uses Pipecat Cloud WebSocket URLs automatically
- Requires `AGENT_NAME` and `ORGANIZATION_NAME` from your Pipecat Cloud deployment
- Set these when deploying to production environments
- WebSocket connections route through Pipecat Cloud infrastructure

## Local Development

1. Start the outbound bot server:

   ```bash
   uv run server.py
   ```

The server will start on port 7860.

2. Using a new terminal, expose your server to the internet (for development)

   ```bash
   ngrok http 7860
   ```

   > Tip: Use the `--subdomain` flag for a reusable ngrok URL.

   Copy the ngrok URL (e.g., `https://abc123.ngrok.io`) and update `LOCAL_SERVER_URL` in your `.env` file.

3. No additional Twilio configuration needed

   Unlike inbound calling, outbound calls don't require webhook configuration in the Twilio console. The server will make direct API calls to Twilio to initiate calls.

## Making an Outbound Call

With the server running and exposed via ngrok, you can initiate outbound calls:

```bash
curl -X POST https://your-ngrok-url.ngrok.io/dialout \
  -H "Content-Type: application/json" \
  -d '{
    "to_number": "+15551234567",
    "from_number": "+15559876543"
  }'
```

Replace:

- `your-ngrok-url.ngrok.io` with your actual ngrok URL
- `+15551234567` with the phone number to call (E.164 format)
- `+15559876543` with your Twilio phone number (E.164 format)

> Note: the `from_number` must be a phone number owned by your Twilio account

## Production Deployment

### 1. Deploy your Bot to Pipecat Cloud

Follow the [quickstart instructions](https://docs.pipecat.ai/getting-started/quickstart#step-2%3A-deploy-to-production) to deploy your bot to Pipecat Cloud.

### 2. Configure Production Environment

Update your production `.env` file with the Pipecat Cloud details:

```bash
# Set to production mode
ENV=production

# Your Pipecat Cloud deployment details
AGENT_NAME=your-agent-name
ORGANIZATION_NAME=your-org-name

# Keep your existing Twilio and AI service keys
```

### 3. Deploy the Server

The `server.py` handles outbound call initiation and should be deployed separately from your bot:

- **Bot**: Runs on Pipecat Cloud (handles the conversation)
- **Server**: Runs on your infrastructure (initiates calls, serves TwiML responses)

When `ENV=production`, the server automatically routes WebSocket connections to your Pipecat Cloud bot.

> Alternatively, you can test your Pipecat Cloud deployment by running your server locally.

### Call your Bot

As you did before, initiate a call via `curl` command to trigger your bot to dial a number.

## Accessing Call Information in Your Bot

Your bot automatically receives call information through Twilio Stream Parameters. In this example, the phone numbers (`to_number` and `from_number`) are passed as parameters and extracted by the `parse_telephony_websocket` function.

You can extend the `DialoutRequest` model in `server_utils.py` to include additional custom data (customer info, campaign data, etc.) and pass it through as stream parameters for personalized conversations. See `bot.py` for implementation details.



================================================
FILE: twilio-chatbot/outbound/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import parse_telephony_websocket
from pipecat.serializers.twilio import TwilioFrameSerializer
from pipecat.services.cartesia.tts import CartesiaTTSService
from pipecat.services.deepgram.stt import DeepgramSTTService
from pipecat.services.google.llm import GoogleLLMService
from pipecat.transports.base_transport import BaseTransport
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


async def run_bot(transport: BaseTransport, handle_sigint: bool):
    llm = GoogleLLMService(api_key=os.getenv("GOOGLE_API_KEY"))

    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))

    tts = CartesiaTTSService(
        api_key=os.getenv("CARTESIA_API_KEY"),
        voice_id="71a7ad14-091c-4e8e-a314-022ece01c121",  # British Reading Lady
    )

    messages = [
        {
            "role": "system",
            "content": (
                "You are a friendly assistant making an outbound phone call. Your responses will be read aloud, "
                "so keep them concise and conversational. Avoid special characters or formatting. "
                "Begin by politely greeting the person and explaining why you're calling."
            ),
        },
    ]

    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Websocket input from client
            stt,  # Speech-To-Text
            context_aggregator.user(),
            llm,  # LLM
            tts,  # Text-To-Speech
            transport.output(),  # Websocket output to client
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_in_sample_rate=8000,
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        # Kick off the outbound conversation, waiting for the user to speak first
        logger.info("Starting outbound call conversation")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Outbound call ended")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    transport_type, call_data = await parse_telephony_websocket(runner_args.websocket)
    logger.info(f"Auto-detected transport: {transport_type}")

    # Access custom stream parameters passed from TwiML
    # Use the body data to personalize the conversation
    # by loading customer data based on the to_number or from_number
    body_data = call_data.get("body", {})
    to_number = body_data.get("to_number")
    from_number = body_data.get("from_number")

    logger.info(f"Call metadata - To: {to_number}, From: {from_number}")

    serializer = TwilioFrameSerializer(
        stream_sid=call_data["stream_id"],
        call_sid=call_data["call_id"],
        account_sid=os.getenv("TWILIO_ACCOUNT_SID", ""),
        auth_token=os.getenv("TWILIO_AUTH_TOKEN", ""),
    )

    transport = FastAPIWebsocketTransport(
        websocket=runner_args.websocket,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=serializer,
        ),
    )

    handle_sigint = runner_args.handle_sigint

    await run_bot(transport, handle_sigint)



================================================
FILE: twilio-chatbot/outbound/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

# Copy the application code
COPY ./bot.py bot.py


================================================
FILE: twilio-chatbot/outbound/env.example
================================================
# Service keys
GOOGLE_API_KEY=
DEEPGRAM_API_KEY=
CARTESIA_API_KEY=

# Twilio credentials
TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=

# Environment configuration
ENV=local
AGENT_NAME=twilio-chatbot-dial-out
ORGANIZATION_NAME=
LOCAL_SERVER_URL=https://your-url.ngrok.io


================================================
FILE: twilio-chatbot/outbound/pcc-deploy.toml
================================================
agent_name = "twilio-chatbot-dial-out"
image = "your_username/twilio-chatbot-dial-out:0.1"
image_credentials = "your_dockerhub_image_pull_secret"
secret_set = "twilio-chatbot"

[scaling]
	min_agents = 1



================================================
FILE: twilio-chatbot/outbound/pyproject.toml
================================================
[project]
name = "twilio-chatbot-dial-out"
version = "0.1.0"
description = "Twilio dial-out example for Pipecat"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
  "pipecat-ai[websocket,cartesia,google,silero,deepgram,runner]>=0.0.91",
  "pipecatcloud>=0.2.7",
  "twilio"
]



================================================
FILE: twilio-chatbot/outbound/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""server.py

Webhook server to handle outbound call requests, initiate calls via Twilio API,
and handle subsequent WebSocket connections for Media Streams.
"""

import os

import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, Request, WebSocket
from fastapi.responses import HTMLResponse, JSONResponse
from loguru import logger
from server_utils import (
    DialoutResponse,
    dialout_request_from_request,
    generate_twiml,
    make_twilio_call,
    parse_twiml_request,
)

load_dotenv(override=True)


app = FastAPI()


@app.post("/dialout", response_model=DialoutResponse)
async def handle_dialout_request(request: Request) -> DialoutResponse:
    """Handle outbound call request and initiate call via Twilio.

    Args:
        request (Request): FastAPI request containing JSON with 'to_number' and 'from_number'.

    Returns:
        DialoutResponse: Response containing call_sid, status, and to_number.

    Raises:
        HTTPException: If request data is invalid or missing required fields.
    """
    logger.info("Received outbound call request")

    dialout_request = await dialout_request_from_request(request)

    call_result = await make_twilio_call(dialout_request)

    return DialoutResponse(
        call_sid=call_result.call_sid,
        status="call_initiated",
        to_number=call_result.to_number,
    )


@app.post("/twiml")
async def get_twiml(request: Request) -> HTMLResponse:
    """Return TwiML instructions for connecting call to WebSocket.

    This endpoint is called by Twilio when a call is initiated. It returns TwiML
    that instructs Twilio to connect the call to our WebSocket endpoint with
    stream parameters containing call metadata.

    Args:
        request (Request): FastAPI request containing Twilio form data with 'To' and 'From'.

    Returns:
        HTMLResponse: TwiML XML response with Stream connection instructions.
    """
    logger.info("Serving TwiML for outbound call")

    twiml_request = await parse_twiml_request(request)

    twiml_content = generate_twiml(twiml_request)

    return HTMLResponse(content=twiml_content, media_type="application/xml")


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """Handle WebSocket connection from Twilio Media Streams.

    This endpoint receives the WebSocket connection from Twilio's Media Streams
    and runs the bot to handle the voice conversation. Stream parameters passed
    from TwiML are available to the bot for customization.

    Args:
        websocket (WebSocket): FastAPI WebSocket connection from Twilio.
    """
    from bot import bot
    from pipecat.runner.types import WebSocketRunnerArguments

    await websocket.accept()
    logger.info("WebSocket connection accepted for outbound call")

    try:
        runner_args = WebSocketRunnerArguments(websocket=websocket)
        await bot(runner_args)
    except Exception as e:
        logger.error(f"Error in WebSocket endpoint: {e}")
        await websocket.close()


if __name__ == "__main__":
    # Run the server
    port = int(os.getenv("PORT", "7860"))
    logger.info(f"Starting Twilio outbound chatbot server on port {port}")
    uvicorn.run(app, host="0.0.0.0", port=port)



================================================
FILE: twilio-chatbot/outbound/server_utils.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from fastapi import HTTPException, Request
from loguru import logger
from pydantic import BaseModel
from twilio.rest import Client as TwilioClient
from twilio.twiml.voice_response import Connect, Stream, VoiceResponse


class DialoutRequest(BaseModel):
    """Request data for initiating a dial-out call.

    Add any custom data here needed for the call. For example,
    you may add customer information, campaign data, or call context.

    Attributes:
        to_number (str): The phone number to dial (E.164 format recommended).
        from_number (str): The Twilio phone number to call from (E.164 format).
    """

    to_number: str
    from_number: str


class TwilioCallResult(BaseModel):
    """Result of a Twilio call.

    Attributes:
        call_sid (str): The unique call SID of the initiated call.
        to_number (str): The phone number that was dialed.
    """

    call_sid: str
    to_number: str


class DialoutResponse(BaseModel):
    """Response from the dialout endpoint.

    Attributes:
        call_sid (str): The unique call SID of the initiated call.
        status (str): The status of the call initiation (e.g., "call_initiated").
        to_number (str): The phone number that was dialed.
    """

    call_sid: str
    status: str
    to_number: str


class TwimlRequest(BaseModel):
    """Request data for generating TwiML.

    Attributes:
        to_number (str): The phone number being called.
        from_number (str): The phone number calling from.
    """

    to_number: str
    from_number: str


async def dialout_request_from_request(request: Request) -> DialoutRequest:
    """Parse and validate dial-out request data.

    Args:
        request (Request): FastAPI request object containing JSON with dial-out data.

    Returns:
        DialoutRequest: Parsed and validated dial-out request.

    Raises:
        HTTPException: If required fields are missing or request data is invalid.
    """
    data = await request.json()
    try:
        return DialoutRequest.model_validate(data)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid request data: {str(e)}")


async def make_twilio_call(dialout_request: DialoutRequest) -> TwilioCallResult:
    """Initiate an outbound call via Twilio API.

    Creates a Twilio call that will request TwiML from the /twiml endpoint,
    which then connects the call to the WebSocket endpoint for bot handling.

    Args:
        dialout_request (DialoutRequest): Object containing call details including
            to_number and from_number.

    Returns:
        TwilioCallResult: Result containing the call SID and destination number.

    Raises:
        ValueError: If required environment variables are missing.
    """
    to_number = dialout_request.to_number
    from_number = dialout_request.from_number

    local_server_url = os.getenv("LOCAL_SERVER_URL")
    if not local_server_url:
        raise ValueError("Missing LOCAL_SERVER_URL")

    twiml_url = f"{local_server_url}/twiml"
    account_sid = os.getenv("TWILIO_ACCOUNT_SID")
    auth_token = os.getenv("TWILIO_AUTH_TOKEN")

    if not account_sid or not auth_token:
        raise ValueError("Missing Twilio credentials")

    # Create Twilio client and make the call
    client = TwilioClient(account_sid, auth_token)
    call = client.calls.create(to=to_number, from_=from_number, url=twiml_url, method="POST")

    return TwilioCallResult(call_sid=call.sid, to_number=to_number)


async def parse_twiml_request(request: Request) -> TwimlRequest:
    """Parse and validate TwiML request data from Twilio.

    Twilio sends webhook data as form-encoded data, not JSON. This function
    extracts the 'To' and 'From' phone numbers from the form data.

    Args:
        request (Request): FastAPI request object containing Twilio form data.

    Returns:
        TwimlRequest: Parsed TwiML request with phone number metadata.
    """
    # Twilio sends form data, not JSON
    form_data = await request.form()
    to_number = form_data.get("To")
    from_number = form_data.get("From")

    return TwimlRequest(to_number=to_number, from_number=from_number)


def get_websocket_url() -> str:
    """Get the appropriate WebSocket URL based on environment.

    Returns the local WebSocket URL for local development or the Pipecat Cloud
    URL for production deployments.

    Returns:
        str: WebSocket URL (wss://) for Twilio Media Streams to connect to.

    Raises:
        ValueError: If LOCAL_SERVER_URL is missing in local environment.
    """
    if os.getenv("ENV", "local").lower() == "local":
        local_server_url = os.getenv("LOCAL_SERVER_URL")
        if not local_server_url:
            raise ValueError("Missing LOCAL_SERVER_URL")
        # Convert https:// to wss://
        ws_url = local_server_url.replace("https://", "wss://")
        return f"{ws_url}/ws"
    else:
        print("If deployed in a region other than us-west (default), update websocket url!")

        ws_url = "wss://api.pipecat.daily.co/ws/twilio"
        # uncomment appropriate region url:
        # ws_url = wss://us-east.api.pipecat.daily.co/ws/twilio
        # ws_url = wss://eu-central.api.pipecat.daily.co/ws/twilio
        # ws_url = wss://ap-south.api.pipecat.daily.co/ws/twilio
        return ws_url


def generate_twiml(twiml_request: TwimlRequest) -> str:
    """Generate TwiML response with WebSocket Stream connection.

    Creates TwiML that instructs Twilio to connect the call to our WebSocket
    endpoint. Call metadata (to_number, from_number) is passed as stream
    parameters, making them available to the bot for customization.

    Args:
        twiml_request (TwimlRequest): Request containing call metadata (phone numbers).

    Returns:
        str: TwiML XML string with Stream connection and parameters.
    """
    websocket_url = get_websocket_url()
    logger.debug(f"Generating TwiML with WebSocket URL: {websocket_url}")

    # Create TwiML response
    response = VoiceResponse()
    connect = Connect()
    stream = Stream(url=websocket_url)

    # Add call metadata as stream parameters so the bot can access them
    # These will be available in the WebSocket 'start' message
    stream.parameter(name="to_number", value=twiml_request.to_number)
    stream.parameter(name="from_number", value=twiml_request.from_number)

    # Add Pipecat Cloud service host for production
    if os.getenv("ENV") == "production":
        agent_name = os.getenv("AGENT_NAME")
        org_name = os.getenv("ORGANIZATION_NAME")
        service_host = f"{agent_name}.{org_name}"
        stream.parameter(name="_pipecatCloudServiceHost", value=service_host)

    connect.append(stream)
    response.append(connect)
    response.pause(length=20)

    return str(response)



================================================
FILE: websocket/README.md
================================================
# Voice Agent

A Pipecat example demonstrating the simplest way to create a voice agent using `WebsocketTransport`.

## 🚀 Quick Start

### 1️⃣ Start the Bot Server

#### 🔧 Set Up the Environment
1. Create and activate a virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Configure environment variables:
   - Copy `env.example` to `.env`
   ```bash
   cp env.example .env
   ```
   - Add your API keys
   - Choose what do you wish to use, 'fast_api' or 'websocket_server'

#### ▶️ Run the Server
```bash
python server/server.py
```

### 3️⃣ Connect Using a Custom Client App

For client-side setup, refer to the:
- [Typescript Guide](client/README.md).

## ⚠️ Important Note
Ensure the bot server is running before using any client implementations.

## 📌 Requirements

- Python **3.10+**
- Node.js **16+** (for JavaScript components)
- Google API Key

---

### 💡 Notes
- Ensure all dependencies are installed before running the server.
- Check the `.env` file for missing configurations.

Happy coding! 🎉


================================================
FILE: websocket/client/README.md
================================================
# JavaScript Implementation

Basic implementation using the [Pipecat JavaScript SDK](https://docs.pipecat.ai/client/js/introduction).

## Setup

1. Run the bot server. See the [server README](../README).

2. Navigate to the `client` directory:

```bash
cd client
```

3. Install dependencies:

```bash
npm install
```

4. Run the client app:

```
npm run dev
```

5. Visit http://localhost:5173 in your browser.



================================================
FILE: websocket/client/index.html
================================================
<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Chatbot</title>
</head>

<body>
<div class="container">
  <div class="status-bar">
    <div class="status">
      Transport: <span id="connection-status">Disconnected</span>
    </div>
    <div class="controls">
      <button id="connect-btn">Connect</button>
      <button id="disconnect-btn" disabled>Disconnect</button>
    </div>
  </div>

  <audio id="bot-audio" autoplay></audio>

  <div class="debug-panel">
    <h3>Debug Info</h3>
    <div id="debug-log"></div>
  </div>
</div>

<script type="module" src="/src/app.ts"></script>
<link rel="stylesheet" href="/src/style.css">
</body>

</html>



================================================
FILE: websocket/client/package.json
================================================
{
  "name": "client",
  "version": "1.0.0",
  "main": "index.js",
  "scripts": {
    "dev": "vite",
    "build": "tsc && vite build",
    "preview": "vite preview"
  },
  "keywords": [],
  "author": "",
  "license": "ISC",
  "description": "",
  "devDependencies": {
    "@types/node": "^22.15.30",
    "@types/protobufjs": "^6.0.0",
    "@vitejs/plugin-react-swc": "^3.10.1",
    "typescript": "^5.8.3",
    "vite": "^6.3.5"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.2.0",
    "@pipecat-ai/websocket-transport": "^1.2.0",
    "protobufjs": "^7.4.0"
  }
}



================================================
FILE: websocket/client/tsconfig.json
================================================
{
  "compilerOptions": {
    /* Visit https://aka.ms/tsconfig to read more about this file */

    /* Projects */
    // "incremental": true,                              /* Save .tsbuildinfo files to allow for incremental compilation of projects. */
    // "composite": true,                                /* Enable constraints that allow a TypeScript project to be used with project references. */
    // "tsBuildInfoFile": "./.tsbuildinfo",              /* Specify the path to .tsbuildinfo incremental compilation file. */
    // "disableSourceOfProjectReferenceRedirect": true,  /* Disable preferring source files instead of declaration files when referencing composite projects. */
    // "disableSolutionSearching": true,                 /* Opt a project out of multi-project reference checking when editing. */
    // "disableReferencedProjectLoad": true,             /* Reduce the number of projects loaded automatically by TypeScript. */

    /* Language and Environment */
    "target": "es2016",                                  /* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
    // "lib": [],                                        /* Specify a set of bundled library declaration files that describe the target runtime environment. */
    // "jsx": "preserve",                                /* Specify what JSX code is generated. */
    // "experimentalDecorators": true,                   /* Enable experimental support for legacy experimental decorators. */
    // "emitDecoratorMetadata": true,                    /* Emit design-type metadata for decorated declarations in source files. */
    // "jsxFactory": "",                                 /* Specify the JSX factory function used when targeting React JSX emit, e.g. 'React.createElement' or 'h'. */
    // "jsxFragmentFactory": "",                         /* Specify the JSX Fragment reference used for fragments when targeting React JSX emit e.g. 'React.Fragment' or 'Fragment'. */
    // "jsxImportSource": "",                            /* Specify module specifier used to import the JSX factory functions when using 'jsx: react-jsx*'. */
    // "reactNamespace": "",                             /* Specify the object invoked for 'createElement'. This only applies when targeting 'react' JSX emit. */
    // "noLib": true,                                    /* Disable including any library files, including the default lib.d.ts. */
    // "useDefineForClassFields": true,                  /* Emit ECMAScript-standard-compliant class fields. */
    // "moduleDetection": "auto",                        /* Control what method is used to detect module-format JS files. */

    /* Modules */
    "module": "commonjs",                                /* Specify what module code is generated. */
    // "rootDir": "./",                                  /* Specify the root folder within your source files. */
    // "moduleResolution": "node10",                     /* Specify how TypeScript looks up a file from a given module specifier. */
    // "baseUrl": "./",                                  /* Specify the base directory to resolve non-relative module names. */
    // "paths": {},                                      /* Specify a set of entries that re-map imports to additional lookup locations. */
    // "rootDirs": [],                                   /* Allow multiple folders to be treated as one when resolving modules. */
    // "typeRoots": [],                                  /* Specify multiple folders that act like './node_modules/@types'. */
    // "types": [],                                      /* Specify type package names to be included without being referenced in a source file. */
    // "allowUmdGlobalAccess": true,                     /* Allow accessing UMD globals from modules. */
    // "moduleSuffixes": [],                             /* List of file name suffixes to search when resolving a module. */
    // "allowImportingTsExtensions": true,               /* Allow imports to include TypeScript file extensions. Requires '--moduleResolution bundler' and either '--noEmit' or '--emitDeclarationOnly' to be set. */
    // "rewriteRelativeImportExtensions": true,          /* Rewrite '.ts', '.tsx', '.mts', and '.cts' file extensions in relative import paths to their JavaScript equivalent in output files. */
    // "resolvePackageJsonExports": true,                /* Use the package.json 'exports' field when resolving package imports. */
    // "resolvePackageJsonImports": true,                /* Use the package.json 'imports' field when resolving imports. */
    // "customConditions": [],                           /* Conditions to set in addition to the resolver-specific defaults when resolving imports. */
    // "noUncheckedSideEffectImports": true,             /* Check side effect imports. */
    // "resolveJsonModule": true,                        /* Enable importing .json files. */
    // "allowArbitraryExtensions": true,                 /* Enable importing files with any extension, provided a declaration file is present. */
    // "noResolve": true,                                /* Disallow 'import's, 'require's or '<reference>'s from expanding the number of files TypeScript should add to a project. */

    /* JavaScript Support */
    // "allowJs": true,                                  /* Allow JavaScript files to be a part of your program. Use the 'checkJS' option to get errors from these files. */
    // "checkJs": true,                                  /* Enable error reporting in type-checked JavaScript files. */
    // "maxNodeModuleJsDepth": 1,                        /* Specify the maximum folder depth used for checking JavaScript files from 'node_modules'. Only applicable with 'allowJs'. */

    /* Emit */
    // "declaration": true,                              /* Generate .d.ts files from TypeScript and JavaScript files in your project. */
    // "declarationMap": true,                           /* Create sourcemaps for d.ts files. */
    // "emitDeclarationOnly": true,                      /* Only output d.ts files and not JavaScript files. */
    // "sourceMap": true,                                /* Create source map files for emitted JavaScript files. */
    // "inlineSourceMap": true,                          /* Include sourcemap files inside the emitted JavaScript. */
    // "noEmit": true,                                   /* Disable emitting files from a compilation. */
    // "outFile": "./",                                  /* Specify a file that bundles all outputs into one JavaScript file. If 'declaration' is true, also designates a file that bundles all .d.ts output. */
    // "outDir": "./",                                   /* Specify an output folder for all emitted files. */
    // "removeComments": true,                           /* Disable emitting comments. */
    // "importHelpers": true,                            /* Allow importing helper functions from tslib once per project, instead of including them per-file. */
    // "downlevelIteration": true,                       /* Emit more compliant, but verbose and less performant JavaScript for iteration. */
    // "sourceRoot": "",                                 /* Specify the root path for debuggers to find the reference source code. */
    // "mapRoot": "",                                    /* Specify the location where debugger should locate map files instead of generated locations. */
    // "inlineSources": true,                            /* Include source code in the sourcemaps inside the emitted JavaScript. */
    // "emitBOM": true,                                  /* Emit a UTF-8 Byte Order Mark (BOM) in the beginning of output files. */
    // "newLine": "crlf",                                /* Set the newline character for emitting files. */
    // "stripInternal": true,                            /* Disable emitting declarations that have '@internal' in their JSDoc comments. */
    // "noEmitHelpers": true,                            /* Disable generating custom helper functions like '__extends' in compiled output. */
    // "noEmitOnError": true,                            /* Disable emitting files if any type checking errors are reported. */
    // "preserveConstEnums": true,                       /* Disable erasing 'const enum' declarations in generated code. */
    // "declarationDir": "./",                           /* Specify the output directory for generated declaration files. */

    /* Interop Constraints */
    // "isolatedModules": true,                          /* Ensure that each file can be safely transpiled without relying on other imports. */
    // "verbatimModuleSyntax": true,                     /* Do not transform or elide any imports or exports not marked as type-only, ensuring they are written in the output file's format based on the 'module' setting. */
    // "isolatedDeclarations": true,                     /* Require sufficient annotation on exports so other tools can trivially generate declaration files. */
    // "allowSyntheticDefaultImports": true,             /* Allow 'import x from y' when a module doesn't have a default export. */
    "esModuleInterop": true,                             /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
    // "preserveSymlinks": true,                         /* Disable resolving symlinks to their realpath. This correlates to the same flag in node. */
    "forceConsistentCasingInFileNames": true,            /* Ensure that casing is correct in imports. */

    /* Type Checking */
    "strict": true,                                      /* Enable all strict type-checking options. */
    // "noImplicitAny": true,                            /* Enable error reporting for expressions and declarations with an implied 'any' type. */
    // "strictNullChecks": true,                         /* When type checking, take into account 'null' and 'undefined'. */
    // "strictFunctionTypes": true,                      /* When assigning functions, check to ensure parameters and the return values are subtype-compatible. */
    // "strictBindCallApply": true,                      /* Check that the arguments for 'bind', 'call', and 'apply' methods match the original function. */
    // "strictPropertyInitialization": true,             /* Check for class properties that are declared but not set in the constructor. */
    // "strictBuiltinIteratorReturn": true,              /* Built-in iterators are instantiated with a 'TReturn' type of 'undefined' instead of 'any'. */
    // "noImplicitThis": true,                           /* Enable error reporting when 'this' is given the type 'any'. */
    // "useUnknownInCatchVariables": true,               /* Default catch clause variables as 'unknown' instead of 'any'. */
    // "alwaysStrict": true,                             /* Ensure 'use strict' is always emitted. */
    // "noUnusedLocals": true,                           /* Enable error reporting when local variables aren't read. */
    // "noUnusedParameters": true,                       /* Raise an error when a function parameter isn't read. */
    // "exactOptionalPropertyTypes": true,               /* Interpret optional property types as written, rather than adding 'undefined'. */
    // "noImplicitReturns": true,                        /* Enable error reporting for codepaths that do not explicitly return in a function. */
    // "noFallthroughCasesInSwitch": true,               /* Enable error reporting for fallthrough cases in switch statements. */
    // "noUncheckedIndexedAccess": true,                 /* Add 'undefined' to a type when accessed using an index. */
    // "noImplicitOverride": true,                       /* Ensure overriding members in derived classes are marked with an override modifier. */
    // "noPropertyAccessFromIndexSignature": true,       /* Enforces using indexed accessors for keys declared using an indexed type. */
    // "allowUnusedLabels": true,                        /* Disable error reporting for unused labels. */
    // "allowUnreachableCode": true,                     /* Disable error reporting for unreachable code. */

    /* Completeness */
    // "skipDefaultLibCheck": true,                      /* Skip type checking .d.ts files that are included with TypeScript. */
    "skipLibCheck": true                                 /* Skip type checking all .d.ts files. */
  }
}



================================================
FILE: websocket/client/vite.config.js
================================================
import { defineConfig } from 'vite';
import react from '@vitejs/plugin-react-swc';

export default defineConfig({
    plugins: [react()],
    server: {
        proxy: {
            // Proxy /api requests to the backend server
            '/connect': {
                target: 'http://0.0.0.0:7860', // Replace with your backend URL
                changeOrigin: true,
            },
        },
    },
});



================================================
FILE: websocket/client/src/app.ts
================================================
/**
 * Copyright (c) 2024–2025, Daily
 *
 * SPDX-License-Identifier: BSD 2-Clause License
 */

/**
 * Pipecat Client Implementation
 *
 * This client connects to an RTVI-compatible bot server using WebSocket.
 *
 * Requirements:
 * - A running RTVI bot server (defaults to http://localhost:7860)
 */

import {
  PipecatClient,
  PipecatClientOptions,
  RTVIEvent,
} from '@pipecat-ai/client-js';
import { WebSocketTransport } from '@pipecat-ai/websocket-transport';

class WebsocketClientApp {
  private pcClient: PipecatClient | null = null;
  private connectBtn: HTMLButtonElement | null = null;
  private disconnectBtn: HTMLButtonElement | null = null;
  private statusSpan: HTMLElement | null = null;
  private debugLog: HTMLElement | null = null;
  private botAudio: HTMLAudioElement;

  constructor() {
    console.log('WebsocketClientApp');
    this.botAudio = document.createElement('audio');
    this.botAudio.autoplay = true;
    //this.botAudio.playsInline = true;
    document.body.appendChild(this.botAudio);

    this.setupDOMElements();
    this.setupEventListeners();
  }

  /**
   * Set up references to DOM elements and create necessary media elements
   */
  private setupDOMElements(): void {
    this.connectBtn = document.getElementById(
      'connect-btn'
    ) as HTMLButtonElement;
    this.disconnectBtn = document.getElementById(
      'disconnect-btn'
    ) as HTMLButtonElement;
    this.statusSpan = document.getElementById('connection-status');
    this.debugLog = document.getElementById('debug-log');
  }

  /**
   * Set up event listeners for connect/disconnect buttons
   */
  private setupEventListeners(): void {
    this.connectBtn?.addEventListener('click', () => this.connect());
    this.disconnectBtn?.addEventListener('click', () => this.disconnect());
  }

  /**
   * Add a timestamped message to the debug log
   */
  private log(message: string): void {
    if (!this.debugLog) return;
    const entry = document.createElement('div');
    entry.textContent = `${new Date().toISOString()} - ${message}`;
    if (message.startsWith('User: ')) {
      entry.style.color = '#2196F3';
    } else if (message.startsWith('Bot: ')) {
      entry.style.color = '#4CAF50';
    }
    this.debugLog.appendChild(entry);
    this.debugLog.scrollTop = this.debugLog.scrollHeight;
    console.log(message);
  }

  /**
   * Update the connection status display
   */
  private updateStatus(status: string): void {
    if (this.statusSpan) {
      this.statusSpan.textContent = status;
    }
    this.log(`Status: ${status}`);
  }

  /**
   * Check for available media tracks and set them up if present
   * This is called when the bot is ready or when the transport state changes to ready
   */
  setupMediaTracks() {
    if (!this.pcClient) return;
    const tracks = this.pcClient.tracks();
    if (tracks.bot?.audio) {
      this.setupAudioTrack(tracks.bot.audio);
    }
  }

  /**
   * Set up listeners for track events (start/stop)
   * This handles new tracks being added during the session
   */
  setupTrackListeners() {
    if (!this.pcClient) return;

    // Listen for new tracks starting
    this.pcClient.on(RTVIEvent.TrackStarted, (track, participant) => {
      // Only handle non-local (bot) tracks
      if (!participant?.local && track.kind === 'audio') {
        this.setupAudioTrack(track);
      }
    });

    // Listen for tracks stopping
    this.pcClient.on(RTVIEvent.TrackStopped, (track, participant) => {
      this.log(
        `Track stopped: ${track.kind} from ${participant?.name || 'unknown'}`
      );
    });
  }

  /**
   * Set up an audio track for playback
   * Handles both initial setup and track updates
   */
  private setupAudioTrack(track: MediaStreamTrack): void {
    this.log('Setting up audio track');
    if (
      this.botAudio.srcObject &&
      'getAudioTracks' in this.botAudio.srcObject
    ) {
      const oldTrack = this.botAudio.srcObject.getAudioTracks()[0];
      if (oldTrack?.id === track.id) return;
    }
    this.botAudio.srcObject = new MediaStream([track]);
  }

  /**
   * Initialize and connect to the bot
   * This sets up the Pipecat client, initializes devices, and establishes the connection
   */
  public async connect(): Promise<void> {
    try {
      const startTime = Date.now();

      //const transport = new DailyTransport();
      const PipecatConfig: PipecatClientOptions = {
        transport: new WebSocketTransport(),
        enableMic: true,
        enableCam: false,
        callbacks: {
          onConnected: () => {
            this.updateStatus('Connected');
            if (this.connectBtn) this.connectBtn.disabled = true;
            if (this.disconnectBtn) this.disconnectBtn.disabled = false;
          },
          onDisconnected: () => {
            this.updateStatus('Disconnected');
            if (this.connectBtn) this.connectBtn.disabled = false;
            if (this.disconnectBtn) this.disconnectBtn.disabled = true;
            this.log('Client disconnected');
          },
          onBotReady: (data) => {
            this.log(`Bot ready: ${JSON.stringify(data)}`);
            this.setupMediaTracks();
          },
          onUserTranscript: (data) => {
            if (data.final) {
              this.log(`User: ${data.text}`);
            }
          },
          onBotTranscript: (data) => this.log(`Bot: ${data.text}`),
          onMessageError: (error) => console.error('Message error:', error),
          onError: (error) => console.error('Error:', error),
        },
      };
      this.pcClient = new PipecatClient(PipecatConfig);
      // @ts-ignore
      window.pcClient = this.pcClient; // Expose for debugging
      this.setupTrackListeners();

      this.log('Initializing devices...');
      await this.pcClient.initDevices();

      this.log('Connecting to bot...');
      await this.pcClient.startBotAndConnect({
        // The baseURL and endpoint of your bot server that the client will connect to
        endpoint: 'http://localhost:7860/connect',
      });

      const timeTaken = Date.now() - startTime;
      this.log(`Connection complete, timeTaken: ${timeTaken}`);
    } catch (error) {
      this.log(`Error connecting: ${(error as Error).message}`);
      this.updateStatus('Error');
      // Clean up if there's an error
      if (this.pcClient) {
        try {
          await this.pcClient.disconnect();
        } catch (disconnectError) {
          this.log(`Error during disconnect: ${disconnectError}`);
        }
      }
    }
  }

  /**
   * Disconnect from the bot and clean up media resources
   */
  public async disconnect(): Promise<void> {
    if (this.pcClient) {
      try {
        await this.pcClient.disconnect();
        this.pcClient = null;
        if (
          this.botAudio.srcObject &&
          'getAudioTracks' in this.botAudio.srcObject
        ) {
          this.botAudio.srcObject
            .getAudioTracks()
            .forEach((track) => track.stop());
          this.botAudio.srcObject = null;
        }
      } catch (error) {
        this.log(`Error disconnecting: ${(error as Error).message}`);
      }
    }
  }
}

declare global {
  interface Window {
    WebsocketClientApp: typeof WebsocketClientApp;
  }
}

window.addEventListener('DOMContentLoaded', () => {
  window.WebsocketClientApp = WebsocketClientApp;
  new WebsocketClientApp();
});



================================================
FILE: websocket/client/src/style.css
================================================
body {
    margin: 0;
    padding: 20px;
    font-family: Arial, sans-serif;
    background-color: #f0f0f0;
}

.container {
    max-width: 1200px;
    margin: 0 auto;
}

.status-bar {
    display: flex;
    justify-content: space-between;
    align-items: center;
    padding: 10px;
    background-color: #fff;
    border-radius: 8px;
    margin-bottom: 20px;
}

.controls button {
    padding: 8px 16px;
    margin-left: 10px;
    border: none;
    border-radius: 4px;
    cursor: pointer;
}

#connect-btn {
    background-color: #4caf50;
    color: white;
}

#disconnect-btn {
    background-color: #f44336;
    color: white;
}

button:disabled {
    opacity: 0.5;
    cursor: not-allowed;
}

.main-content {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
    margin-bottom: 20px;
}

.bot-container {
    display: flex;
    flex-direction: column;
    align-items: center;
}

#bot-video-container {
    width: 640px;
    height: 360px;
    background-color: #e0e0e0;
    border-radius: 8px;
    margin: 20px auto;
    overflow: hidden;
    display: flex;
    align-items: center;
    justify-content: center;
}

#bot-video-container video {
    width: 100%;
    height: 100%;
    object-fit: cover;
}

.debug-panel {
    background-color: #fff;
    border-radius: 8px;
    padding: 20px;
}

.debug-panel h3 {
    margin: 0 0 10px 0;
    font-size: 16px;
    font-weight: bold;
}

#debug-log {
    height: 500px;
    overflow-y: auto;
    background-color: #f8f8f8;
    padding: 10px;
    border-radius: 4px;
    font-family: monospace;
    font-size: 12px;
    line-height: 1.4;
}



================================================
FILE: websocket/server/bot_fast_api.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import os
import sys

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
    FastAPIWebsocketTransport,
)

load_dotenv(override=True)

logger.remove(0)
logger.add(sys.stderr, level="DEBUG")


SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


async def run_bot(websocket_client):
    ws_transport = FastAPIWebsocketTransport(
        websocket=websocket_client,
        params=FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            serializer=ProtobufFrameSerializer(),
        ),
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_model_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
    )

    context = LLMContext(
        [
            {
                "role": "user",
                "content": "Start by greeting the user warmly and introducing yourself.",
            }
        ],
    )
    context_aggregator = LLMContextAggregatorPair(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            ws_transport.input(),
            context_aggregator.user(),
            rtvi,
            llm,  # LLM
            ws_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.info("Pipecat client ready.")
        await rtvi.set_bot_ready()
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @ws_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")

    @ws_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)



================================================
FILE: websocket/server/bot_websocket_server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import os

from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.frameworks.rtvi import RTVIConfig, RTVIObserver, RTVIProcessor
from pipecat.serializers.protobuf import ProtobufFrameSerializer
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.websocket.server import (
    WebsocketServerParams,
    WebsocketServerTransport,
)

SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


async def run_bot_websocket_server():
    ws_transport = WebsocketServerTransport(
        params=WebsocketServerParams(
            serializer=ProtobufFrameSerializer(),
            audio_in_enabled=True,
            audio_out_enabled=True,
            add_wav_header=False,
            vad_analyzer=SileroVADAnalyzer(),
            session_timeout=60 * 3,  # 3 minutes
        )
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        transcribe_model_audio=True,
        system_instruction=SYSTEM_INSTRUCTION,
    )

    context = LLMContext(
        [
            {
                "role": "user",
                "content": "Start by greeting the user warmly and introducing yourself.",
            }
        ],
    )
    context_aggregator = LLMContextAggregatorPair(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            ws_transport.input(),
            context_aggregator.user(),
            rtvi,
            llm,  # LLM
            ws_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.info("Pipecat client ready.")
        await rtvi.set_bot_ready()
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @ws_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")

    @ws_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    @ws_transport.event_handler("on_session_timeout")
    async def on_session_timeout(transport, client):
        logger.info(f"Entering in timeout for {client.remote_address}")
        await task.cancel()

    runner = PipelineRunner()

    await runner.run(task)



================================================
FILE: websocket/server/env.example
================================================
GOOGLE_API_KEY=
WEBSOCKET_SERVER= # Options: 'fast_api' or 'websocket_server'


================================================
FILE: websocket/server/requirements.txt
================================================
python-dotenv
fastapi[all]
uvicorn
pipecat-ai[silero,websocket,google]>=0.0.82



================================================
FILE: websocket/server/server.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import asyncio
import os
from contextlib import asynccontextmanager
from typing import Any, Dict

import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, Request, WebSocket
from fastapi.middleware.cors import CORSMiddleware

# Load environment variables
load_dotenv(override=True)

from bot_fast_api import run_bot
from bot_websocket_server import run_bot_websocket_server


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Handles FastAPI startup and shutdown."""
    yield  # Run app


# Initialize FastAPI app with lifespan manager
app = FastAPI(lifespan=lifespan)

# Configure CORS to allow requests from any origin
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    print("WebSocket connection accepted")
    try:
        await run_bot(websocket)
    except Exception as e:
        print(f"Exception in run_bot: {e}")


@app.post("/connect")
async def bot_connect(request: Request) -> Dict[Any, Any]:
    server_mode = os.getenv("WEBSOCKET_SERVER", "fast_api")
    if server_mode == "websocket_server":
        ws_url = "ws://localhost:8765"
    else:
        ws_url = "ws://localhost:7860/ws"
    return {"ws_url": ws_url}


async def main():
    server_mode = os.getenv("WEBSOCKET_SERVER", "fast_api")
    tasks = []
    try:
        if server_mode == "websocket_server":
            tasks.append(run_bot_websocket_server())

        config = uvicorn.Config(app, host="0.0.0.0", port=7860)
        server = uvicorn.Server(config)
        tasks.append(server.serve())

        await asyncio.gather(*tasks)
    except asyncio.CancelledError:
        print("Tasks cancelled (probably due to shutdown).")


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: whatsapp/README.md
================================================
# WhatsApp WebRTC Bot

A real-time voice bot that integrates with WhatsApp Business API to handle voice calls using WebRTC technology. Users can call your WhatsApp Business number and have natural conversations with an AI-powered bot.

## Prerequisites

### WhatsApp Business API Setup

1. **Facebook Account**: Create an account at [facebook.com](https://facebook.com)
2. **Facebook Developer Account**: Create an account at [developers.facebook.com](https://developers.facebook.com)
3. **WhatsApp Business App**: Create a new [WhatsApp Business API application](https://developers.facebook.com/apps)
4. **Phone Number**: Add and verify a WhatsApp Business phone number
5. **Business Verification**: Complete business verification process (required for production only)
6. **Webhook Configuration**: Set up webhook endpoint for your application

> **Important Note**: For production, make sure your WhatsApp Business account has access to this feature.

> Find more details here:
> - [Getting Started Guide](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started/)
> - [Voice Calling Documentation](https://developers.facebook.com/docs/whatsapp/cloud-api/calling/)
> - [Webhooks Setup](https://developers.facebook.com/docs/whatsapp/webhooks/)

### WhatsApp Business API Configuration

#### Enable Voice Calls
Your WhatsApp Business phone number must be configured to accept voice calls[[2]](https://developers.facebook.com/docs/whatsapp/cloud-api/calling/):

> For development, you'll be provided with a free test phone number valid for 90 days.

1. Go to your WhatsApp Business API dashboard in Meta Developer Console
2. Navigate to **Configuration** → **Phone Numbers** → **Manage phone numbers**
3. Select your phone number
4. In the **Calls** tab, enable "Allow voice calls" capability
5. Save the configuration

#### Configure Webhook
Set up your webhook endpoint in the Meta Developer Console[[3]](https://developers.facebook.com/docs/whatsapp/webhooks/):

1. Go to **WhatsApp** → **Configuration** → **Webhooks**
2. Set callback URL: `https://your-domain.com/`
3. Set verify token: `your_webhook_verification_token`
   - This token should match your `WHATSAPP_WEBHOOK_VERIFICATION_TOKEN` environment variable
4. Click "Verify and save"
5. In the webhook fields below, select: `calls` (required for voice call events)

#### Configure Access Token
1. Go to **WhatsApp** → **API Setup**
2. Click "Generate access token"
   - Use this token for your `WHATSAPP_TOKEN` environment variable
3. Note your Phone Number ID - you'll need this for `PHONE_NUMBER_ID` configuration

## 🚀 Quick Start

### Environment Setup

1. **Install dependencies**:
   ```bash
   uv sync
   ```

2. **Configure environment variables**:
   ```bash
   cp env.example .env
   ```
   Edit `.env` file and add your API keys and configuration values.

### Run the Server

```bash
python server.py
```

> The server will start and listen for incoming WhatsApp webhook events.

### Connect Using WhatsApp

1. Find your WhatsApp test number in the Meta Developer Console
2. Call the number from your WhatsApp app
3. The bot should answer and engage in conversation

## Documentation References
- [WhatsApp Cloud API Getting Started](https://developers.facebook.com/docs/whatsapp/cloud-api/get-started/)
- [Voice Calling API Documentation](https://developers.facebook.com/docs/whatsapp/cloud-api/calling/)
- [Webhook Configuration Guide](https://developers.facebook.com/docs/whatsapp/webhooks/)
- [SDP Overview and Samples](https://developers.facebook.com/docs/whatsapp/cloud-api/calling/reference#sdp-overview-and-sample-sdp-structures)

## 💡 Troubleshooting
- Ensure all dependencies are installed before running the server
- Verify your `.env` file contains all required configuration values
- Make sure voice calling is enabled for your WhatsApp Business number
- Check that your webhook URL is publicly accessible and properly configured
- Ensure your business account is verified for production use

## Notes
- Voice calling feature requires WhatsApp Business API access
- Test numbers are valid for 90 days in development mode
- Production deployment requires business verification




================================================
FILE: whatsapp/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#
import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.base_transport import TransportParams
from pipecat.transports.smallwebrtc.transport import SmallWebRTCTransport

load_dotenv(override=True)

SYSTEM_INSTRUCTION = f"""
"You are Gemini Chatbot, a friendly, helpful robot.

Your goal is to demonstrate your capabilities in a succinct way.

Your output will be converted to audio so don't include special characters in your answers.

Respond to what the user said in a creative and helpful way. Keep your responses brief. One or two sentences at most.
"""


async def run_bot(webrtc_connection):
    pipecat_transport = SmallWebRTCTransport(
        webrtc_connection=webrtc_connection,
        params=TransportParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(),
            audio_out_10ms_chunks=2,
        ),
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        voice_id="Puck",  # Aoede, Charon, Fenrir, Kore, Puck
        system_instruction=SYSTEM_INSTRUCTION,
    )

    context = LLMContext(
        [
            {
                "role": "user",
                "content": "Start by greeting the user warmly and introducing yourself.",
            }
        ],
    )
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            pipecat_transport.input(),
            context_aggregator.user(),
            llm,  # LLM
            pipecat_transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    @pipecat_transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info("Pipecat Client connected")
        # Kick off the conversation.
        await task.queue_frames([LLMRunFrame()])

    @pipecat_transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info("Pipecat Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=False)

    await runner.run(task)



================================================
FILE: whatsapp/env.example
================================================
GOOGLE_API_KEY=
WHATSAPP_TOKEN=
WHATSAPP_WEBHOOK_VERIFICATION_TOKEN=
WHATSAPP_PHONE_NUMBER_ID=


================================================
FILE: whatsapp/pyproject.toml
================================================
[project]
name = "whatsapp"
version = "0.1.0"
description = "A real-time voice bot that integrates with WhatsApp Business API"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[google,silero, webrtc]>=0.0.83",
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
    "uvicorn>=0.32.0,<1.0.0",
    "python-dotenv>=1.0.1,<2.0.0",
    "fastapi>=0.115.6,<0.117.0"
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: whatsapp/server.py
================================================
#
# Copyright (c) 2024–2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""WhatsApp WebRTC Bot Server

A FastAPI server that handles WhatsApp webhook events and manages WebRTC connections
for real-time communication with WhatsApp users. The server integrates with WhatsApp's
Business API to receive incoming calls and messages, then establishes WebRTC connections
to enable audio/video communication through a bot.

Key features:
- WhatsApp webhook verification and message handling
- WebRTC connection management with ICE server support
- Graceful shutdown handling with signal management
- Background task processing for bot instances
- Connection cleanup and resource management

Environment Variables Required:
- WHATSAPP_TOKEN: WhatsApp Business API access token
- WHATSAPP_WEBHOOK_VERIFICATION_TOKEN: Token for webhook verification
- WHATSAPP_PHONE_NUMBER_ID: WhatsApp Business phone number ID

Usage:
    python server.py --host 0.0.0.0 --port 8080 --verbose
"""

import argparse
import asyncio
import signal
import sys
from contextlib import asynccontextmanager
from typing import Optional

import aiohttp
import uvicorn
from dotenv import load_dotenv
from fastapi import BackgroundTasks, FastAPI, HTTPException, Request
from loguru import logger
from pipecat.transports.smallwebrtc.connection import SmallWebRTCConnection
from pipecat.transports.whatsapp.api import WhatsAppWebhookRequest
from pipecat.transports.whatsapp.client import WhatsAppClient

from bot import run_bot

# Load environment variables first
load_dotenv(override=True)
import os

# Global configuration - loaded from environment variables
WHATSAPP_TOKEN = os.getenv("WHATSAPP_TOKEN")
WHATSAPP_WEBHOOK_VERIFICATION_TOKEN = os.getenv("WHATSAPP_WEBHOOK_VERIFICATION_TOKEN")
WHATSAPP_PHONE_NUMBER_ID = os.getenv("WHATSAPP_PHONE_NUMBER_ID")

# Validate required environment variables
if not all([WHATSAPP_TOKEN, WHATSAPP_WEBHOOK_VERIFICATION_TOKEN, WHATSAPP_PHONE_NUMBER_ID]):
    missing_vars = [
        var
        for var, val in [
            ("WHATSAPP_TOKEN", WHATSAPP_TOKEN),
            ("WHATSAPP_WEBHOOK_VERIFICATION_TOKEN", WHATSAPP_WEBHOOK_VERIFICATION_TOKEN),
            ("WHATSAPP_PHONE_NUMBER_ID", WHATSAPP_PHONE_NUMBER_ID),
        ]
        if not val
    ]
    raise ValueError(f"Missing required environment variables: {', '.join(missing_vars)}")

# Global state
whatsapp_client: Optional[WhatsAppClient] = None
shutdown_event = asyncio.Event()


def signal_handler() -> None:
    """Handle shutdown signals (SIGINT, SIGTERM) gracefully.

    Sets the shutdown event to initiate graceful server shutdown.
    This allows the server to complete ongoing requests and cleanup resources.
    """
    logger.info("Received shutdown signal, initiating graceful shutdown...")
    shutdown_event.set()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan and resources.

    Sets up the WhatsApp client with an HTTP session on startup
    and ensures proper cleanup on shutdown.

    Args:
        app: The FastAPI application instance

    Yields:
        None: Control back to the application during runtime
    """
    global whatsapp_client

    # Initialize WhatsApp client with persistent HTTP session
    async with aiohttp.ClientSession() as session:
        whatsapp_client = WhatsAppClient(
            whatsapp_token=WHATSAPP_TOKEN, phone_number_id=WHATSAPP_PHONE_NUMBER_ID, session=session
        )
        logger.info("WhatsApp client initialized successfully")

        try:
            yield  # Run the application
        finally:
            # Cleanup all active calls on shutdown
            logger.info("Cleaning up WhatsApp client resources...")
            if whatsapp_client:
                await whatsapp_client.terminate_all_calls()
            logger.info("Cleanup completed")


# Initialize FastAPI app with lifespan management
app = FastAPI(
    title="WhatsApp WebRTC Bot Server",
    description="Handles WhatsApp webhooks and manages WebRTC connections for bot communication",
    version="1.0.0",
    lifespan=lifespan,
)


@app.get(
    "/",
    summary="Verify WhatsApp webhook",
    description="Handles WhatsApp webhook verification requests from Meta",
)
async def verify_webhook(request: Request):
    """Verify WhatsApp webhook endpoint.

    This endpoint is called by Meta's WhatsApp Business API to verify
    the webhook URL during setup. It validates the verification token
    and returns the challenge parameter if successful.

    Args:
        request: FastAPI request object containing query parameters

    Returns:
        dict: Verification response or challenge string

    Raises:
        HTTPException: 403 if verification token is invalid
    """
    params = dict(request.query_params)
    logger.debug(f"Webhook verification request received with params: {list(params.keys())}")

    try:
        result = await whatsapp_client.handle_verify_webhook_request(
            params=params, expected_verification_token=WHATSAPP_WEBHOOK_VERIFICATION_TOKEN
        )
        logger.info("Webhook verification successful")
        return result
    except ValueError as e:
        logger.warning(f"Webhook verification failed: {e}")
        raise HTTPException(status_code=403, detail="Verification failed")


@app.post(
    "/",
    summary="Handle WhatsApp webhook events",
    description="Processes incoming WhatsApp messages and call events",
)
async def whatsapp_webhook(body: WhatsAppWebhookRequest, background_tasks: BackgroundTasks):
    """Handle incoming WhatsApp webhook events.

    Processes WhatsApp Business API webhook requests including:
    - Incoming messages
    - Call requests and status updates
    - User interactions

    For call events, establishes WebRTC connections and spawns bot instances
    in the background to handle real-time communication.

    Args:
        body: Parsed WhatsApp webhook request body
        background_tasks: FastAPI background tasks manager

    Returns:
        dict: Success response with processing status

    Raises:
        HTTPException:
            400 for invalid request format or object type
            500 for internal processing errors
    """
    # Validate webhook object type
    if body.object != "whatsapp_business_account":
        logger.warning(f"Invalid webhook object type: {body.object}")
        raise HTTPException(status_code=400, detail="Invalid object type")

    logger.info(f"Processing WhatsApp webhook: {body.dict()}")

    async def connection_callback(connection: SmallWebRTCConnection):
        """Handle new WebRTC connections from WhatsApp calls.

        Called when a WebRTC connection is established for a WhatsApp call.
        Spawns a bot instance to handle the conversation.

        Args:
            connection: The established WebRTC connection
        """
        try:
            logger.info(f"Starting bot for WebRTC connection: {connection.pc_id}")
            background_tasks.add_task(run_bot, connection)
            logger.debug(f"Bot task queued successfully for connection: {connection.pc_id}")
        except Exception as e:
            logger.error(f"Failed to start bot for connection {connection.pc_id}: {e}")
            # Attempt to cleanup the connection on error
            try:
                await connection.disconnect()
                logger.debug(f"Connection {connection.pc_id} disconnected after error")
            except Exception as disconnect_error:
                logger.error(f"Failed to disconnect connection after error: {disconnect_error}")

    try:
        # Process the webhook request
        result = await whatsapp_client.handle_webhook_request(body, connection_callback)
        logger.debug(f"Webhook processed successfully: {result}")
        return {"status": "success", "message": "Webhook processed successfully"}

    except ValueError as ve:
        logger.warning(f"Invalid webhook request format: {ve}")
        raise HTTPException(status_code=400, detail=f"Invalid request: {str(ve)}")
    except Exception as e:
        logger.error(f"Internal error processing webhook: {e}")
        raise HTTPException(status_code=500, detail="Internal server error processing webhook")


async def run_server_with_signal_handling(host: str, port: int) -> None:
    """Run the FastAPI server with proper signal handling.

    Sets up signal handlers for graceful shutdown and manages the server lifecycle.
    Handles SIGINT (Ctrl+C) and SIGTERM signals to ensure proper cleanup.

    Args:
        host: The host address to bind the server to
        port: The port number to listen on
    """
    # Set up signal handlers for graceful shutdown
    loop = asyncio.get_running_loop()
    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, signal_handler)

    # Configure and create the server
    config = uvicorn.Config(
        app,
        host=host,
        port=port,
        log_config=None,
    )
    server = uvicorn.Server(config)

    # Start server in background task
    server_task = asyncio.create_task(server.serve())
    logger.info(f"WhatsApp WebRTC server started on {host}:{port}")
    logger.info("Press Ctrl+C to stop the server")

    # Wait for shutdown signal
    await shutdown_event.wait()

    # Initiate graceful shutdown
    logger.info("Shutting down server.")

    # Cleanup WhatsApp client resources
    if whatsapp_client:
        await whatsapp_client.terminate_all_calls()

    # Stop the server
    server.should_exit = True
    await server_task
    logger.info("Server shutdown completed")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="WhatsApp WebRTC Bot Server - Handles WhatsApp webhooks and WebRTC connections"
    )
    parser.add_argument(
        "--host", default="localhost", help="Host for HTTP server (default: localhost)"
    )
    parser.add_argument(
        "--port", type=int, default=7860, help="Port for HTTP server (default: 7860)"
    )
    parser.add_argument("--verbose", "-v", action="count")
    args = parser.parse_args()

    logger.remove(0)
    if args.verbose:
        logger.add(sys.stderr, level="TRACE")
    else:
        logger.add(sys.stderr, level="DEBUG")

    # Validate configuration
    logger.info("Starting WhatsApp WebRTC Bot Server...")
    logger.debug(f"Configuration: host={args.host}, port={args.port}, verbose={args.verbose}")

    # Run the server
    try:
        asyncio.run(run_server_with_signal_handling(args.host, args.port))
    except KeyboardInterrupt:
        logger.info("Server interrupted by user")
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        sys.exit(1)



================================================
FILE: whatsapp/.python-version
================================================
3.12



================================================
FILE: word-wrangler-gemini-live/README.md
================================================
# Word Wrangler

Word Wrangler is a voice-based word guessing game powered by [Pipecat](https://github.com/pipecat-ai/pipecat) and the [Gemini Live API](https://ai.google.dev/gemini-api/docs/live). The game is available in two versions: a web-based experience and a phone-based experience. Test your description skills in this AI-powered twist on classic word games!

## Game Modes

### Web-Based Game

In this version, you provide the words, and an AI player attempts to guess them based on your descriptions.

**Try it now:** https://word-wrangler.vercel.app

<img src="images/word-wrangler-web-screenshot.png" width="800">

### Phone-Based Game

In this three-way conversation, an AI host provides words, you describe them without saying the actual word, and an AI player tries to guess. The host tracks your score and manages game flow.

**Try it now:** Call +1-929-**LLM-GAME** (+1-929-556-4263)

## Game Rules

### Web-based Game

1. The web app provides words for you to describe
2. You describe the word WITHOUT saying any part of it
3. The AI player tries to guess based on your description
4. The app will automatically check the guesses and keep score
5. Click "Skip" to advance to the next word
6. You have 60 seconds to score as many points as possible

### Phone Game

1. The AI host provides a word for you to describe
2. You describe the word WITHOUT saying any part of it
3. The AI player tries to guess based on your description
4. Score points for each correct guess
5. Use commands like "skip" to get a new word or "repeat" to hear the current word again
6. You have 120 seconds to score as many points as possible

## Architecture

### Web Game Architecture

The web game uses a simple linear flow:

1. **Transport Input** - Receives audio from the web browser via a Daily WebRTC transport.
2. **RTVIProcessor** - RTVI is a standard for client/server communication in a voice AI context. This processor collects server-side information and makes it available to the client. Additionally, the client can send events to the server, which are handled through this processor.
3. **STTMuteFilter** - Filters out speech during specific conditions. In this game, the user's initial speech is "muted", ensuring that the bot can deliver the entire initial message without being interrupted.
4. **User Context Aggregator** - Aggregates user messages as part of the conversation context.
5. **LLM** - The LLM powers the AI player's interactions.
6. **Transport Output** - Sends audio back to the browser using the Daily WebRTC transport.
7. **Assistant Context Aggregator** - Aggregates assistant messages as part of the conversation context.

### Phone Game Architecture

The phone game implements a three-way conversation using Pipecat's parallel pipeline architecture. This design addresses the fundamental challenge of LLMs - they're built for turn-based interactions, while this game requires real-time, multi-participant conversation management.

<img src="images/word-wrangler-twilio-architecture.png" height="900">

#### Conversation Participants

**Audio Flow Requirements:**

- **User:** Must hear both the Host and Player outputs; must be heard by both Host and Player
- **Host:** Must hear the User and Player inputs; its output must be heard by User but NOT by Player
- **Player:** Must hear only the User inputs; its output must be heard by both User and Host

#### Technical Implementation

The parallel pipeline pattern allows us to create two isolated processing branches, with controlled audio flow between them:

1. **Transport Input** - Receives audio from the phone call (Twilio)
2. **Audio Branch Separation:**
   - **Left Branch (Host Pipeline):** `ConsumerProcessor → Host LLM → Game State Tracker → TTS → Bot Stop Detector`
   - **Right Branch (Player Pipeline):** `StartFrame Gate → Player LLM → ProducerProcessor`

**Host LLM Configuration:**

The Host uses Gemini Live API, configured with specific response patterns to handle different input types:

```
- Correct guess: "Correct! That's [N] points. Your next word is [new word]"
- Incorrect guess: "NO" (filtered out by TTS filter)
- User descriptions: "IGNORE" (filtered out by TTS filter)
- Skip requests: "The new word is [new word]"
- Repeat requests: "Your word is [current word]"
```

**Audio Flow Management:**

By default, all input audio flows to both branches, so both LLMs hear the user. To implement the complex routing:

1. **Producer/Consumer Pattern:** Captures the Player's output audio and feeds it to the Host

   - `ProducerProcessor` filters TTSAudioRawFrames from the Player
   - Transforms them from 24kHz to 16kHz (required by Gemini Live)
   - Passes them to the `ConsumerProcessor` at the top of the Host branch

2. **Text Filtering:** The `HostResponseTextFilter` intercepts the "NO" and "IGNORE" responses

   - Prevents TTS vocalization of these responses
   - Ensures that only meaningful Host responses are spoken

3. **Host-Player Synchronization:**

   - `BotStoppedSpeakingNotifier` detects when the Host finishes speaking
   - `GameStateTracker` parses the streamed text to detect new words and track score
   - `NewWordNotifier` triggers the `ResettablePlayerLLM` to disconnect and reconnect when a new word is presented
   - This reset ensures the Player has no context of previous words or guesses

4. **StartFrameGate:** The gate holds the Player's StartFrame until the Host has completed its introduction
   - Ensures the Player doesn't start interacting until the game has been properly set up

All processed audio is collected at the end of the Parallel Pipeline and sent via the transport output back to Twilio.

#### Game State Management

The implementation tracks:

- Current words being guessed
- Running score (points for correct guesses)
- Game duration with automatic timeout

This architecture enables complex interaction patterns that would be difficult to achieve with traditional turn-based conversation models, allowing each AI participant to function effectively in their specific game role.

## Run Locally

### Web Game

#### Run the Server

1. Switch to the server directory:

   ```bash
   cd web-game/server
   ```

2. Set up your virtual environment and install dependencies:

   ```bash
   uv sync
   ```

3. Create an .env file and add your API keys:

   ```bash
   cp env.example .env
   ```

4. Add environment variables for:

   ```
   DAILY_API_KEY=
   DAILY_SAMPLE_ROOM_URL=
   GOOGLE_API_KEY=
   ```

5. Run the server:

   ```bash
   uv run bot -t daily
   ```

#### Run the Client

1. In a new terminal window, navigate to client:

   ```bash
   cd web-game/client
   ```

2. Install dependencies:

   ```bash
   npm install
   ```

3. Create an .env.local file:

   ```bash
   cp env.example .env.local
   ```

4. In .env.local:

   - `NEXT_PUBLIC_API_BASE_URL=http://localhost:7860` is used for local development. For deployments, either remove this env var or replace with `/api`.
   - `AGENT_NAME` should be set to the name of your deployed Pipecat agent (e.g., "word-wrangler").
   - `PIPECAT_CLOUD_API_KEY` is used only for deployments to Pipecat Cloud.

5. Run the app:

   ```bash
   npm run dev
   ```

6. Open http://localhost:3000 in your browser

### Phone Game

#### Running Locally

1. Navigate to phone-game directory:

   ```bash
   cd phone-game
   ```

2. Set up your virtual environment and install dependencies:

   ```bash
   uv sync
   ```

3. Create an .env file and add your API keys:

   ```bash
   cp env.example .env
   ```

4. Add environment variables for:

   ```
   GOOGLE_API_KEY=
   GOOGLE_TEST_CREDENTIALS_FILE=
   TWILIO_ACCOUNT_SID=
   TWILIO_AUTH_TOKEN=
   ```

5. Run the local bot:

   ```bash
   uv run bot.py
   ```

## Deployment

### Web Game

#### Deploy your Server

You can deploy your server code using Pipecat Cloud. For a full walkthrough, start with the [Pipecat Cloud Quickstart](https://docs.pipecat.daily.co/quickstart).

Here are the steps you'll need to complete:

- Build, tag, and push your Docker image to a registry (e.g. `uv run pcc docker build-push`)
- Create Pipecat Cloud secrets using the CLI or dashboard. For this agent, you only need a `GOOGLE_API_KEY`. Your `DAILY_API_KEY` is automatically applied.
- Deploy your agent image. You can use a pcc-deploy.toml file to make deploying easier. For example:

```toml
agent_name = "word-wrangler"
image = "your-dockerhub-name/word-wrangler:0.1"
secret_set = "word-wrangler-secrets"
enable_krisp = true

[scaling]
  min_agents = 1
  max_agents = 5
```

Then, you can deploy with the CLI using `uv run pcc deploy`.

- Finally, confirm that your agent is deployed. You'll get feedback in the terminal.

#### Deploy your Client

This project uses TypeScript, React, and Next.js, making it a perfect fit for [Vercel](https://vercel.com/).

- In your client directory, install Vercel's CLI tool: `npm install -g vercel`
- Verify it's installed using `vercel --version`
- Log in your Vercel account using `vercel login`
- Deploy your client to Vercel using `vercel`

### Phone Game

#### Deploy your Server

Again, we'll use Pipecat Cloud. Follow the steps from above. The only difference will be the secrets required; in addition to a GOOGLE_API_KEY, you'll need `GOOGLE_APPLICATION_CREDENTIALS` in the format of a .json file with your [Google Cloud service account](https://console.cloud.google.com/iam-admin/serviceaccounts) information.

You'll need to modify the Dockerfile so that the credentials.json and word_list.py are accessible. This Dockerfile will work:

```Dockerfile
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

COPY ./word_list.py word_list.py
COPY ./credentials.json credentials.json
COPY ./bot.py bot.py
```

Note: Your `credentials.json` file should have your Google service account credentials.

#### Buy and Configure a Twilio Number

Check out the [Twilio Websocket Telephony guide](https://docs.pipecat.ai/deployment/pipecat-cloud/guides/telephony/twilio-websocket) for a step-by-step walkthrough on how to purchase a phone number, configure your TwiML, and make or receive calls.

## Tech stack

Both games are built using:

- [Pipecat](https://www.pipecat.ai/) framework for real-time voice conversation
- Google's Gemini Live API
- Real-time communication (Web via Daily, Phone via Twilio)

The phone game features:

- Parallel processing of host and player interactions
- State tracking for game progress and scoring
- Dynamic word selection from multiple categories
- Automated game timing and scoring



================================================
FILE: word-wrangler-gemini-live/phone-game/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

"""Word Wrangler: A voice-based word guessing game.

This demo version is intended to be deployed to
Pipecat Cloud. For more information, visit:
- Deployment Quickstart: https://docs.pipecat.daily.co/quickstart
- Build for Twilio: https://docs.pipecat.daily.co/pipecat-in-production/telephony/twilio-mediastreams
"""

import asyncio
import os
import re
from typing import Any, Mapping, Optional

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.resamplers.soxr_resampler import SOXRAudioResampler
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import (
    BotStoppedSpeakingFrame,
    CancelFrame,
    EndFrame,
    Frame,
    InputAudioRawFrame,
    LLMContextFrame,
    LLMFullResponseEndFrame,
    LLMRunFrame,
    LLMTextFrame,
    StartFrame,
    TTSAudioRawFrame,
    TTSSpeakFrame,
)
from pipecat.pipeline.parallel_pipeline import ParallelPipeline
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.consumer_processor import ConsumerProcessor
from pipecat.processors.filters.stt_mute_filter import STTMuteConfig, STTMuteFilter, STTMuteStrategy
from pipecat.processors.frame_processor import FrameDirection, FrameProcessor, FrameProcessorSetup
from pipecat.processors.producer_processor import ProducerProcessor
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.google.gemini_live.llm import (
    GeminiLiveLLMService,
    GeminiModalities,
    InputParams,
)
from pipecat.services.google.tts import GoogleTTSService
from pipecat.sync.base_notifier import BaseNotifier
from pipecat.sync.event_notifier import EventNotifier
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.websocket.fastapi import (
    FastAPIWebsocketParams,
)
from pipecat.utils.text.base_text_filter import BaseTextFilter

from word_list import generate_game_words

load_dotenv(override=True)


GAME_DURATION_SECONDS = 120
NUM_WORDS_PER_GAME = 20
HOST_VOICE_ID = "en-US-Chirp3-HD-Charon"
PLAYER_VOICE_ID = "Kore"

# Define conversation modes with their respective prompt templates
game_player_prompt = """You are a player for a game of Word Wrangler.

GAME RULES:
1. The user will be given a word or phrase that they must describe to you
2. The user CANNOT say any part of the word/phrase directly
3. You must try to guess the word/phrase based on the user's description
4. Once you guess correctly, the user will move on to their next word
5. The user is trying to get through as many words as possible in 60 seconds
6. The external application will handle timing and keeping score

YOUR ROLE:
1. Listen carefully to the user's descriptions
2. Make intelligent guesses based on what they say
3. When you think you know the answer, state it clearly: "Is it [your guess]?"
4. If you're struggling, ask for more specific clues
5. Keep the game moving quickly - make guesses promptly
6. Be enthusiastic and encouraging

IMPORTANT:
- Keep all responses brief - the game is timed!
- Make multiple guesses if needed
- Use your common knowledge to make educated guesses
- If the user indicates you got it right, just say "Got it!" and prepare for the next word
- If you've made several wrong guesses, simply ask for "Another clue please?"

Start by guessing once you hear the user describe the word or phrase."""

game_host_prompt = """You are the AI host for a game of Word Wrangler. There are two players in the game: the human describer and the AI guesser.

GAME RULES:
1. You, the host, will give the human describer a word or phrase that they must describe
2. The describer CANNOT say any part of the word/phrase directly
3. The AI guesser will try to guess the word/phrase based on the describer's description
4. Once the guesser guesses correctly, move on to the next word
5. The describer is trying to get through as many words as possible in 60 seconds
6. The describer can say "skip" or "pass" to get a new word if they find a word too difficult
7. The describer can ask you to repeat the current word if they didn't hear it clearly
8. You'll keep track of the score (1 point for each correct guess)
9. The external application will handle timing

YOUR ROLE:
1. Start with this exact brief introduction: "Welcome to Word Wrangler! I'll give you words to describe, and the A.I. player will try to guess them. Remember, don't say any part of the word itself. Here's your first word: [word]."
2. Provide words to the describer. Choose 1 or 2 word phrases that cover a variety of topics, including animals, objects, places, and actions.
3. IMPORTANT: You will hear DIFFERENT types of input:
   a. DESCRIPTIONS from the human (which you should IGNORE)
   b. AFFIRMATIONS from the human (like "correct", "that's right", "you got it") which you should IGNORE
   c. GUESSES from the AI player (which will be in the form of "Is it [word]?" or similar question format)
   d. SKIP REQUESTS from the human (if they say "skip", "pass", or "next word please")
   e. REPEAT REQUESTS from the human (if they say "repeat", "what was that?", "say again", etc.)

4. HOW TO RESPOND:
   - If you hear a DESCRIPTION or AFFIRMATION from the human, respond with exactly "IGNORE" (no other text)
   - If you hear a GUESS (in question form) and it's INCORRECT, respond with exactly "NO" (no other text)
   - If you hear a GUESS (in question form) and it's CORRECT, respond with "Correct! That's [N] points. Your next word is [new word]" where N is the current score
   - If you hear a SKIP REQUEST, respond with "The new word is [new word]" (don't change the score)
   - If you hear a REPEAT REQUEST, respond with "Your word is [current word]" (don't change the score)

5. SCORING:
   - Start with a score of 0
   - Add 1 point for each correct guess by the AI player
   - Do NOT add points for skipped words
   - Announce the current score after every correct guess

RESPONSE EXAMPLES:
- Human says: "This is something you use to write" → You respond: "IGNORE"
- Human says: "That's right!" or "You got it!" → You respond: "IGNORE"
- Human says: "Wait, what was my word again?" → You respond: "Your word is [current word]"
- Human says: "Can you repeat that?" → You respond: "Your word is [current word]"
- AI says: "Is it a pen?" → If correct and it's the first point, you respond: "Correct! That's 1 point. Your next word is [new word]"
- AI says: "Is it a pencil?" → If correct and it's the third point, you respond: "Correct! That's 3 points. Your next word is [new word]"
- AI says: "Is it a marker?" → If incorrect, you respond: "NO"
- Human says: "Skip this one" or "Pass" → You respond: "The new word is [new word]"

IMPORTANT GUIDELINES:
- Choose words that range from easy to moderately difficult
- Keep all responses brief - the game is timed!
- Your "NO" and "IGNORE" responses won't be verbalized, but will be visible in the chat
- Always keep track of the CURRENT word so you can repeat it when asked
- Always keep track of the CURRENT SCORE and announce it after every correct guess
- Make sure your word choices are appropriate for all audiences
- If the human asks to skip, always provide a new word immediately without changing the score
- If the human asks you to repeat the word, say ONLY "Your word is [current word]" - don't add additional text
- CRUCIAL: Never interpret the human saying "correct", "that's right", "good job", or similar affirmations as a correct guess. These are just the human giving feedback to the AI player.

Start with the exact introduction specified above and give the first word."""


class HostResponseTextFilter(BaseTextFilter):
    """Custom text filter for Word Wrangler game.

    This filter removes "NO" and "IGNORE" responses from the host so they don't get verbalized,
    allowing for silent incorrect guess handling and ignoring descriptions.
    """

    def __init__(self):
        self._interrupted = False

    def update_settings(self, settings: Mapping[str, Any]):
        # No settings to update for this filter
        pass

    async def filter(self, text: str) -> str:
        # Remove case and whitespace for comparison
        clean_text = text.strip().upper()

        # If the text is exactly "NO" or "IGNORE", return empty string
        if clean_text == "NO" or clean_text == "IGNORE":
            return ""

        return text

    async def handle_interruption(self):
        self._interrupted = True

    async def reset_interruption(self):
        self._interrupted = False


class BotStoppedSpeakingNotifier(FrameProcessor):
    """A processor that notifies whenever a BotStoppedSpeakingFrame is detected."""

    def __init__(self, notifier: BaseNotifier):
        super().__init__()
        self._notifier = notifier

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        # Check if this is a BotStoppedSpeakingFrame
        if isinstance(frame, BotStoppedSpeakingFrame):
            logger.debug(f"{self}: Host bot stopped speaking, notifying listeners")
            await self._notifier.notify()

        # Always push the frame through
        await self.push_frame(frame, direction)


class StartGate(FrameProcessor):
    """A gate that blocks the inital context message to prevent the player from responding first.

    Blocks LLMContextFrame until the gate opens. This frame is dropped
    (not stored) to ignore the initial message. Once opened, all frames pass through normally.
    Note that we don't need to block User input frames or speaking frames because the STTMuteFilter
    will handle that.
    """

    def __init__(self, notifier: BaseNotifier):
        super().__init__()
        self._notifier = notifier
        self._gate_opened = False
        self._gate_task: Optional[asyncio.Task] = None

    async def setup(self, setup: FrameProcessorSetup):
        """Set up the processor with required components.

        Args:
            setup: Configuration object containing setup parameters.
        """
        await super().setup(setup)
        self._gate_task = self.create_task(self._wait_for_notification())

    async def cleanup(self):
        """Clean up the processor resources."""
        await super().cleanup()
        if self._gate_task:
            await self.cancel_task(self._gate_task)
            self._gate_task = None

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        if self._gate_opened:
            # Once the gate is open, let everything through
            await self.push_frame(frame, direction)
        elif isinstance(frame, LLMContextFrame):
            # Drop these frames until the gate opens - we want to ignore this audio
            logger.trace(f"{self}: Dropping {type(frame).__name__} until host bot stops speaking")

            # Start the gate task if not already running

        else:
            # Let all other frames through
            await self.push_frame(frame, direction)

    async def _wait_for_notification(self):
        try:
            # Wait for the notifier
            await self._notifier.wait()

            # Gate is now open - only run this code once
            if not self._gate_opened:
                self._gate_opened = True
                logger.debug(f"{self}: Gate opened, all frames will now pass through")

        except asyncio.CancelledError:
            logger.debug(f"{self}: Gate task was cancelled")
            raise
        except Exception as e:
            logger.exception(f"{self}: Error in gate task: {e}")
            raise


class GameStateTracker(FrameProcessor):
    """Tracks game state including new words and score by monitoring host responses.

    This processor aggregates streamed text from the host LLM to detect:
    1. New word announcements (triggering player LLM resets)
    2. Score updates (to track the current score)
    """

    def __init__(self, new_word_notifier: BaseNotifier):
        super().__init__()
        self._new_word_notifier = new_word_notifier
        self._text_buffer = ""
        self._current_score = 0

        # Words/phrases that indicate a new word being provided
        self._key_phrases = ["your word is", "new word is", "next word is"]

        # Pattern to extract score from responses
        self._score_pattern = re.compile(r"that's (\d+) point", re.IGNORECASE)

    async def process_frame(self, frame: Frame, direction: FrameDirection):
        await super().process_frame(frame, direction)

        # Collect text from LLMTextFrames
        if isinstance(frame, LLMTextFrame):
            text = frame.text

            # Skip responses that are "NO" or "IGNORE"
            if text.strip() in ["NO", "IGNORE"]:
                logger.debug(f"Skipping NO/IGNORE response")
                await self.push_frame(frame, direction)
                return

            # Add the new text to our buffer
            self._text_buffer += text

        # Process complete responses when we get an end frame
        elif isinstance(frame, LLMFullResponseEndFrame):
            if self._text_buffer:
                buffer_lower = self._text_buffer.lower()

                # 1. Check for new word announcements
                new_word_detected = False
                for phrase in self._key_phrases:
                    if phrase in buffer_lower:
                        await self._new_word_notifier.notify()
                        new_word_detected = True
                        break

                if not new_word_detected:
                    logger.debug(f"No new word phrases detected")

                # 2. Check for score updates
                score_match = self._score_pattern.search(buffer_lower)
                if score_match:
                    try:
                        score = int(score_match.group(1))
                        # Only update if the new score is higher
                        if score > self._current_score:
                            logger.debug(f"Score updated from {self._current_score} to {score}")
                            self._current_score = score
                        else:
                            logger.debug(
                                f"Ignoring score {score} <= current score {self._current_score}"
                            )
                    except ValueError as e:
                        logger.warning(f"Error parsing score: {e}")
                else:
                    logger.debug(f"No score pattern match in: '{buffer_lower}'")

                # Reset the buffer after processing the complete response
                self._text_buffer = ""

        # Always push the frame through
        await self.push_frame(frame, direction)

    @property
    def current_score(self) -> int:
        """Get the current score."""
        return self._current_score


class GameTimer:
    """Manages the game timer and triggers end-game events."""

    def __init__(
        self,
        task: PipelineTask,
        game_state_tracker: GameStateTracker,
        game_duration_seconds: int = 120,
    ):
        self._task = task
        self._game_state_tracker = game_state_tracker
        self._game_duration = game_duration_seconds
        self._timer_task = None
        self._start_time = None

    def start(self):
        """Start the game timer."""
        if self._timer_task is None:
            self._start_time = asyncio.get_event_loop().time()
            self._timer_task = asyncio.create_task(self._run_timer())
            logger.info(f"Game timer started: {self._game_duration} seconds")

    def stop(self):
        """Stop the game timer."""
        if self._timer_task:
            self._timer_task.cancel()
            self._timer_task = None
            logger.info("Game timer stopped")

    def get_remaining_time(self) -> int:
        """Get the remaining time in seconds."""
        if self._start_time is None:
            return self._game_duration

        elapsed = asyncio.get_event_loop().time() - self._start_time
        remaining = max(0, self._game_duration - int(elapsed))
        return remaining

    async def _run_timer(self):
        """Run the timer and end the game when time is up."""
        try:
            # Wait for the game duration
            await asyncio.sleep(self._game_duration)

            # Game time is up, get the final score
            final_score = self._game_state_tracker.current_score

            # Create end game message
            end_message = f"Time's up! Thank you for playing Word Wrangler. Your final score is {final_score} point"
            if final_score != 1:
                end_message += "s"
            end_message += ". Great job!"

            # Send end game message as TTSSpeakFrame
            logger.info(f"Game over! Final score: {final_score}")
            await self._task.queue_frames([TTSSpeakFrame(text=end_message)])

            # End the game
            await self._task.queue_frames([EndFrame()])

        except asyncio.CancelledError:
            logger.debug("Game timer task cancelled")
        except Exception as e:
            logger.exception(f"Error in game timer: {e}")


class ResettablePlayerLLM(GeminiLiveLLMService):
    """A specialized LLM service that can reset its context when notified about a new word.

    This LLM intelligently waits for the host to finish speaking before reconnecting.
    """

    def __init__(
        self,
        api_key: str,
        system_instruction: str,
        new_word_notifier: BaseNotifier,
        host_stopped_speaking_notifier: BaseNotifier,
        voice_id: str = PLAYER_VOICE_ID,
        **kwargs,
    ):
        super().__init__(
            api_key=api_key, voice_id=voice_id, system_instruction=system_instruction, **kwargs
        )
        self._new_word_notifier = new_word_notifier
        self._host_stopped_speaking_notifier = host_stopped_speaking_notifier
        self._base_system_instruction = system_instruction
        self._reset_task: Optional[asyncio.Task] = None
        self._pending_reset: bool = False

    async def start(self, frame: StartFrame):
        await super().start(frame)

        # Start the notifier listener task
        if not self._reset_task or self._reset_task.done():
            self._reset_task = self.create_task(self._listen_for_notifications())

    async def stop(self, frame: EndFrame):
        # Cancel the reset task if it exists
        if self._reset_task and not self._reset_task.done():
            await self.cancel_task(self._reset_task)
            self._reset_task = None

        await super().stop(frame)

    async def cancel(self, frame: CancelFrame):
        # Cancel the reset task if it exists
        if self._reset_task and not self._reset_task.done():
            await self.cancel_task(self._reset_task)
            self._reset_task = None

        await super().cancel(frame)

    async def _listen_for_notifications(self):
        """Listen for new word and host stopped speaking notifications."""
        try:
            # Create tasks for both notifiers
            new_word_task = self.create_task(self._listen_for_new_word())
            host_stopped_task = self.create_task(self._listen_for_host_stopped())

            # Wait for both tasks to complete (which should never happen)
            await asyncio.gather(new_word_task, host_stopped_task)

        except asyncio.CancelledError:
            logger.debug(f"{self}: Notification listener tasks cancelled")
            raise
        except Exception as e:
            logger.exception(f"{self}: Error in notification listeners: {e}")
            raise

    async def _listen_for_new_word(self):
        """Listen for new word notifications and flag a reset is needed."""
        while True:
            # Wait for a new word notification
            await self._new_word_notifier.wait()
            logger.info(
                f"{self}: Received new word notification, disconnecting and waiting for host to finish"
            )

            # Disconnect immediately to stop processing
            await self._disconnect()

            # Reset the system instruction
            self._system_instruction = self._base_system_instruction

            # Flag that we need to reconnect when the host stops speaking
            self._pending_reset = True

    async def _listen_for_host_stopped(self):
        """Listen for host stopped speaking and reconnect if a reset is pending."""
        while True:
            # Wait for host stopped speaking notification
            await self._host_stopped_speaking_notifier.wait()

            # If we have a pending reset, reconnect now
            if self._pending_reset:
                logger.info(f"{self}: Host finished speaking, completing the LLM reset")

                # Reconnect
                await self._connect()

                # Reset the flag
                self._pending_reset = False

                logger.info(f"{self}: LLM reset complete")


async def tts_audio_raw_frame_filter(frame: Frame):
    """Filter to check if the frame is a TTSAudioRawFrame."""
    return isinstance(frame, TTSAudioRawFrame)


# Create a resampler instance once
resampler = SOXRAudioResampler()


async def tts_to_input_audio_transformer(frame: Frame):
    """Transform TTS audio frames to InputAudioRawFrame with resampling.

    Converts 24kHz TTS output to 16kHz input audio required by the player LLM.

    Args:
        frame (Frame): The frame to transform (expected to be TTSAudioRawFrame)

    Returns:
        InputAudioRawFrame: The transformed and resampled input audio frame
    """
    if isinstance(frame, TTSAudioRawFrame):
        # Resample the audio from 24kHz to 16kHz
        resampled_audio = await resampler.resample(
            frame.audio,
            frame.sample_rate,  # Source rate (24kHz)
            16000,  # Target rate (16kHz)
        )

        # Create a new InputAudioRawFrame with the resampled audio
        input_frame = InputAudioRawFrame(
            audio=resampled_audio,
            sample_rate=16000,  # New sample rate
            num_channels=frame.num_channels,
        )
        return input_frame


async def run_bot(transport: BaseTransport, runner_args: RunnerArguments):
    logger.debug("Starting WebSocket bot")

    game_words = generate_game_words(NUM_WORDS_PER_GAME)
    words_string = ", ".join(f'"{word}"' for word in game_words)
    logger.debug(f"Game words: {words_string}")

    player_instruction = f"""{game_player_prompt}

Important guidelines:
1. Your responses will be converted to speech, so keep them concise and conversational.
2. Don't use special characters or formatting that wouldn't be natural in speech.
3. Encourage the user to elaborate when appropriate."""

    host_instruction = f"""{game_host_prompt}

GAME WORDS:
Use ONLY these words for the game (in any order): {words_string}

Important guidelines:
1. Your responses will be converted to speech, so keep them concise and conversational.
2. Don't use special characters or formatting that wouldn't be natural in speech.
3. ONLY use words from the provided list above when giving words to the player."""

    intro_message = """Start with this exact brief introduction: "Welcome to Word Wrangler! I'll give you words to describe, and the A.I. player will try to guess them. Remember, don't say any part of the word itself. Here's your first word: [word]." """

    # Create the STT mute filter if we have strategies to apply
    stt_mute_filter = STTMuteFilter(
        config=STTMuteConfig(strategies={STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE})
    )

    host_llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        system_instruction=host_instruction,
        params=InputParams(modalities=GeminiModalities.TEXT),
    )

    host_tts = GoogleTTSService(
        voice_id=HOST_VOICE_ID,
        credentials_path=os.getenv("GOOGLE_TEST_CREDENTIALS_FILE"),
        text_filters=[HostResponseTextFilter()],
    )

    producer = ProducerProcessor(
        filter=tts_audio_raw_frame_filter,
        transformer=tts_to_input_audio_transformer,
        passthrough=True,
    )
    consumer = ConsumerProcessor(producer=producer)

    # Create the notifiers
    bot_speaking_notifier = EventNotifier()
    new_word_notifier = EventNotifier()

    # Create BotStoppedSpeakingNotifier to detect when host bot stops speaking
    bot_stopped_speaking_detector = BotStoppedSpeakingNotifier(bot_speaking_notifier)

    # Create StartGate to block Player LLM until host has stopped speaking
    start_gate = StartGate(bot_speaking_notifier)

    # Create GameStateTracker to handle new words and score tracking
    game_state_tracker = GameStateTracker(new_word_notifier)

    # Create a resettable player LLM that coordinates between notifiers
    player_llm = ResettablePlayerLLM(
        api_key=os.getenv("GOOGLE_API_KEY"),
        system_instruction=player_instruction,
        new_word_notifier=new_word_notifier,
        host_stopped_speaking_notifier=bot_speaking_notifier,
        voice_id=PLAYER_VOICE_ID,
    )

    # Set up the initial context for the conversation
    messages = [
        {
            "role": "user",
            "content": intro_message,
        },
    ]

    # While there are no context aggregators in the Pipeline, we need to create
    # one here to be able to push the context frame to the PipelineTask. This is
    # what initiates the conversation.
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Receive audio/video from Daily call
            stt_mute_filter,  # Filter out speech during the bot's initial turn
            context_aggregator.user(),
            ParallelPipeline(
                # Host branch: manages the game and provides words
                [
                    consumer,  # Receives audio from the player branch
                    host_llm,  # AI host that provides words and tracks score
                    game_state_tracker,  # Tracks words and score from host responses
                    host_tts,  # Converts host text to speech
                    bot_stopped_speaking_detector,  # Notifies when host stops speaking
                ],
                # Player branch: guesses words based on human descriptions
                [
                    start_gate,  # Gates the player until host finishes intro
                    player_llm,  # AI player that makes guesses
                    producer,  # Collects audio frames to be passed to the consumer
                ],
            ),
            transport.output(),  # Send audio/video back to Daily call
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            audio_out_sample_rate=8000,
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
    )

    # Create the game timer
    game_timer = GameTimer(task, game_state_tracker, game_duration_seconds=GAME_DURATION_SECONDS)

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected: {client}")
        # Kick off the conversation by getting the context frame and pushing it.
        # There is no aggegrator in the Pipeline, so we need to rely on the
        # PipelineTask to push the frame.
        await task.queue_frames([LLMRunFrame()])
        # Start the game timer
        game_timer.start()

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected: {client}")
        # Stop the timer
        game_timer.stop()
        # Cancel the pipeline task
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with Pipecat Cloud."""
    if os.environ.get("ENV") != "local":
        from pipecat.audio.filters.krisp_filter import KrispFilter

        krisp_filter = KrispFilter()
    else:
        krisp_filter = None

    # We store functions so objects (e.g. SileroVADAnalyzer) don't get
    # instantiated. The function will be called when the desired transport gets
    # selected.
    transport_params = {
        "webrtc": lambda: TransportParams(
            audio_in_enabled=True,
            audio_in_filter=krisp_filter,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
        ),
        "twilio": lambda: FastAPIWebsocketParams(
            audio_in_enabled=True,
            audio_in_filter=krisp_filter,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
        ),
    }

    transport = await create_transport(runner_args, transport_params)
    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: word-wrangler-gemini-live/phone-game/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

COPY ./word_list.py word_list.py
COPY ./credentials.json credentials.json
COPY ./bot.py bot.py



================================================
FILE: word-wrangler-gemini-live/phone-game/env.example
================================================
GOOGLE_API_KEY=
GOOGLE_TEST_CREDENTIALS_FILE=

TWILIO_ACCOUNT_SID=
TWILIO_AUTH_TOKEN=


================================================
FILE: word-wrangler-gemini-live/phone-game/pcc-deploy.toml
================================================
agent_name = "word-wrangler-twilio"
image = "your_username/word-wrangler-twilio:0.1"
image_credentials = "your_dockerhub_pull_secret"
secret_set = "word-wrangler-twilio-secrets"
enable_krisp = true
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: word-wrangler-gemini-live/phone-game/pyproject.toml
================================================
[project]
name = "word-wrangler-twilio"
version = "0.1.0"
description = "Word Wrangler guessing game powered by Gemini and Twilio"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[webrtc,websocket,silero,google,local-smart-turn-v3,runner]>=0.0.93",
    "pipecatcloud>=0.2.6"
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: word-wrangler-gemini-live/phone-game/word_list.py
================================================
import random

# Define categories and words for the Word Wrangler game
WORD_CATEGORIES = {
    "animals": [
        "elephant",
        "penguin",
        "giraffe",
        "dolphin",
        "kangaroo",
        "octopus",
        "panda",
        "tiger",
        "koala",
        "flamingo",
        "hedgehog",
        "turtle",
        "zebra",
        "eagle",
        "sloth",
        "raccoon",
        "chameleon",
        "squirrel",
        "hamster",
        "cheetah",
        "platypus",
        "jellyfish",
        "parrot",
        "wolf",
        "hippo",
        "porcupine",
        "ostrich",
        "peacock",
        "alligator",
        "gorilla",
        "armadillo",
        "chipmunk",
        "walrus",
        "weasel",
        "skunk",
        "llama",
        "badger",
        "mongoose",
        "lemur",
        "otter",
        "bison",
        "falcon",
        "meerkat",
        "pelican",
        "cobra",
        "salamander",
        "lobster",
        "seal",
        "narwhal",
        "iguana",
        "piranha",
        "toucan",
        "moose",
        "lynx",
        "stingray",
        "starfish",
        "beaver",
        "vulture",
        "antelope",
        "jaguar",
        "seahorse",
    ],
    "food": [
        "pizza",
        "sushi",
        "burrito",
        "pancake",
        "donut",
        "lasagna",
        "popcorn",
        "chocolate",
        "mango",
        "pretzel",
        "taco",
        "waffle",
        "cupcake",
        "avocado",
        "cookie",
        "croissant",
        "omelette",
        "cheesecake",
        "dumpling",
        "hummus",
        "gelato",
        "risotto",
        "ramen",
        "salsa",
        "kebab",
        "brownie",
        "guacamole",
        "bagel",
        "falafel",
        "biscuit",
        "churro",
        "meatball",
        "tiramisu",
        "enchilada",
        "couscous",
        "gumbo",
        "jambalaya",
        "baklava",
        "popsicle",
        "cannoli",
        "tofu",
        "macaron",
        "empanada",
        "pho",
        "casserole",
        "porridge",
        "granola",
        "fritter",
        "hazelnut",
        "kiwi",
        "pomegranate",
        "artichoke",
        "edamame",
        "zucchini",
        "cashew",
        "brisket",
        "custard",
        "nutmeg",
        "ginger",
    ],
    "household": [
        "chair",
        "pillow",
        "mirror",
        "blanket",
        "lamp",
        "curtain",
        "sofa",
        "refrigerator",
        "blender",
        "bookshelf",
        "dishwasher",
        "carpet",
        "microwave",
        "table",
        "clock",
        "vase",
        "ottoman",
        "candle",
        "drawer",
        "cabinet",
        "doorknob",
        "silverware",
        "bathtub",
        "plunger",
        "toaster",
        "kettle",
        "spatula",
        "doormat",
        "hanger",
        "blinds",
        "ladle",
        "platter",
        "coaster",
        "napkin",
        "sponge",
        "thermostat",
        "showerhead",
        "coatrack",
        "nightstand",
        "cushion",
        "windowsill",
        "bedsheet",
        "countertop",
        "dustpan",
        "footstool",
        "flowerpot",
        "trashcan",
        "colander",
        "detergent",
        "chandelier",
        "laundry",
        "vacuum",
        "teapot",
        "duster",
        "lightbulb",
        "corkscrew",
        "paperweight",
        "doorstop",
        "radiator",
    ],
    "activities": [
        "swimming",
        "painting",
        "dancing",
        "gardening",
        "skiing",
        "cooking",
        "hiking",
        "reading",
        "yoga",
        "fishing",
        "jogging",
        "biking",
        "baking",
        "singing",
        "camping",
        "knitting",
        "surfing",
        "photography",
        "bowling",
        "archery",
        "horseback",
        "meditation",
        "gymnastics",
        "volleyball",
        "tennis",
        "skating",
        "kayaking",
        "climbing",
        "juggling",
        "rowing",
        "snorkeling",
        "embroidery",
        "canoeing",
        "paddleboarding",
        "pottery",
        "birdwatching",
        "karaoke",
        "sailing",
        "pilates",
        "calligraphy",
        "skateboarding",
        "crossword",
        "origami",
        "beekeeping",
        "stargazing",
        "snowboarding",
        "woodworking",
        "fencing",
        "quilting",
        "foraging",
        "geocaching",
        "scrapbooking",
        "welding",
        "glassblowing",
        "whittling",
        "ziplining",
    ],
    "places": [
        "beach",
        "library",
        "mountain",
        "airport",
        "stadium",
        "museum",
        "hospital",
        "castle",
        "garden",
        "hotel",
        "island",
        "desert",
        "university",
        "restaurant",
        "forest",
        "aquarium",
        "theater",
        "canyon",
        "lighthouse",
        "waterfall",
        "vineyard",
        "cathedral",
        "rainforest",
        "farmhouse",
        "greenhouse",
        "observatory",
        "marketplace",
        "boardwalk",
        "temple",
        "courtyard",
        "plantation",
        "lagoon",
        "volcano",
        "meadow",
        "oasis",
        "grotto",
        "peninsula",
        "aviary",
        "chapel",
        "coliseum",
        "bazaar",
        "marina",
        "orchard",
        "brewery",
        "sanctuary",
        "fortress",
        "prairie",
        "reservation",
        "tavern",
        "monument",
        "manor",
        "pavilion",
        "boulevard",
        "campground",
    ],
    "objects": [
        "umbrella",
        "scissors",
        "camera",
        "wallet",
        "bicycle",
        "backpack",
        "telescope",
        "balloon",
        "compass",
        "notebook",
        "keyboard",
        "magnet",
        "headphones",
        "hammer",
        "envelope",
        "binoculars",
        "tambourine",
        "boomerang",
        "megaphone",
        "suitcase",
        "pinwheel",
        "kaleidoscope",
        "microscope",
        "hourglass",
        "harmonica",
        "trampoline",
        "bubblegum",
        "xylophone",
        "typewriter",
        "screwdriver",
        "whistle",
        "chessboard",
        "handcuffs",
        "stethoscope",
        "stopwatch",
        "parachute",
        "blowtorch",
        "calculator",
        "thermometer",
        "mousetrap",
        "crowbar",
        "paintbrush",
        "metronome",
        "surfboard",
        "flipchart",
        "dartboard",
        "wrench",
        "flippers",
        "thimble",
        "protractor",
        "snorkel",
        "doorbell",
        "flashlight",
        "pendulum",
        "abacus",
    ],
    "jobs": [
        "teacher",
        "doctor",
        "chef",
        "firefighter",
        "pilot",
        "astronaut",
        "carpenter",
        "musician",
        "detective",
        "scientist",
        "farmer",
        "architect",
        "journalist",
        "electrician",
        "dentist",
        "veterinarian",
        "librarian",
        "photographer",
        "mechanic",
        "attorney",
        "barista",
        "plumber",
        "bartender",
        "surgeon",
        "therapist",
        "animator",
        "programmer",
        "pharmacist",
        "translator",
        "accountant",
        "florist",
        "butcher",
        "lifeguard",
        "beekeeper",
        "locksmith",
        "choreographer",
        "mortician",
        "paramedic",
        "blacksmith",
        "surveyor",
        "botanist",
        "chiropractor",
        "undertaker",
        "acrobat",
        "welder",
        "hypnotist",
        "zoologist",
        "mime",
        "sommelier",
        "meteorologist",
        "stuntman",
        "diplomat",
        "entomologist",
        "puppeteer",
        "archivist",
        "cartographer",
        "paleontologist",
    ],
    "transportation": [
        "helicopter",
        "submarine",
        "scooter",
        "sailboat",
        "train",
        "motorcycle",
        "airplane",
        "canoe",
        "tractor",
        "limousine",
        "escalator",
        "skateboard",
        "ambulance",
        "ferry",
        "rocket",
        "hovercraft",
        "gondola",
        "segway",
        "zeppelin",
        "bulldozer",
        "speedboat",
        "unicycle",
        "monorail",
        "snowmobile",
        "paddleboat",
        "trolley",
        "rickshaw",
        "caboose",
        "glider",
        "bobsled",
        "jetpack",
        "forklift",
        "dirigible",
        "chariot",
        "sidecar",
        "tandem",
        "battleship",
        "catamaran",
        "toboggan",
        "dinghy",
        "hydrofoil",
        "sleigh",
        "hatchback",
        "kayak",
        "stagecoach",
        "tugboat",
        "airship",
        "skiff",
        "carriage",
        "rowboat",
        "chairlift",
        "steamroller",
    ],
    "clothing": [
        "sweater",
        "sandals",
        "tuxedo",
        "poncho",
        "sneakers",
        "bikini",
        "cardigan",
        "overalls",
        "kimono",
        "mittens",
        "suspenders",
        "kilt",
        "leggings",
        "apron",
        "bowtie",
        "earmuffs",
        "fedora",
        "wetsuit",
        "pajamas",
        "sombrero",
        "raincoat",
        "beret",
        "turtleneck",
        "parka",
        "tiara",
        "toga",
        "bandana",
        "corset",
        "sarong",
        "tunic",
        "visor",
        "ascot",
        "fez",
        "moccasins",
        "blazer",
        "chaps",
        "romper",
        "waders",
        "clogs",
        "garter",
        "camisole",
        "galoshes",
        "bolero",
        "spats",
        "pantyhose",
        "onesie",
        "stiletto",
        "vest",
        "windbreaker",
        "scarf",
        "bonnet",
    ],
    "nature": [
        "glacier",
        "sequoia",
        "geyser",
        "avalanche",
        "tornado",
        "quicksand",
        "stalactite",
        "hurricane",
        "asteroid",
        "tundra",
        "galaxy",
        "nebula",
        "earthquake",
        "stalagmite",
        "constellation",
        "crystal",
        "tributary",
        "abyss",
        "monsoon",
        "magma",
        "erosion",
        "iceberg",
        "mudslide",
        "delta",
        "aurora",
        "gravity",
        "humidity",
        "sinkhole",
        "wildfire",
        "tropics",
        "tsunami",
        "eclipse",
        "metabolism",
        "mirage",
        "hemisphere",
        "spectrum",
        "fossil",
        "plateau",
        "groundwater",
        "undergrowth",
        "oxygen",
        "molecule",
        "pollination",
        "algae",
        "carbon",
        "nitrogen",
        "organism",
        "nucleus",
        "equator",
        "solstice",
        "cocoon",
        "germination",
        "metamorphosis",
        "nocturnal",
        "symbiosis",
        "ecosystem",
        "biodiversity",
    ],
    "emotions": [
        "happiness",
        "sadness",
        "anxiety",
        "surprise",
        "anger",
        "curiosity",
        "embarrassment",
        "nostalgia",
        "envy",
        "gratitude",
        "remorse",
        "boredom",
        "excitement",
        "loneliness",
        "pride",
        "jealousy",
        "contentment",
        "disgust",
        "empathy",
        "euphoria",
        "melancholy",
        "frustration",
        "anticipation",
        "amusement",
        "serenity",
        "disappointment",
        "confidence",
        "resentment",
        "apathy",
        "optimism",
        "pessimism",
        "bewilderment",
        "exhilaration",
        "indifference",
        "enthusiasm",
        "desperation",
        "satisfaction",
        "regret",
        "determination",
        "compassion",
        "hopelessness",
        "relief",
        "infatuation",
        "tranquility",
        "impatience",
        "exasperation",
        "agitation",
        "yearning",
        "sympathy",
        "admiration",
        "astonishment",
        "inspiration",
        "dread",
        "hope",
    ],
}


def generate_game_words(num_words=20):
    """Generate a random selection of words for the Word Wrangler game.

    1. Create a flat list of all words
    2. Remove any duplicates
    3. Randomly select the requested number of words

    Args:
        num_words: Number of words to select for the game

    Returns:
        List of randomly selected words
    """
    # Create a flat list of all words from all categories
    all_words = []
    for category_words in WORD_CATEGORIES.values():
        all_words.extend(category_words)

    # Remove duplicates by converting to a set and back to a list
    all_words = list(set(all_words))

    # Randomly select words
    selected_words = random.sample(all_words, min(num_words, len(all_words)))

    return selected_words



================================================
FILE: word-wrangler-gemini-live/web-game/client/env.example
================================================
BOT_START_URL=http://localhost:7860/start
BOT_START_PUBLIC_API_KEY=
AGENT_NAME=word-wrangler


================================================
FILE: word-wrangler-gemini-live/web-game/client/eslint.config.mjs
================================================
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;



================================================
FILE: word-wrangler-gemini-live/web-game/client/next.config.ts
================================================
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
  reactStrictMode: true,
};

export default nextConfig;



================================================
FILE: word-wrangler-gemini-live/web-game/client/package.json
================================================
{
  "name": "client",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "build": "next build",
    "start": "next start",
    "lint": "next lint"
  },
  "dependencies": {
    "@pipecat-ai/client-js": "^1.4.0",
    "@pipecat-ai/client-react": "^1.1.0",
    "@pipecat-ai/daily-transport": "^1.4.1",
    "@tabler/icons-react": "^3.31.0",
    "@tailwindcss/postcss": "^4.1.3",
    "jotai": "^2.12.5",
    "js-confetti": "^0.12.0",
    "next": "15.2.4",
    "react": "^19.0.0",
    "react-dom": "^19.0.0"
  },
  "devDependencies": {
    "@eslint/eslintrc": "^3",
    "@types/node": "^20",
    "@types/react": "^19",
    "@types/react-dom": "^19",
    "eslint": "^9",
    "eslint-config-next": "15.2.4",
    "postcss": "^8.5.3",
    "tailwindcss": "^4.1.3",
    "typescript": "^5"
  }
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/postcss.config.mjs
================================================
const config = {
  plugins: ["@tailwindcss/postcss"],
};

export default config;



================================================
FILE: word-wrangler-gemini-live/web-game/client/tsconfig.json
================================================
{
  "compilerOptions": {
    "target": "ES2017",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "paths": {
      "@/components/*": [
        "./src/components/*"
      ],
      "@/contexts/*": [
        "./src/contexts/*"
      ],
      "@/providers/*": [
        "./src/providers/*"
      ],
      "@/styles/*": [
        "./src/styles/*"
      ],
      "@/data/*": [
        "./src/data/*"
      ],
      "@/types/*": [
        "./src/types/*"
      ],
      "@/constants/*": [
        "./src/constants/*"
      ],
      "@/utils/*": [
        "./src/utils/*"
      ],
      "@/hooks/*": [
        "./src/hooks/*"
      ]
    },
    "plugins": [
      {
        "name": "next"
      }
    ]
  },
  "include": [
    "**/*.ts",
    "**/*.tsx",
    "next-env.d.ts",
    ".next/types/**/*.ts"
  ],
  "exclude": [
    "node_modules"
  ]
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/app/layout.tsx
================================================
import { Nunito } from 'next/font/google';
import { Metadata } from 'next';
import { Providers } from './providers';
import '../styles/globals.css';

const nunito = Nunito({
  subsets: ['latin'],
  display: 'swap',
  variable: '--font-sans',
});

export const metadata: Metadata = {
  title: 'Daily | Word Wrangler',
  description: 'Describe words without saying them and an AI will guess them!',
  icons: {
    icon: [
      { url: '/favicon.ico' },
      { url: '/favicon.svg', type: 'image/svg+xml' },
    ],
  },
  openGraph: {
    type: 'website',
    url: 'https://word-wrangler.vercel.app/',
    title: 'Word Wrangler - AI Word Guessing Game',
    description:
      'Describe words without saying them and an AI will guess them!',
    images: ['/og-image.png'],
  },
  twitter: {
    card: 'summary_large_image',
    title: 'Word Wrangler - AI Word Guessing Game',
    description:
      'Describe words without saying them and an AI will guess them!',
    images: ['/og-image.png'],
  },
};

export default function RootLayout({
  children,
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en">
      <body>
        <main className={`${nunito.variable}`}>
          <Providers>{children}</Providers>
        </main>
      </body>
    </html>
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/app/page.tsx
================================================
'use client';

import { Card, CardInner } from '@/components/Card';
import { WordWrangler } from '@/components/Game/WordWrangler';
import { StartGameButton } from '@/components/StartButton';
import { GAME_TEXT } from '@/constants/gameConstants';
import { useConfigurationSettings } from '@/contexts/Configuration';
import { PERSONALITY_PRESETS, PersonalityType } from '@/types/personality';
import {
  IconArrowForwardUp,
  IconCheck,
  IconCode,
  IconX,
} from '@tabler/icons-react';
import JSConfetti from 'js-confetti';
import Image from 'next/image';
import Link from 'next/link';
import { useEffect, useState } from 'react';
import Logo from '../assets/logo.png';
import Star from '../assets/star.png';

export default function Home() {
  const [hasStarted, setHasStarted] = useState(false);
  const [gameEnded, setGameEnded] = useState(false);
  const [score, setScore] = useState(0);
  const [bestScore, setBestScore] = useState(0);
  const config = useConfigurationSettings();

  useEffect(() => {
    if (gameEnded) {
      const confetti = new JSConfetti();
      confetti.addConfetti({
        emojis: ['⭐', '⚡️', '👑', '✨', '💫', '🏆', '💯'],
      });
    }
  }, [gameEnded]);

  if (gameEnded) {
    return (
      <div className="flex flex-col justify-between lg:justify-center items-center min-h-[100dvh] py-4">
        <div className="flex flex-1 w-full">
          <Card className="w-full lg:max-w-2xl mx-auto mt-[50px] lg:mt-[120px] self-center text-center pt-[62px]">
            <div className="flex items-center justify-center w-[162px] h-[162px] rounded-full absolute z-20 -top-[81px] left-1/2 -translate-x-1/2 animate-bounce-in">
              <Image src={Star} alt="Star" priority />
            </div>
            <CardInner>
              <h2 className="text-xl font-extrabold">{GAME_TEXT.finalScore}</h2>
              <p className="text-4xl font-extrabold text-emerald-700 bg-emerald-50 rounded-full px-4 py-4 my-4">
                {score}
              </p>
              <p className="font-medium text-slate-500">
                {GAME_TEXT.finalScoreMessage}{' '}
                <span className="text-slate-700 font-extrabold">
                  {bestScore}
                </span>
              </p>
              <div className="h-[1px] bg-slate-200 my-6" />
              <div className="flex items-center justify-center">
                <Link
                  href="https://github.com/daily-co/word-wrangler-gemini-live"
                  className="button ghost w-full lg:w-auto"
                  target="_blank"
                  rel="noopener noreferrer">
                  <IconCode size={24} />
                  View project source code
                </Link>
              </div>
            </CardInner>
          </Card>
        </div>
        <footer className="flex flex-col justify-center w-full py-4 lg:py-12">
          <StartGameButton
            isGameEnded={true}
            onGameStarted={() => {
              setGameEnded(false);
              setScore(0);
              setHasStarted(true);
            }}
          />
        </footer>
      </div>
    );
  }

  if (!hasStarted) {
    return (
      <div className="flex flex-col justify-between items-center min-h-[100dvh] py-4 overflow-hidden">
        <div className="flex flex-1">
          <Card className="lg:min-w-2xl mx-auto mt-[50px] lg:mt-[120px] self-center">
            <Image
              src={Logo}
              alt="Word Wrangler"
              className="logo size-[150px] lg:size-[278px] absolute top-[-75px] lg:top-[-139px] left-[50%] -translate-x-1/2 z-10 animate-bounce-in"
              priority
            />

            <CardInner>
              <div className="flex flex-col gap-5 lg:gap-8 text-center mt-[50px] lg:mt-[100px]">
                <h2 className="text-xl font-extrabold">
                  {GAME_TEXT.introTitle}
                </h2>
                <div className="flex flex-col gap-3 lg:gap-4">
                  <div className="flex flex-row gap-3 relative">
                    <div className="absolute -top-3 -left-3 border-3 border-white lg:static size-10 lg:size-12 bg-emerald-100 text-emerald-500 rounded-full flex items-center justify-center font-semibold">
                      <IconCheck size={24} />
                    </div>
                    <div className="flex-1 flex h-[53px] lg:h-auto bg-slate-100 rounded-full text-slate-500 leading-5 px-12 items-center justify-center font-semibold text-pretty text-sm lg:text-base">
                      {GAME_TEXT.introGuide1}
                    </div>
                  </div>
                  <div className="flex flex-row gap-3 relative">
                    <div className="absolute -top-3 -left-3 border-3 border-white lg:static size-10 lg:size-12 bg-red-100 text-red-500 rounded-full flex items-center justify-center font-semibold">
                      <IconX size={24} />
                    </div>
                    <div className="flex-1 flex h-[53px] lg:h-auto bg-slate-100 rounded-full text-slate-500 leading-5 px-12 items-center justify-center font-semibold text-pretty text-sm lg:text-base">
                      {GAME_TEXT.introGuide2}
                    </div>
                  </div>
                  <div className="flex flex-row gap-3 relative">
                    <div className="absolute -top-3 -left-3 border-3 border-white lg:static size-10 lg:size-12 bg-slate-100 text-slate-400 rounded-full flex items-center justify-center font-semibold">
                      <IconArrowForwardUp size={24} />
                    </div>
                    <div className="flex-1 flex h-[53px] lg:h-auto bg-slate-100 rounded-full text-slate-500 leading-5 px-12 items-center justify-center font-semibold text-pretty text-sm lg:text-base">
                      {GAME_TEXT.introGuide3}
                    </div>
                  </div>
                </div>
              </div>
              <div className="flex-1 bg-slate-100 h-[1px] my-4 lg:my-6" />
              <div>
                <label className="font-bold flex flex-col gap-2 flex-1">
                  {GAME_TEXT.aiPersonality}
                  <select
                    className="rounded-xl h-11 font-normal"
                    value={config.personality}
                    onChange={(e) =>
                      config.setPersonality(e.target.value as PersonalityType)
                    }>
                    {Object.entries(PERSONALITY_PRESETS).map(
                      ([value, label]) => (
                        <option key={value} value={value}>
                          {label}
                        </option>
                      )
                    )}
                  </select>
                </label>
              </div>
            </CardInner>
          </Card>
        </div>
        <footer className="flex flex-col justify-center w-full py-4 lg:py-12">
          <StartGameButton onGameStarted={() => setHasStarted(true)} />
        </footer>
      </div>
    );
  }

  return (
    <WordWrangler
      onGameEnded={(score, bestScore = 0) => {
        setScore(score);
        setBestScore(bestScore);
        setGameEnded(true);
      }}
    />
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/app/providers.tsx
================================================
'use client';

import { ConfigurationProvider } from '@/contexts/Configuration';
import { PipecatProvider } from '@/providers/PipecatProvider';
import { PipecatClientAudio } from '@pipecat-ai/client-react';
import { ReactNode } from 'react';

export function Providers({ children }: { children: ReactNode }) {
  return (
    <ConfigurationProvider>
      <PipecatProvider>
        <PipecatClientAudio />
        {children}
      </PipecatProvider>
    </ConfigurationProvider>
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/app/api/start/route.ts
================================================
import { NextRequest, NextResponse } from 'next/server';

export async function POST(request: NextRequest) {
  const { personality } = await request.json();
  // Use BOT_START_URL from environment or fallback to localhost
  const botStartUrl =
    process.env.BOT_START_URL || 'http://localhost:7860/start';

  try {
    // Prepare headers - make API key optional
    const headers: Record<string, string> = {
      'Content-Type': 'application/json',
    };

    // Only add Authorization header if API key is provided
    if (process.env.BOT_START_PUBLIC_API_KEY) {
      headers.Authorization = `Bearer ${process.env.BOT_START_PUBLIC_API_KEY}`;
    }

    const response = await fetch(botStartUrl, {
      method: 'POST',
      headers,
      body: JSON.stringify({
        createDailyRoom: true,
        dailyRoomProperties: { start_video_off: true },
        body: {
          personality,
        },
      }),
    });

    if (!response.ok) {
      throw new Error(`Failed to connect to Pipecat: ${response.statusText}`);
    }

    const data = await response.json();

    if (data.error) {
      throw new Error(data.error);
    }

    return NextResponse.json(data);
  } catch (error) {
    return NextResponse.json(
      { error: `Failed to process connection request: ${error}` },
      { status: 500 }
    );
  }
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Card.tsx
================================================
export function Card({
  children,
  className,
}: {
  children: React.ReactNode;
  className?: string;
}) {
  return (
    <div
      className={`bg-white rounded-3xl relative card-border text-black ${className}`}
    >
      {children}
    </div>
  );
}

export function CardInner({ children }: { children: React.ReactNode }) {
  return <div className="p-6 lg:p-10 relative z-2">{children}</div>;
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/GameContent.tsx
================================================
import { GAME_STATES, GAME_TEXT, GameState } from "@/constants/gameConstants";
import { IconArrowForwardUp, IconClockPause } from "@tabler/icons-react";
import React from "react";
import { GameWord } from "./GameWord";
import { Timer } from "./Timer";
import styles from "./WordWrangler.module.css";

interface GameContentProps {
  gameState: GameState;
  currentWord: string;
  showAutoDetected: boolean;
  timeLeft: number;
  showIncorrect: boolean;
  score: number;
  skipsRemaining: number;
  // onCorrect: () => void;
  onSkip: () => void;
}

export const GameContent: React.FC<GameContentProps> = ({
  gameState,
  currentWord,
  showAutoDetected,
  showIncorrect,
  timeLeft,
  score,
  skipsRemaining,
  //onCorrect,
  onSkip,
}) => {
  // Idle or Connecting State
  if (gameState === GAME_STATES.IDLE || gameState === GAME_STATES.CONNECTING) {
    return (
      <div className={styles.simpleLoadingPlaceholder}>
        {GAME_TEXT.startingGame}
      </div>
    );
  }

  // Waiting for Intro State
  if (gameState === GAME_STATES.WAITING_FOR_INTRO) {
    return (
      <div className="animate-pulse flex flex-1 flex-col gap-3 items-center justify-center">
        <span className="size-18 flex items-center justify-center rounded-full bg-slate-900/50 text-white">
          <IconClockPause size={42} className="opacity-50" />
        </span>
        <span className="text-white text-2xl font-bold">
          {GAME_TEXT.waitingForIntro}
        </span>
      </div>
    );
  }

  // Finished State
  if (gameState === GAME_STATES.FINISHED) {
    return (
      <div className={styles.gameReadyArea}>
        <div className={styles.gameResults}>
          <h2>{GAME_TEXT.gameOver}</h2>
          <p>
            {GAME_TEXT.finalScore}: <strong>{score}</strong>
          </p>
        </div>
        <div className={styles.statusNote}>{GAME_TEXT.clickToStart}</div>
      </div>
    );
  }

  // Active Game State
  return (
    <div className={styles.gameArea}>
      <GameWord
        word={currentWord}
        showAutoDetected={showAutoDetected}
        showIncorrect={showIncorrect}
      />
      <div className="flex flex-col lg:flex-row gap-2 w-full">
        <Timer timeLeft={timeLeft} gameState={gameState} />
        <button
          className="button"
          onClick={onSkip}
          disabled={skipsRemaining <= 0}
        >
          <IconArrowForwardUp size={24} className="opacity-50" />
          {skipsRemaining > 0
            ? GAME_TEXT.skipsRemaining(skipsRemaining)
            : GAME_TEXT.noSkips}
        </button>
      </div>
    </div>
  );
};



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/GameWord.tsx
================================================
import { GAME_TEXT } from "@/constants/gameConstants";
import React from "react";
import styles from "./WordWrangler.module.css";

interface GameWordProps {
  word: string;
  showAutoDetected: boolean;
  showIncorrect: boolean;
}

export const GameWord: React.FC<GameWordProps> = ({
  word,
  showAutoDetected,
  showIncorrect,
}) => {
  return (
    <div
      className={`${styles.currentWord} ${
        showAutoDetected ? styles.correctWordDetected : ""
      } ${showIncorrect ? styles.incorrectWordDetected : ""}`}
    >
      <span className={styles.helpText}>{GAME_TEXT.describeWord}</span>
      <span className={styles.word}>{word}</span>

      {showAutoDetected && <CorrectOverlay />}
      {showIncorrect && <IncorrectOverlay />}
    </div>
  );
};

const CorrectOverlay: React.FC = () => (
  <div className={styles.autoDetectedOverlay}>
    <div className={styles.checkmarkContainer}>
      <svg
        className={styles.checkmarkSvg}
        xmlns="http://www.w3.org/2000/svg"
        viewBox="0 0 52 52"
      >
        <circle
          className={styles.checkmarkCircle}
          cx="26"
          cy="26"
          r="25"
          fill="none"
        />
        <path
          className={styles.checkmarkCheck}
          fill="none"
          d="M14.1 27.2l7.1 7.2 16.7-16.8"
        />
      </svg>
    </div>
  </div>
);

const IncorrectOverlay: React.FC = () => (
  <div className={styles.incorrectOverlay}>
    <div className={styles.xmarkContainer}>
      <svg
        className={styles.xmarkSvg}
        xmlns="http://www.w3.org/2000/svg"
        viewBox="0 0 52 52"
      >
        <circle
          className={styles.xmarkCircle}
          cx="26"
          cy="26"
          r="25"
          fill="none"
        />
        <path
          className={styles.xmarkX}
          fill="none"
          d="M16 16 L36 36 M36 16 L16 36"
        />
      </svg>
    </div>
  </div>
);



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/Timer.tsx
================================================
import { GAME_CONFIG, GAME_STATES } from "@/constants/gameConstants";
import { formatTime } from "@/utils/formatTime";
import { IconStopwatch } from "@tabler/icons-react";
import styles from "./WordWrangler.module.css";

interface TimerProps {
  timeLeft: number;
  gameState: string;
}

export function Timer({ timeLeft, gameState }: TimerProps) {
  const lowTimer =
    gameState === GAME_STATES.ACTIVE &&
    timeLeft <= GAME_CONFIG.LOW_TIME_WARNING;

  return (
    <div className={`${styles.timer} ${lowTimer ? styles.lowTime : ""}`}>
      <div className={styles.timerBadge}>
        <IconStopwatch size={24} />
        <span>{formatTime(timeLeft)}</span>
      </div>
      <div className={styles.timerBar}>
        <div
          className={styles.timerBarFill}
          style={{ width: `${(timeLeft / GAME_CONFIG.GAME_DURATION) * 100}%` }}
        />
      </div>
    </div>
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/WordWrangler.module.css
================================================
.gameContainer {
  position: relative;
  z-index: 1;
  padding: 4px;
  width: 100%;
  border-radius: 28px;
  margin-top: 50px;
  min-height: 300px;
  box-shadow: 0px 66px 26px rgba(0, 0, 0, 0.01),
    0px 37px 22px rgba(0, 0, 0, 0.05), 0px 16px 16px rgba(0, 0, 0, 0.09),
    0px 4px 9px rgba(0, 0, 0, 0.1);
}

@media (min-width: 1024px) {
  .gameContainer {
    width: auto;
    flex: none;
    min-width: 626px;
    height: 260px;
    margin-top: 0;
  }
}

.gameContainer:before {
  content: "";
  position: absolute;
  inset: -4px -4px -8px -4px;
  border-radius: 28px;
  background: linear-gradient(
    to bottom,
    rgba(0, 0, 0, 1) 0%,
    rgba(0, 0, 0, 0.15) 100%
  );
  z-index: -1;
}

.gameContainer:after {
  content: "";
  box-sizing: border-box;
  position: absolute;
  inset: 0;
  border-radius: var(--border-radius-card);
  border: var(--border-width-card) solid transparent;
  background-image: linear-gradient(#001146, #0655cc),
    linear-gradient(
      180deg,
      var(--theme-gradient-start) 0%,
      var(--theme-gradient-end) 100%
    );
  background-origin: border-box;
  background-clip: padding-box, border-box;
}

.gameContent {
  position: relative;
  z-index: 1;
  background: transparent;
  border-radius: 20px;
  width: 100%;
  height: 100%;
  min-height: 292px;
  display: flex;
  overflow: hidden;
  border: 6px solid rgba(0, 0, 0, 0.25);
}

.gameContent:after {
  content: "";
  position: absolute;
  inset: 0;
  background: radial-gradient(
    70% 40% at 50% 40%,
    #2da6ee 0%,
    rgba(45, 166, 238, 0) 100%
  );
  opacity: 0.76;
  z-index: -1;
}

.gameArea {
  display: flex;
  flex-direction: column;
  align-items: center;
  flex: 1;
  padding: 12px;
  position: relative;
  z-index: 2;
}

.timer {
  height: var(--button-height);
  border-radius: 9999px;
  width: 100%;
  flex-direction: row;
  gap: 12px;
  display: flex;
  align-items: center;
  justify-content: center;
  background-color: rgba(0, 0, 0, 0.2);
  padding: 12px;

  @media (min-width: 1024px) {
    flex: 1;
  }

  .timerBadge {
    display: flex;
    flex-direction: row;
    align-items: center;
    gap: 6px;
    background-color: black;
    border-radius: 9999px;
    color: white;
    height: 100%;
    padding: 0 12px;
    font-weight: 800;
  }

  .timerBar {
    height: 100%;
    width: 100%;
    border-radius: 9999px;
    overflow: hidden;
    background-color: var(--color-emerald-100);
  }

  .timerBarFill {
    height: 100%;
    width: 100%;
    background-color: var(--color-emerald-400);
    transition: width 0.3s ease;
  }

  &.lowTime {
    color: #e74c3c;
    animation: pulse 1s infinite;

    .timerBar {
      background-color: var(--color-orange-100);
    }

    .timerBarFill {
      background-color: var(--color-orange-400);
    }
  }
}

.scoreDisplay {
  font-size: 1.25rem;
  font-weight: 500;
  color: #0071e3;
}

.currentWord {
  display: flex;
  flex: 1;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  text-align: center;
  width: 100%;
  margin-top: 50px;
  .helpText {
    font-size: 1rem;
    font-weight: 700;
    color: rgba(255, 255, 255, 0.5);
  }

  .word {
    font-size: 2rem;
    font-weight: 800;
    letter-spacing: 0.05em;
    line-height: 2;
    color: #ffffff;
    text-shadow: 0px 4px 0px rgba(0, 0, 0, 0.45);
  }

  @media (min-width: 1024px) {
    margin-top: 0;
    .word {
      font-size: 3rem;
      text-shadow: 0px 6px 0px rgba(0, 0, 0, 0.45);
    }
  }
}

.gameButton {
  padding: 0.85rem 0;
  font-size: 1.1rem;
  font-weight: 500;
  border: none;
  border-radius: 8px;
  cursor: pointer;
  transition: all 0.2s ease;
}

/* Primary button (Skip) */
.skipButton {
  flex: 2; /* Takes more space */
  background-color: #e74c3c;
  color: white;
}

.skipButton:hover {
  background-color: #c0392b;
  transform: translateY(-2px);
}

/* Secondary button (Correct) - more subdued */
.correctButton {
  flex: 1; /* Takes less space */
  background-color: #f5f5f7; /* Light gray background */
  color: #333; /* Dark text */
  border: 1px solid #ddd; /* Subtle border */
}

.correctButton:hover {
  background-color: #e8e8ed;
  transform: translateY(-1px);
}

.gameReadyArea {
  display: flex;
  flex-direction: column;
  align-items: center;
}

.gameResults {
  margin-bottom: 1rem;
  padding: 0.75rem;
  background-color: #f8f9fa;
  border-radius: 8px;
  width: 100%;
  text-align: center;
}

.gameResults h2 {
  margin: 0 0 0.5rem 0;
  color: #333;
  font-size: 1.3rem;
}

.statusNote {
  margin: 0.5rem 0;
  padding: 0.6rem 1rem;
  background-color: #f8f9fa;
  border-left: 3px solid #0071e3;
  font-size: 0.95rem;
  color: #333;
  width: 100%;
  text-align: center;
  border-radius: 4px;
}

.compactInstructions {
  margin: 0.75rem 0;
  width: 100%;
  max-width: 400px;
  background-color: #f8f9fa;
  border-radius: 8px;
  padding: 0.75rem 1rem;
}

.compactInstructions h3 {
  margin: 0 0 0.5rem 0;
  color: #333;
  font-size: 1.1rem;
  text-align: center;
}

.compactInstructions ul {
  margin: 0;
  padding-left: 1.5rem;
  line-height: 1.4;
}

.compactInstructions li {
  margin-bottom: 0.4rem;
  font-size: 0.9rem;
}

.loadingDots {
  display: inline-block;
  animation: dotPulse 1.5s infinite linear;
}

@keyframes dotPulse {
  0% {
    opacity: 0.2;
  }
  20% {
    opacity: 1;
  }
  100% {
    opacity: 0.2;
  }
}

@keyframes pulse {
  0% {
    opacity: 0.8;
  }
  50% {
    opacity: 1;
  }
  100% {
    opacity: 0.8;
  }
}

/* Animation styles */
.correctWordDetected {
  animation: correctPulse 1.5s ease-in-out;
  position: relative;
}

.autoDetectedOverlay {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  display: flex;
  justify-content: center;
  align-items: center;
  background-color: rgba(46, 204, 113, 0.6);
  border-radius: 8px;
  animation: fadeIn 0.3s ease-in-out;
  z-index: 10;
}

.checkmarkContainer {
  width: 80px;
  height: 80px;
  animation: scaleUp 0.4s ease-out;
}

.checkmarkSvg {
  width: 100%;
  height: 100%;
  border-radius: 50%;
  display: block;
  stroke-width: 4;
  stroke: #fff;
  stroke-miterlimit: 10;
  box-shadow: 0 0 0 rgba(46, 204, 113, 0.7);
  animation: fillCheck 0.3s ease-in-out 0.3s forwards,
    scale 0.2s ease-in-out 0.7s both;
}

.checkmarkCircle {
  stroke-dasharray: 166;
  stroke-dashoffset: 166;
  stroke-width: 4;
  stroke-miterlimit: 10;
  stroke: #fff;
  fill: transparent;
  animation: strokeCheck 0.5s cubic-bezier(0.65, 0, 0.45, 1) forwards;
}

.checkmarkCheck {
  transform-origin: 50% 50%;
  stroke-dasharray: 48;
  stroke-dashoffset: 48;
  animation: strokeCheck 0.25s cubic-bezier(0.65, 0, 0.45, 1) 0.6s forwards;
}

@keyframes strokeCheck {
  100% {
    stroke-dashoffset: 0;
  }
}

@keyframes fillCheck {
  100% {
    box-shadow: inset 0 0 0 50px transparent;
  }
}

@keyframes correctPulse {
  0% {
    box-shadow: 0 0 0 0 rgba(46, 204, 113, 0.7);
  }
  50% {
    box-shadow: 0 0 0 15px rgba(46, 204, 113, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(46, 204, 113, 0);
  }
}

@keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}

@keyframes scaleUp {
  from {
    transform: scale(0.5);
    opacity: 0;
  }
  to {
    transform: scale(1);
    opacity: 1;
  }
}

.incorrectWordDetected {
  animation: incorrectPulse 1.5s ease-in-out,
    shake 0.5s cubic-bezier(0.36, 0.07, 0.19, 0.97) both;
  position: relative;
}

.incorrectOverlay {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  display: flex;
  justify-content: center;
  align-items: center;
  background-color: rgba(255, 59, 48, 0.6); /* Red with transparency */
  border-radius: 8px;
  animation: fadeIn 0.3s ease-in-out;
  z-index: 10;
}

.xmarkContainer {
  width: 80px;
  height: 80px;
  animation: scaleUp 0.4s ease-out;
}

.xmarkSvg {
  width: 100%;
  height: 100%;
  border-radius: 50%;
  display: block;
  stroke-width: 4;
  stroke: #fff;
  stroke-miterlimit: 10;
  box-shadow: 0 0 0 rgba(255, 59, 48, 0.7);
  animation: fillX 0.3s ease-in-out 0.3s forwards,
    scale 0.2s ease-in-out 0.7s both;
}

.xmarkCircle {
  stroke-dasharray: 166;
  stroke-dashoffset: 166;
  stroke-width: 4;
  stroke-miterlimit: 10;
  stroke: #fff;
  fill: transparent;
  animation: strokeX 0.5s cubic-bezier(0.65, 0, 0.45, 1) forwards;
}

.xmarkX {
  transform-origin: 50% 50%;
  stroke-dasharray: 48;
  stroke-dashoffset: 48;
  animation: strokeX 0.25s cubic-bezier(0.65, 0, 0.45, 1) 0.6s forwards;
}

@keyframes strokeX {
  100% {
    stroke-dashoffset: 0;
  }
}

@keyframes fillX {
  100% {
    box-shadow: inset 0 0 0 50px transparent;
  }
}

@keyframes incorrectPulse {
  0% {
    box-shadow: 0 0 0 0 rgba(255, 59, 48, 0.7);
  }
  50% {
    box-shadow: 0 0 0 15px rgba(255, 59, 48, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(255, 59, 48, 0);
  }
}

@keyframes scale {
  0%,
  100% {
    transform: none;
  }
  50% {
    transform: scale3d(1.1, 1.1, 1);
  }
}

@keyframes shake {
  10%,
  90% {
    transform: translate3d(-1px, 0, 0);
  }
  20%,
  80% {
    transform: translate3d(2px, 0, 0);
  }
  30%,
  50%,
  70% {
    transform: translate3d(-3px, 0, 0);
  }
  40%,
  60% {
    transform: translate3d(3px, 0, 0);
  }
}

/* Game loading UI styles */
.gameLoadingContainer {
  display: flex;
  justify-content: center;
  align-items: center;
  height: 250px; /* Fixed height to prevent layout shifts */
  width: 100%;
}

.gameLoadingContent {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 1.5rem;
  text-align: center;
}

.gameLoadingIcon {
  position: relative;
  width: 60px;
  height: 60px;
  display: flex;
  justify-content: center;
  align-items: center;
}

.pulseDot {
  width: 16px;
  height: 16px;
  background-color: #0071e3;
  border-radius: 50%;
  position: relative;
}

.pulseDot:before {
  content: "";
  position: absolute;
  width: 100%;
  height: 100%;
  border-radius: 50%;
  background-color: #0071e3;
  opacity: 0.7;
  animation: pulse-wave 1.5s linear infinite;
}

.gameLoadingTitle {
  font-size: 1.5rem;
  font-weight: 500;
  color: #0071e3;
  margin: 0;
}

@keyframes pulse-wave {
  0% {
    transform: scale(1);
    opacity: 0.7;
  }
  50% {
    transform: scale(2.5);
    opacity: 0;
  }
  100% {
    transform: scale(1);
    opacity: 0;
  }
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/WordWrangler.tsx
================================================
import { GAME_STATES, GAME_TEXT } from "@/constants/gameConstants";
import { useConnectionState } from "@/hooks/useConnectionState";
import { useGameState } from "@/hooks/useGameState";
import { useGameTimer } from "@/hooks/useGameTimer";
import { useVisualFeedback } from "@/hooks/useVisualFeedback";
import { useWordDetection } from "@/hooks/useWordDetection";
import { RTVIEvent } from "@pipecat-ai/client-js";
import { useRTVIClientEvent } from "@pipecat-ai/client-react";
import { IconCircleDashedCheck, IconDoorExit } from "@tabler/icons-react";
import { useCallback, useEffect, useRef } from "react";
import Logo from "../../assets/logo.png";
import { GameContent } from "./GameContent";
import { ScoreRow } from "./ScoreRow";

import JSConfetti from "js-confetti";

import Image from "next/image";
import styles from "./WordWrangler.module.css";

export const WordWrangler: React.FC<{
  onGameEnded: (score: number, bestScore: number) => void;
}> = ({ onGameEnded }) => {
  const botIntroCompletedRef = useRef(false);
  const currentScoreRef = useRef(0);
  const gameState = useGameState();
  const visualFeedback = useVisualFeedback();
  const { isConnected, client } = useConnectionState();

  // Update the ref whenever score changes
  useEffect(() => {
    currentScoreRef.current = gameState.score;
  }, [gameState.score]);

  // End the game
  const endGame = useCallback(async () => {
    const scoreAtCallTime = currentScoreRef.current;

    // Prevent multiple calls to endGame
    if (gameState.gameState === GAME_STATES.FINISHED) {
      console.log("endGame prevented - game already finished");
      return;
    }

    // Capture the current score before any state changes
    const finalScore = scoreAtCallTime;
    const currentBestScore = gameState.bestScore;

    // Update game state
    gameState.finishGame();
    visualFeedback.resetVisuals();

    // Update best score if needed
    if (currentBestScore < finalScore) {
      gameState.setBestScore(finalScore);
    }

    // Disconnect the bot
    if (client && isConnected) {
      try {
        await client.disconnectBot();
        await client.disconnect();
      } catch (error) {
        console.error("Error disconnecting bot:", error);
      }
    }

    // Call the callback with the captured scores
    onGameEnded(finalScore, Math.max(finalScore, currentBestScore));
  }, [gameState, visualFeedback, client, isConnected, onGameEnded]);

  const gameTimer = useGameTimer(endGame);

  const wordDetection = useWordDetection({
    gameState: gameState.gameState,
    currentWord: gameState.currentWord,
    onCorrectGuess: handleCorrectGuess,
    onIncorrectGuess: handleIncorrectGuess,
  });

  // Initialize on component mount
  useEffect(() => {
    gameState.initializeGame();
  }, []);

  // Handle connection state changes
  useEffect(() => {
    if (isConnected) {
      if (!botIntroCompletedRef.current) {
        // Connection is active, but bot hasn't completed intro
        gameState.setGameState(GAME_STATES.WAITING_FOR_INTRO);
      }
    } else {
      // Connection lost or never established
      if (gameState.gameState === GAME_STATES.ACTIVE) {
        // If game was active, it's now finished
        endGame();
      } else if (gameState.gameState !== GAME_STATES.FINISHED) {
        // Reset to idle state if not already finished
        gameState.setGameState(GAME_STATES.IDLE);
      }

      // Reset intro state when connection is lost
      botIntroCompletedRef.current = false;
    }
  }, [isConnected, gameState.gameState, endGame]);

  // Listen for the bot to stop speaking to detect intro completion
  useRTVIClientEvent(RTVIEvent.BotStoppedSpeaking, () => {
    if (
      gameState.gameState === GAME_STATES.WAITING_FOR_INTRO &&
      !botIntroCompletedRef.current
    ) {
      // First time the bot stops speaking, consider intro done and start the game
      botIntroCompletedRef.current = true;
      startGame();
    }
  });

  // Handle correct guess with animation
  function handleCorrectGuess() {
    visualFeedback.showCorrect(() => {
      gameState.incrementScore();
      gameState.moveToNextWord();
      wordDetection.resetLastProcessedMessage();
    });
    const jsConfetti = new JSConfetti();
    jsConfetti.addConfetti();
  }

  // Handle incorrect guess with animation
  function handleIncorrectGuess() {
    visualFeedback.showIncorrectAnimation();
  }

  // Start the game
  function startGame() {
    // Initialize game state
    gameState.initializeGame();
    wordDetection.resetLastProcessedMessage();

    // Start the timer - now it internally manages countdown and calls endGame when done
    gameTimer.startTimer();
  }

  // Handle manual marking as correct
  function handleManualCorrect() {
    if (gameState.gameState !== GAME_STATES.ACTIVE) return;

    gameState.incrementScore();

    const jsConfetti = new JSConfetti();
    jsConfetti.addConfetti();

    gameState.moveToNextWord();
    wordDetection.resetLastProcessedMessage();
  }

  // Handle skipping a word
  function handleSkip() {
    if (gameState.gameState !== GAME_STATES.ACTIVE) return;

    // Try to use a skip and proceed if successful
    if (gameState.useSkip()) {
      gameState.moveToNextWord();
      wordDetection.resetLastProcessedMessage();
    }
  }

  // Clean up on unmount
  useEffect(() => {
    return () => {
      gameTimer.stopTimer();
      visualFeedback.cleanup();
    };
  }, []);

  return (
    <div className="min-h-[100dvh] flex flex-col">
      <div className="flex-1 flex flex-col items-center justify-center h-screen">
        <div className="flex flex-1 flex-col lg:flex-row gap-6 lg:gap-12 items-center justify-center w-full lg:w-auto">
          <div className={styles.gameContainer}>
            <Image
              src={Logo}
              alt="Word Wrangler"
              className="logo size-[140px] absolute top-[-50px] lg:top-[-60px] left-[50%] -translate-x-1/2 lg:left-auto lg:-translate-x-0 lg:right-[-50px] z-10"
              priority
            />
            <div className={styles.gameContent}>
              <GameContent
                gameState={gameState.gameState}
                timeLeft={gameTimer.timeLeft}
                currentWord={gameState.currentWord}
                showAutoDetected={visualFeedback.showAutoDetected}
                showIncorrect={visualFeedback.showIncorrect}
                score={gameState.score}
                skipsRemaining={gameState.skipsRemaining}
                onSkip={handleSkip}
              />
            </div>
          </div>
          <ScoreRow score={gameState.score} bestScore={gameState.bestScore} />
        </div>
        <footer className="flex gap-2 py-4 lg:flex-row lg:gap-4 lg:py-6 w-full items-center justify-center">
          <button
            className="button outline w-full lg:w-auto"
            onClick={handleManualCorrect}
            disabled={gameState.gameState !== GAME_STATES.ACTIVE}
          >
            <IconCircleDashedCheck size={24} />
            {GAME_TEXT.correct}
          </button>
          <button
            className="button outline w-full lg:w-auto"
            onClick={endGame}
            disabled={
              gameState.gameState == GAME_STATES.CONNECTING ||
              gameState.gameState == GAME_STATES.WAITING_FOR_INTRO
            }
          >
            <IconDoorExit size={24} />
            End Game
          </button>
        </footer>
      </div>
    </div>
  );
};



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/ScoreRow/index.tsx
================================================
import { IconLaurelWreathFilled, IconStarFilled } from "@tabler/icons-react";
import styles from "./ScoreRow.module.css";
interface ScoreRowProps {
  score: number;
  bestScore: number;
}

export function ScoreRow({ score, bestScore = 0 }: ScoreRowProps) {
  return (
    <div className="flex flex-col w-full lg:w-auto justify-between gap-3 lg:gap-5">
      <div className="flex flex-1 w-full lg:w-auto flex-row items-center gap-3 lg:gap-5 text-white bg-black/20 rounded-2xl lg:rounded-3xl px-4 py-3 lg:px-6 lg:py-4">
        <IconStarFilled
          size={42}
          className="text-amber-300 size-8 lg:size-10"
        />
        <div className="flex flex-col gap-1">
          <span className="text-xs lg:text-sm uppercase font-extrabold tracking-wider">
            Current score
          </span>
          <span className="text-xl lg:text-2xl font-extrabold leading-none">
            {score}
          </span>
        </div>
      </div>
      <div className={styles.divider} />
      <div className="flex flex-row items-center gap-5 text-white rounded-3xl px-6">
        <IconLaurelWreathFilled
          size={42}
          className="text-amber-300 size-8 lg:size-10"
        />
        <div className="flex flex-col gap-1">
          <span className="text-xs lg:text-sm uppercase font-extrabold tracking-wider">
            Best score
          </span>
          <span className="text-xl lg:text-2xl font-extrabold leading-none">
            {bestScore}
          </span>
        </div>
      </div>
    </div>
  );
}

export default ScoreRow;



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/Game/ScoreRow/ScoreRow.module.css
================================================
.divider {
  width: 100%;
  height: 2px;
  background: linear-gradient(
    90deg,
    transparent 0%,
    rgba(255, 255, 255, 0.15) 30%,
    rgba(255, 255, 255, 0.15) 70%,
    transparent 100%
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/components/StartButton/index.tsx
================================================
import { BUTTON_TEXT } from "@/constants/gameConstants";
import { useConnectionState } from "@/hooks/useConnectionState";
import { IconArrowRight } from "@tabler/icons-react";

interface StartGameButtonProps {
  onGameStarted?: () => void;
  onGameEnded?: () => void;
  isGameEnded?: boolean;
}

export function StartGameButton({
  onGameStarted,
  onGameEnded,
  isGameEnded,
}: StartGameButtonProps) {
  const { isConnecting, isDisconnecting, toggleConnection } =
    useConnectionState(onGameStarted, onGameEnded);

  // Show spinner during connection process
  const showSpinner = isConnecting;
  const btnText = isGameEnded ? BUTTON_TEXT.RESTART : BUTTON_TEXT.START;

  return (
    <div className="flex justify-center">
      <button
        className="styled-button"
        onClick={toggleConnection}
        disabled={isConnecting || isDisconnecting}
      >
        <>
          <span className="styled-button-text">
            {isConnecting ? BUTTON_TEXT.CONNECTING : btnText}
          </span>
          <span className="styled-button-icon">
            {showSpinner ? (
              <span className="spinner"></span>
            ) : (
              <IconArrowRight size={16} strokeWidth={3} />
            )}
          </span>
        </>
      </button>
    </div>
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/constants/gameConstants.ts
================================================
// Game configuration
export const GAME_CONFIG = {
  MAX_SKIPS: 3,
  GAME_DURATION: 60, // seconds
  WORD_POOL_SIZE: 30,
  ANIMATION_DURATION: 1000, // ms
  TIMER_INTERVAL: 1000, // ms
  LOW_TIME_WARNING: 10, // seconds
};

// Game states
export const GAME_STATES = {
  IDLE: "idle",
  CONNECTING: "connecting",
  WAITING_FOR_INTRO: "waitingForIntro",
  ACTIVE: "active",
  FINISHED: "finished",
} as const;

export type GameState = (typeof GAME_STATES)[keyof typeof GAME_STATES];

// Text used in the game
export const GAME_TEXT = {
  time: "Time",
  score: "Score",
  gameOver: "Game Over!",
  finalScore: "Final Score",
  correct: "Mark Correct",
  skip: "Skip →",
  noSkips: "No Skips Left",
  skipsRemaining: (num: number) => `Skip (${num} left)`,
  startingGame: `How many words can you describe in ${GAME_CONFIG.GAME_DURATION} seconds?`,
  waitingForIntro: "Getting ready...",
  clickToStart: "Press Start Game to begin",
  describeWord: "Describe the following word:",
  introTitle: "How many words can you describe within 60 seconds?",
  introGuide1: "Earn points each time the AI correctly guesses the word",
  introGuide2: "Do not say the word, or you will lose points",
  introGuide3: "You can skip the word if you don't know it",
  aiPersonality: "AI Personality",
  finalScoreMessage: "Your best score:",
};

// Pattern for detecting guesses in transcripts
export const TRANSCRIPT_PATTERNS = {
  // Match both "Is it "word"?" and "Is it a/an word?" patterns
  GUESS_PATTERN:
    /is it [""]?([^""?]+)[""]?(?:\?)?|is it (?:a|an) ([^?]+)(?:\?)?/i,
};

// Connection states
export const CONNECTION_STATES = {
  ACTIVE: ["connected", "ready"],
  CONNECTING: ["connecting", "initializing", "initialized", "authenticating"],
  DISCONNECTING: ["disconnecting"],
};

// Button text
export const BUTTON_TEXT = {
  START: "Start Game",
  END: "End Game",
  CONNECTING: "Connecting...",
  STARTING: "Starting...",
  RESTART: "Play Again",
};



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/contexts/Configuration.tsx
================================================
'use client';

import React, { createContext, useContext, useState, ReactNode } from 'react';
import { PersonalityType, DEFAULT_PERSONALITY } from '@/types/personality';

interface ConfigurationContextProps {
  personality: PersonalityType;
  setPersonality: (personality: PersonalityType) => void;
}

const ConfigurationContext = createContext<
  ConfigurationContextProps | undefined
>(undefined);

interface ConfigurationProviderProps {
  children: ReactNode;
}

export function ConfigurationProvider({
  children,
}: ConfigurationProviderProps) {
  const [personality, setPersonality] =
    useState<PersonalityType>(DEFAULT_PERSONALITY);

  const value = {
    personality,
    setPersonality,
  };

  return (
    <ConfigurationContext.Provider value={value}>
      {children}
    </ConfigurationContext.Provider>
  );
}

export function useConfigurationSettings() {
  const context = useContext(ConfigurationContext);
  if (context === undefined) {
    throw new Error(
      'useConfigurationSettings must be used within a ConfigurationProvider'
    );
  }
  return context;
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/data/wordWranglerWords.ts
================================================
// 100 Easy Words - Common, everyday objects and concepts
export const EASY_CATCH_PHRASE_WORDS = [
  // Common Objects
  'Chair',
  'Table',
  'Door',
  'Window',
  'Book',
  'Pencil',
  'Phone',
  'Computer',
  'Ball',
  'Car',
  'Shoe',
  'Hat',
  'Cup',
  'Plate',
  'Fork',
  'Spoon',
  'Knife',
  'Key',
  'Clock',
  'Watch',

  // Food & Drink
  'Pizza',
  'Hamburger',
  'Ice cream',
  'Chocolate',
  'Apple',
  'Banana',
  'Orange',
  'Milk',
  'Water',
  'Cake',
  'Cookie',
  'Bread',
  'Egg',
  'Cheese',
  'Chicken',

  // Animals
  'Dog',
  'Cat',
  'Fish',
  'Bird',
  'Horse',
  'Cow',
  'Pig',
  'Duck',
  'Lion',
  'Tiger',
  'Bear',
  'Elephant',
  'Monkey',
  'Rabbit',
  'Frog',

  // Colors & Simple Concepts
  'Red',
  'Blue',
  'Green',
  'Yellow',
  'Black',
  'White',
  'Big',
  'Small',
  'Hot',
  'Cold',
  'Happy',
  'Sad',
  'Fast',
  'Slow',
  'Old',
  'Young',
  'Up',
  'Down',
  'Left',
  'Right',

  // Simple Activities
  'Run',
  'Walk',
  'Jump',
  'Swim',
  'Sleep',
  'Eat',
  'Drink',
  'Laugh',
  'Cry',
  'Smile',
  'Play',
  'Work',
  'Read',
  'Write',
  'Draw',
  'Sing',
  'Dance',
  'Talk',
  'Listen',
  'Cook',
];

// 300 Medium Words - More specific items, common concepts, popular culture
export const MEDIUM_CATCH_PHRASE_WORDS = [
  // Household Items & Technology
  'Refrigerator',
  'Television',
  'Microwave',
  'Bookshelf',
  'Couch',
  'Dishwasher',
  'Ceiling fan',
  'Toaster',
  'Vacuum cleaner',
  'Blender',
  'Printer',
  'Headphones',
  'Smartphone',
  'Laptop',
  'Tablet',
  'Camera',
  'Remote control',
  'Charger',
  'Keyboard',
  'Mouse',
  'Lightbulb',
  'Shower curtain',
  'Doorknob',
  'Power outlet',
  'Coffee maker',

  // Food & Cuisine
  'Spaghetti',
  'Burrito',
  'Sushi',
  'Pancake',
  'Waffle',
  'Cereal',
  'Sandwich',
  'Salad',
  'French fries',
  'Hot dog',
  'Cupcake',
  'Donut',
  'Milkshake',
  'Smoothie',
  'Oatmeal',
  'Peanut butter',
  'Jelly',
  'Bacon',
  'Scrambled eggs',
  'Toast',
  'Steak',
  'Mashed potatoes',
  'Broccoli',
  'Carrot',
  'Onion',

  // Animals & Nature
  'Giraffe',
  'Penguin',
  'Kangaroo',
  'Dolphin',
  'Octopus',
  'Butterfly',
  'Spider',
  'Eagle',
  'Turtle',
  'Squirrel',
  'Rainbow',
  'Waterfall',
  'Mountain',
  'Beach',
  'Forest',
  'Hurricane',
  'Snowflake',
  'Thunderstorm',
  'Volcano',
  'Desert',
  'Sunrise',
  'Sunset',
  'Moon',
  'Stars',
  'Planet',

  // Sports & Activities
  'Basketball',
  'Football',
  'Soccer',
  'Baseball',
  'Tennis',
  'Golf',
  'Swimming',
  'Skiing',
  'Snowboarding',
  'Hiking',
  'Camping',
  'Fishing',
  'Gardening',
  'Painting',
  'Photography',
  'Cycling',
  'Jogging',
  'Yoga',
  'Dancing',
  'Cooking',
  'Driving',
  'Flying',
  'Sailing',
  'Surfing',
  'Rock climbing',

  // Clothing & Accessories
  'Sunglasses',
  'Umbrella',
  'Necklace',
  'Bracelet',
  'Ring',
  'Earrings',
  'Backpack',
  'Purse',
  'Wallet',
  'Watch',
  'Sneakers',
  'Sandals',
  'Boots',
  'High heels',
  'Flip flops',
  'Scarf',
  'Gloves',
  'Belt',
  'Tie',
  'Jacket',
  'Sweater',
  'Sweatshirt',
  'Jeans',
  'Shorts',
  'Dress',

  // Places
  'Restaurant',
  'Grocery store',
  'Shopping mall',
  'Movie theater',
  'Park',
  'School',
  'Library',
  'Museum',
  'Zoo',
  'Airport',
  'Hospital',
  'Hotel',
  'Bank',
  'Post office',
  'Gym',
  'Beach',
  'Swimming pool',
  'Church',
  'Stadium',
  'Concert hall',
  'Farm',
  'City',
  'Village',
  'Country',
  'Island',

  // Jobs & Professions
  'Teacher',
  'Doctor',
  'Nurse',
  'Police officer',
  'Firefighter',
  'Chef',
  'Waiter',
  'Pilot',
  'Engineer',
  'Scientist',
  'Actor',
  'Singer',
  'Artist',
  'Writer',
  'Photographer',
  'Farmer',
  'Mechanic',
  'Electrician',
  'Plumber',
  'Carpenter',
  'Lawyer',
  'Accountant',
  'Businessman',
  'Salesperson',
  'Architect',

  // Transportation
  'Bicycle',
  'Motorcycle',
  'Bus',
  'Train',
  'Airplane',
  'Helicopter',
  'Boat',
  'Ship',
  'Submarine',
  'Rocket',
  'Taxi',
  'Ambulance',
  'Fire truck',
  'Police car',
  'School bus',
  'Skateboard',
  'Scooter',
  'Rollerblades',
  'Wagon',
  'Sled',
  'Escalator',
  'Elevator',
  'Tractor',
  'Bulldozer',
  'Crane',

  // Entertainment & Hobbies
  'Movie',
  'Music',
  'Book',
  'Game',
  'Puzzle',
  'Toy',
  'Doll',
  'Action figure',
  'Video game',
  'Board game',
  'Knitting',
  'Sewing',
  'Woodworking',
  'Baking',
  'Grilling',
  'Hunting',
  'Archery',
  'Bowling',
  'Karaoke',
  'Dancing',
  'Collecting',
  'Reading',
  'Writing',
  'Drawing',
  'Painting',

  // Body Parts & Health
  'Heart',
  'Brain',
  'Stomach',
  'Lungs',
  'Liver',
  'Kidneys',
  'Skin',
  'Hair',
  'Nails',
  'Teeth',
  'Eyes',
  'Ears',
  'Nose',
  'Mouth',
  'Throat',
  'Shoulder',
  'Elbow',
  'Wrist',
  'Hip',
  'Knee',
  'Ankle',
  'Exercise',
  'Medicine',
  'Doctor',
  'Hospital',

  // Common Concepts
  'Birthday',
  'Wedding',
  'Funeral',
  'Holiday',
  'Vacation',
  'Friendship',
  'Love',
  'Family',
  'Education',
  'Career',
  'Money',
  'Time',
  'Weather',
  'Season',
  'History',
  'Future',
  'Success',
  'Failure',
  'Challenge',
  'Opportunity',
  'Competition',
  'Cooperation',
  'Leadership',
  'Creativity',
  'Innovation',
];

// 100 Hard Words - Abstract concepts, specific terminology, complex ideas
export const HARD_CATCH_PHRASE_WORDS = [
  // Academic & Scientific Terms
  'Photosynthesis',
  'Mitochondria',
  'Quantum physics',
  'Relativity',
  'Biodiversity',
  'Logarithm',
  'Algorithm',
  'Neural network',
  'Nanotechnology',
  'Thermodynamics',
  'Paleontology',
  'Archaeology',
  'Linguistics',
  'Metaphysics',
  'Epistemology',
  'Cryptocurrency',
  'Blockchain',
  'Artificial intelligence',
  'Machine learning',
  'Virtual reality',

  // Abstract Concepts
  'Nostalgia',
  'Ambivalence',
  'Empathy',
  'Serendipity',
  'Irony',
  'Existentialism',
  'Utilitarianism',
  'Nihilism',
  'Altruism',
  'Pragmatism',
  'Transcendence',
  'Symbolism',
  'Paradox',
  'Dichotomy',
  'Synchronicity',
  'Meritocracy',
  'Bureaucracy',
  'Democracy',
  'Capitalism',
  'Socialism',

  // Specialized Terminology
  'Amortization',
  'Cryptocurrency',
  'Jurisprudence',
  'Cartography',
  'Meteorology',
  'Gentrification',
  'Immunology',
  'Horticulture',
  'Gerontology',
  'Acoustics',
  'Encryption',
  'Astrophysics',
  'Ornithology',
  'Entomology',
  'Viticulture',
  'Numismatics',
  'Philately',
  'Calligraphy',
  'Lexicography',
  'Etymology',

  // Cultural & Historical References
  'Renaissance',
  'Industrial Revolution',
  'Cold War',
  'Enlightenment',
  'Reformation',
  'Colonialism',
  'Globalization',
  'Diaspora',
  'Apartheid',
  'Imperialism',
  'Monarchy',
  'Republic',
  'Federation',
  'Aristocracy',
  'Oligarchy',
  'Surrealism',
  'Impressionism',
  'Baroque',
  'Neoclassicism',
  'Romanticism',

  // Complex Activities & Processes
  'Meditation',
  'Negotiation',
  'Diplomacy',
  'Arbitration',
  'Litigation',
  'Legislation',
  'Deliberation',
  'Investigation',
  'Implementation',
  'Extrapolation',
  'Procurement',
  'Outsourcing',
  'Diversification',
  'Consolidation',
  'Optimization',
  'Orchestration',
  'Choreography',
  'Composition',
  'Improvisation',
  'Interpretation',
];

// Combined lists for random selection
export const ALL_CATCH_PHRASE_WORDS = [
  ...EASY_CATCH_PHRASE_WORDS,
  ...MEDIUM_CATCH_PHRASE_WORDS,
  ...HARD_CATCH_PHRASE_WORDS,
];

// Get a batch of random words (useful for starting a game with multiple words)
export const getRandomCatchPhraseWords = (count: number = 30): string[] => {
  const wordList = [...ALL_CATCH_PHRASE_WORDS];

  // Shuffle the array using Fisher-Yates algorithm
  for (let i = wordList.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [wordList[i], wordList[j]] = [wordList[j], wordList[i]];
  }

  return wordList.slice(0, count);
};



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/hooks/useConnectionState.ts
================================================
import { useEffect, useCallback } from 'react';
import {
  usePipecatClient,
  usePipecatClientTransportState,
} from '@pipecat-ai/client-react';
import { CONNECTION_STATES } from '@/constants/gameConstants';
import { useConfigurationSettings } from '@/contexts/Configuration';

export function useConnectionState(
  onConnected?: () => void,
  onDisconnected?: () => void
) {
  const client = usePipecatClient();
  const transportState = usePipecatClientTransportState();
  const config = useConfigurationSettings();

  const isConnected = CONNECTION_STATES.ACTIVE.includes(transportState);
  const isConnecting = CONNECTION_STATES.CONNECTING.includes(transportState);
  const isDisconnecting =
    CONNECTION_STATES.DISCONNECTING.includes(transportState);

  // Handle connection changes
  useEffect(() => {
    if (isConnected && onConnected) {
      onConnected();
    }
    if (!isConnected && !isConnecting && onDisconnected) {
      onDisconnected();
    }
  }, [isConnected, isConnecting, onConnected, onDisconnected]);

  // Toggle connection state
  const toggleConnection = useCallback(async () => {
    if (!client) return;

    try {
      if (isConnected) {
        await client.disconnect();
      } else {
        await client.startBotAndConnect({
          endpoint: `api/start`,
          requestData: {
            personality: config.personality,
          },
        });
      }
    } catch (error) {
      console.error('Connection error:', error);
    }
  }, [client, config, isConnected]);

  return {
    isConnected,
    isConnecting,
    isDisconnecting,
    toggleConnection,
    transportState,
    client, // Expose the client for direct access when needed
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/hooks/useGameState.ts
================================================
import { GAME_CONFIG, GAME_STATES, GameState } from "@/constants/gameConstants";
import { getRandomCatchPhraseWords } from "@/data/wordWranglerWords";
import { useCallback, useState } from "react";

export function useGameState() {
  // Game state
  const [gameState, setGameState] = useState<GameState>(GAME_STATES.IDLE);
  const [timeLeft, setTimeLeft] = useState(GAME_CONFIG.GAME_DURATION);
  const [score, setScore] = useState(0);
  const [words, setWords] = useState<string[]>([]);
  const [currentWordIndex, setCurrentWordIndex] = useState(0);
  const [skipsRemaining, setSkipsRemaining] = useState(GAME_CONFIG.MAX_SKIPS);
  const [bestScore, _setBestScore] = useState(0);

  // Initialize or reset game state
  const initializeGame = useCallback(() => {
    const freshWords = getRandomCatchPhraseWords(GAME_CONFIG.WORD_POOL_SIZE);
    setWords(freshWords);
    setGameState(GAME_STATES.ACTIVE);
    setTimeLeft(GAME_CONFIG.GAME_DURATION);
    setScore(0);
    setCurrentWordIndex(0);
    setSkipsRemaining(GAME_CONFIG.MAX_SKIPS);

    // Get best score from local storage
    const storedScore = localStorage.getItem("bestScore");
    if (storedScore) {
      _setBestScore(Number(storedScore) || 0);
    }
    return freshWords;
  }, []);

  // End game
  const finishGame = useCallback(() => {
    setGameState(GAME_STATES.FINISHED);
  }, []);

  // Handle scoring
  const incrementScore = useCallback(() => {
    setScore((prev) => prev + 1);
  }, []);

  // Handle best score
  const setBestScore = useCallback((newBestScore: number) => {
    _setBestScore(newBestScore);
    localStorage.setItem("bestScore", newBestScore.toString());
  }, []);

  // Handle word navigation
  const moveToNextWord = useCallback(() => {
    setCurrentWordIndex((prev) => {
      if (prev >= words.length - 1) {
        // If we're at the end of the word list, get new words
        setWords(getRandomCatchPhraseWords(GAME_CONFIG.WORD_POOL_SIZE));
        return 0;
      }
      return prev + 1;
    });
  }, [words]);

  // Handle skipping
  const useSkip = useCallback(() => {
    if (skipsRemaining <= 0) return false;
    setSkipsRemaining((prev) => prev - 1);
    return true;
  }, [skipsRemaining]);

  // Update timer
  const decrementTimer = useCallback(() => {
    return setTimeLeft((prev) => {
      if (prev <= 1) {
        return 0;
      }
      return prev - 1;
    });
  }, []);

  return {
    // State
    gameState,
    setGameState,
    timeLeft,
    score,
    bestScore,
    words,
    currentWord: words[currentWordIndex] || "",
    skipsRemaining,

    // Actions
    initializeGame,
    finishGame,
    incrementScore,
    setBestScore,
    moveToNextWord,
    useSkip,
    decrementTimer,
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/hooks/useGameTimer.ts
================================================
import { GAME_CONFIG } from "@/constants/gameConstants";
import { clearTimer } from "@/utils/timerUtils";
import { useCallback, useEffect, useRef, useState } from "react";

export function useGameTimer(onTimeUp: () => void) {
  const [timeLeft, setTimeLeft] = useState(GAME_CONFIG.GAME_DURATION);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const hasCalledTimeUpRef = useRef(false);

  // Start the game timer with initial duration
  const startTimer = useCallback(() => {
    // Reset time left and timeUp flag
    setTimeLeft(GAME_CONFIG.GAME_DURATION);
    hasCalledTimeUpRef.current = false;

    // Clear any existing timer
    timerRef.current = clearTimer(timerRef.current);

    // Start a new timer
    timerRef.current = setInterval(() => {
      setTimeLeft((prev) => {
        if (prev <= 1 && !hasCalledTimeUpRef.current) {
          // Time's up - clear the interval and call the callback
          timerRef.current = clearTimer(timerRef.current);
          hasCalledTimeUpRef.current = true;
          onTimeUp();
          return 0;
        }
        return prev - 1;
      });
    }, GAME_CONFIG.TIMER_INTERVAL);
  }, [onTimeUp]);

  // Stop the timer
  const stopTimer = useCallback(() => {
    timerRef.current = clearTimer(timerRef.current);
    hasCalledTimeUpRef.current = false;
  }, []);

  // Reset the timer to initial value without starting it
  const resetTimer = useCallback(() => {
    setTimeLeft(GAME_CONFIG.GAME_DURATION);
    hasCalledTimeUpRef.current = false;
  }, []);

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      timerRef.current = clearTimer(timerRef.current);
    };
  }, []);

  return {
    timeLeft,
    startTimer,
    stopTimer,
    resetTimer,
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/hooks/useVisualFeedback.ts
================================================
import { useState, useRef, useCallback } from 'react';
import { clearTimer } from '@/utils/timerUtils';
import { GAME_CONFIG } from '@/constants/gameConstants';

export function useVisualFeedback() {
  // Visual feedback state
  const [showAutoDetected, setShowAutoDetected] = useState(false);
  const [showIncorrect, setShowIncorrect] = useState(false);
  const autoDetectTimerRef = useRef<NodeJS.Timeout | null>(null);

  // Reset all visual states
  const resetVisuals = useCallback(() => {
    setShowAutoDetected(false);
    setShowIncorrect(false);
    autoDetectTimerRef.current = clearTimer(autoDetectTimerRef.current);
  }, []);

  // Show correct animation
  const showCorrect = useCallback((onComplete?: () => void) => {
    // Clear any existing animation
    autoDetectTimerRef.current = clearTimer(autoDetectTimerRef.current);

    // Show correct animation
    setShowAutoDetected(true);
    setShowIncorrect(false);

    // Set timeout to hide animation
    autoDetectTimerRef.current = setTimeout(() => {
      setShowAutoDetected(false);
      if (onComplete) onComplete();
    }, GAME_CONFIG.ANIMATION_DURATION);
  }, []);

  // Show incorrect animation
  const showIncorrectAnimation = useCallback(() => {
    // Clear any existing animation
    autoDetectTimerRef.current = clearTimer(autoDetectTimerRef.current);

    // Show incorrect animation
    setShowIncorrect(true);
    setShowAutoDetected(false);

    // Set timeout to hide animation
    autoDetectTimerRef.current = setTimeout(() => {
      setShowIncorrect(false);
    }, GAME_CONFIG.ANIMATION_DURATION);
  }, []);

  // Clean up function
  const cleanup = useCallback(() => {
    autoDetectTimerRef.current = clearTimer(autoDetectTimerRef.current);
  }, []);

  return {
    showAutoDetected,
    showIncorrect,
    resetVisuals,
    showCorrect,
    showIncorrectAnimation,
    cleanup,
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/hooks/useWordDetection.ts
================================================
import { useRef } from 'react';
import { useRTVIClientEvent } from '@pipecat-ai/client-react';
import { RTVIEvent } from '@pipecat-ai/client-js';
import { detectWordGuess } from '@/utils/wordDetection';
import { GAME_STATES, GameState } from '@/constants/gameConstants';

interface UseWordDetectionProps {
  gameState: GameState;
  currentWord: string;
  onCorrectGuess: () => void;
  onIncorrectGuess: () => void;
}

export function useWordDetection({
  gameState,
  currentWord,
  onCorrectGuess,
  onIncorrectGuess,
}: UseWordDetectionProps) {
  const lastProcessedMessageRef = useRef('');

  // Reset the last processed message
  const resetLastProcessedMessage = () => {
    lastProcessedMessageRef.current = '';
  };

  // Listen for bot transcripts to detect correct answers
  useRTVIClientEvent(RTVIEvent.BotTranscript, (data) => {
    if (gameState !== GAME_STATES.ACTIVE) {
      return;
    }

    if (!currentWord) {
      return;
    }

    if (!data.text) {
      return;
    }

    // Skip if this is a repeat of the same transcript
    if (data.text === lastProcessedMessageRef.current) {
      return;
    }

    lastProcessedMessageRef.current = data.text;

    // Use the utility function to detect word guesses
    const result = detectWordGuess(data.text, currentWord);

    if (result.isCorrect) {
      onCorrectGuess();
    } else if (result.isExplicitGuess) {
      onIncorrectGuess();
    } else {
    }
  });

  return {
    resetLastProcessedMessage,
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/providers/PipecatProvider.tsx
================================================
'use client';

import { PipecatClient } from '@pipecat-ai/client-js';
import { DailyTransport } from '@pipecat-ai/daily-transport';
import { PipecatClientProvider } from '@pipecat-ai/client-react';
import { PropsWithChildren, useEffect, useState, useRef } from 'react';

export function PipecatProvider({ children }: PropsWithChildren) {
  const [client, setClient] = useState<PipecatClient | null>(null);
  const clientCreated = useRef(false);

  useEffect(() => {
    // Only create the client once
    if (clientCreated.current) return;

    const pcClient = new PipecatClient({
      transport: new DailyTransport(),
      enableMic: true,
      enableCam: false,
    });

    setClient(pcClient);
    clientCreated.current = true;

    // Cleanup when component unmounts
    return () => {
      if (pcClient) {
        pcClient.disconnect().catch((err) => {
          console.error('Error disconnecting client:', err);
        });
      }
      clientCreated.current = false;
    };
  }, []);

  if (!client) {
    return null;
  }

  return (
    <PipecatClientProvider client={client}>{children}</PipecatClientProvider>
  );
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/styles/globals.css
================================================
@import "tailwindcss";

@theme {
  --border-radius-card: 24px;
  --border-width-card: 4px;
  --theme-gradient-start: #fdd256;
  --theme-gradient-end: #a62249;
  --button-height-sm: 52px;
  --button-height: 58px;
  --animate-bounce-in: zoom-bounce 0.75s ease-out forwards;
}

html,
body {
  max-width: 100vw;
  overflow-x: hidden;
  font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen,
    Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif;
}

body {
  height: 100dvh;
  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  background: linear-gradient(180deg, #0059b7 0%, #7dceff 100%);
}

main {
  font-family: var(--font-sans);
  padding: 0 12px;
}

a {
  color: inherit;
  text-decoration: none;
}

button:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

button:not(:disabled):hover {
  opacity: 0.9;
}

button:not(:disabled):active {
  transform: translateY(1px);
}

.card-border {
  position: relative;
  z-index: 1;
}

.card-border:before {
  content: "";
  position: absolute;
  inset: -4px -4px -8px -4px;
  border-radius: 28px;
  background: linear-gradient(
    to bottom,
    rgba(0, 0, 0, 1) 0%,
    rgba(0, 0, 0, 0.15) 100%
  );
  z-index: -1;
}

.card-border:after {
  content: "";
  box-sizing: border-box;
  position: absolute;
  inset: 0;
  background: #ffffff;
  border-radius: var(--border-radius-card);
  border: var(--border-width-card) solid transparent;
  background-image: linear-gradient(#ffffff, #ffffff),
    linear-gradient(
      180deg,
      var(--theme-gradient-start) 0%,
      var(--theme-gradient-end) 100%
    );
  background-origin: border-box;
  background-clip: padding-box, border-box;
}

select {
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  padding: 0 36px 0 12px;
  background-image: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48ZyBjbGlwLXBhdGg9InVybCgjY2xpcDBfNV80MSkiPjxwYXRoIGQ9Ik04IDlMMTIgNUwxNiA5IiBzdHJva2U9IiNCQkQ1RTEiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIi8+PHBhdGggZD0iTTE2IDE1TDEyIDE5TDggMTUiIHN0cm9rZT0iI0JCRDU4MSIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiLz48L2c+PGRlZnM+PGNsaXBQYXRoIGlkPSJjbGlwMF81XzQxIj48cmVjdCB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIGZpbGw9IndoaXRlIi8+PC9jbGlwUGF0aD48L2RlZnM+PC9zdmc+");
  background-repeat: no-repeat;
  background-position: right 12px center;
  background-size: 24px;
  border: 1px solid var(--color-slate-200);
  color: var(--color-slate-600);
}

select::-ms-expand {
  display: none;
}

.styled-button {
  display: flex;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  gap: 12px;
  border-radius: 16px;
  position: relative;
  color: #ffffff;
  text-align: center;
  height: 56px;
  padding: 0 32px;
  background: linear-gradient(
      to bottom,
      transparent 0%,
      transparent 88%,
      rgba(255, 255, 255, 0.2) 88%,
      rgba(255, 255, 255, 0.2) 100%
    ),
    linear-gradient(180deg, #10abe3 0%, #0046b5 98.08%);
  border-width: 2px 2px 5px 2px;
  border-style: solid;
  border-color: #000000;
  overflow: hidden;
  box-shadow: 0px 4px 0px 4px rgba(0, 0, 0, 0.12);
}

.styled-button:before {
  content: "";
  position: absolute;
  inset: 0;
  pointer-events: none;
  border-radius: 12px;
  border: 4px solid transparent;
  background-image: linear-gradient(
    180deg,
    var(--theme-gradient-start) 0%,
    var(--theme-gradient-end) 100%
  );
  background-origin: border-box;
  background-clip: border-box;
  -webkit-mask: linear-gradient(#fff 0 0) padding-box, linear-gradient(#fff 0 0);
  -webkit-mask-composite: xor;
  mask-composite: exclude;
  z-index: 1;
}

.styled-button:after {
  content: "";
  position: absolute;
  inset: 2px;
  border-radius: 12px;
  border: 2px solid #0d1e4c;
  z-index: 2;
}

.styled-button-text {
  color: white;
  font-size: 14px;
  font-weight: 800;
  text-shadow: 2px 2px 0 rgba(0, 0, 0, 0.35);
  text-transform: uppercase;
  letter-spacing: 1px;
  z-index: 2;
  position: relative;
}

.styled-button:active {
  transform: translateY(6px);
  box-shadow: 0 0 0 #000000, inset 0 -2px 12px rgba(0, 0, 0, 0.2),
    inset 0 2px 12px rgba(255, 255, 255, 0.4);
}

.styled-button-icon {
  position: relative;
  box-sizing: border-box;
  display: flex;
  flex-direction: row;
  justify-content: center;
  align-items: center;
  flex-grow: 0;
  padding: 0px;
  width: 30px;
  height: 30px;
  background: rgba(42, 88, 173, 0.25);
  border-radius: 999px;
  z-index: 1;
}
.styled-button-icon:after {
  pointer-events: none;
  content: "";
  position: absolute;
  inset: 0px;
  background: rgba(3, 85, 188, 0.5);
  border-radius: 999px;
  border: 2px solid #0a82d1;
  z-index: -1;
}

@media (min-width: 1024px) {
  .styled-button {
    height: 60px;
  }
  .styled-button-text {
    font-size: 16px;
  }
}

.spinner {
  width: 16px;
  height: 16px;
  border: 2px solid rgba(255, 255, 255, 0.3);
  border-top: 2px solid #ffffff;
  border-radius: 50%;
  animation: spin 1s linear infinite;
}

@keyframes spin {
  0% {
    transform: rotate(0deg);
  }
  100% {
    transform: rotate(360deg);
  }
}

.button {
  appearance: none;
  display: flex;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  gap: 6px;
  cursor: pointer;
  background-color: rgba(0, 0, 0, 0.15);
  color: #ffffff;
  font-weight: 800;
  padding: 0 12px;
  height: var(--button-height-sm);
  border-radius: 999px;
  font-size: 16px;

  &.outline {
    border: 2px solid rgba(255, 255, 255, 0.35);
    background-color: transparent;
    outline: none;
    transition: background-color 0.2s ease-in-out, border-color 0.2s ease-in-out;
  }
  &.outline:hover {
    background-color: rgba(255, 255, 255, 0.1);
    border-color: rgba(255, 255, 255, 0.5);
  }

  &.ghost {
    background-color: var(--color-slate-100);
    color: var(--color-slate-600);
    height: var(--button-height-sm);
  }

  &.ghost:hover {
    background-color: var(--color-slate-200);
  }
}

@media (min-width: 1024px) {
  .button {
    padding: 0 24px;
    gap: 12px;
    height: var(--button-height);
  }
}

/* Animations */

@keyframes zoom-bounce {
  0% {
    transform: scale(0.25);
    animation-timing-function: cubic-bezier(0.8, 0, 1, 1);
  }
  50% {
    transform: scale(1.1);
    animation-timing-function: cubic-bezier(0, 0, 0.2, 1);
  }
  75% {
    transform: scale(0.95);
    animation-timing-function: cubic-bezier(0.8, 0, 1, 1);
  }
  100% {
    transform: scale(1);
  }
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/styles/HomeStyles.ts
================================================
export const styles = {
  main: {
    display: "flex",
    flexDirection: "column" as const,
    justifyContent: "flex-start",
    alignItems: "center",
    minHeight: "100vh",
    padding: "2rem 0",
  },
  container: {
    width: "100%",
    maxWidth: "800px",
    padding: "0 1rem",
  },
  title: {
    fontSize: "2rem",
    textAlign: "center" as const,
    marginBottom: "2rem",
    color: "#333",
  },
  gameContainer: {
    marginBottom: "2rem",
  },
  controlsContainer: {
    display: "flex",
    flexDirection: "column" as const,
    gap: "1rem",
    marginBottom: "2rem",
  },
  settings: {
    backgroundColor: "white",
    padding: "1rem",
    borderRadius: "8px",
    boxShadow: "0 1px 3px rgba(0,0,0,0.1)",
  },
  label: {
    display: "flex",
    flexDirection: "column" as const,
    gap: "0.5rem",
    fontSize: "0.9rem",
    color: "#555",
  },
  select: {
    padding: "0.5rem",
    border: "1px solid #ddd",
    borderRadius: "4px",
    fontSize: "1rem",
  },
  instructions: {
    backgroundColor: "white",
    padding: "1.5rem",
    borderRadius: "8px",
    boxShadow: "0 1px 3px rgba(0,0,0,0.1)",
  },
  instructionsTitle: {
    fontSize: "1.4rem",
    marginBottom: "1rem",
    color: "#333",
  },
  instructionsList: {
    paddingLeft: "1.5rem",
    lineHeight: 1.6,
  },
};



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/types/personality.ts
================================================
export type PersonalityType =
  | 'friendly'
  | 'professional'
  | 'enthusiastic'
  | 'thoughtful'
  | 'witty';

// This object can be useful for displaying user-friendly labels or descriptions
export const PERSONALITY_PRESETS: Record<PersonalityType, string> = {
  friendly: 'Friendly',
  professional: 'Professional',
  enthusiastic: 'Enthusiastic',
  thoughtful: 'Thoughtful',
  witty: 'Witty',
};

// Default personality to use
export const DEFAULT_PERSONALITY: PersonalityType = 'witty';



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/utils/formatTime.ts
================================================
/**
 * Formats seconds into MM:SS format
 */
export function formatTime(seconds: number): string {
  const mins = Math.floor(seconds / 60);
  const secs = seconds % 60;
  return `${mins}:${secs < 10 ? '0' : ''}${secs}`;
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/utils/timerUtils.ts
================================================
/**
 * Safely clears any type of timer
 * @param timer The timer to clear
 * @returns null to reassign to the timer reference
 */
export function clearTimer(timer: NodeJS.Timeout | null): null {
  if (timer) {
    clearTimeout(timer);
  }
  return null;
}

/**
 * Creates a countdown timer that calls the callback every second
 * @returns A function to stop the timer
 */
export function createCountdownTimer(
  durationSeconds: number,
  onTick: (secondsLeft: number) => void,
  onComplete: () => void
): () => void {
  let secondsLeft = durationSeconds;

  const timer = setInterval(() => {
    secondsLeft--;
    onTick(secondsLeft);

    if (secondsLeft <= 0) {
      clearInterval(timer);
      onComplete();
    }
  }, 1000);

  return () => {
    clearInterval(timer);
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/client/src/utils/wordDetection.ts
================================================
import { TRANSCRIPT_PATTERNS } from '@/constants/gameConstants';

/**
 * Checks if a transcript contains a correct guess for the target word
 */
export function detectWordGuess(transcript: string, targetWord: string) {
  const currentWordLower = targetWord.toLowerCase().trim();

  // Primary detection: Look for explicit guesses
  const guessPattern = TRANSCRIPT_PATTERNS.GUESS_PATTERN;
  const guessMatch = transcript.match(guessPattern);

  if (guessMatch) {
    // Extract the guessed word from whichever group matched (group 1 or 2)
    let guessedWord = (guessMatch[1] || guessMatch[2] || '')
      .toLowerCase()
      .trim();

    // Remove articles ("a", "an", "the") from the beginning of the guessed word
    guessedWord = guessedWord.replace(/^(a|an|the)\s+/i, '');

    return {
      isCorrect: guessedWord === currentWordLower,
      isExplicitGuess: true,
      guessedWord,
    };
  }

  // Secondary detection: Check if word appears in transcript
  const containsWord = transcript.toLowerCase().includes(currentWordLower);

  return {
    isCorrect: containsWord,
    isExplicitGuess: false,
    guessedWord: containsWord ? targetWord : null,
  };
}



================================================
FILE: word-wrangler-gemini-live/web-game/server/bot.py
================================================
#
# Copyright (c) 2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#


import os

from dotenv import load_dotenv
from loguru import logger
from pipecat.audio.turn.smart_turn.base_smart_turn import SmartTurnParams
from pipecat.audio.turn.smart_turn.local_smart_turn_v3 import LocalSmartTurnAnalyzerV3
from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.audio.vad.vad_analyzer import VADParams
from pipecat.frames.frames import LLMRunFrame
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.llm_context import LLMContext
from pipecat.processors.aggregators.llm_response_universal import LLMContextAggregatorPair
from pipecat.processors.filters.stt_mute_filter import STTMuteConfig, STTMuteFilter, STTMuteStrategy
from pipecat.processors.frameworks.rtvi import (
    RTVIConfig,
    RTVIObserver,
    RTVIProcessor,
)
from pipecat.runner.types import RunnerArguments
from pipecat.runner.utils import create_transport
from pipecat.services.google.gemini_live.llm import GeminiLiveLLMService
from pipecat.transports.daily.transport import DailyParams, DailyTransport

load_dotenv(override=True)


# Define conversation modes with their respective prompt templates
game_prompt = """You are the AI host and player for a game of Word Wrangler.

GAME RULES:
1. The user will be given a word or phrase that they must describe to you
2. The user CANNOT say any part of the word/phrase directly
3. You must try to guess the word/phrase based on the user's description
4. Once you guess correctly, the user will move on to their next word
5. The user is trying to get through as many words as possible in 60 seconds
6. The external application will handle timing and keeping score

YOUR ROLE:
1. Start with this exact brief introduction: "Welcome to Word Wrangler! I'll try to guess the words you describe. Remember, don't say any part of the word itself. Ready? Let's go!"
2. Listen carefully to the user's descriptions
3. Make intelligent guesses based on what they say
4. When you think you know the answer, state it clearly: "Is it [your guess]?"
5. If you're struggling, ask for more specific clues
6. Keep the game moving quickly - make guesses promptly
7. Be enthusiastic and encouraging

IMPORTANT:
- Keep all responses brief - the game is timed!
- Make multiple guesses if needed
- Use your common knowledge to make educated guesses
- If the user indicates you got it right, just say "Got it!" and prepare for the next word
- If you've made several wrong guesses, simply ask for "Another clue please?"

Start with the exact introduction specified above, then wait for the user to begin describing their first word."""

# Define personality presets
PERSONALITY_PRESETS = {
    "friendly": "You have a warm, approachable personality. You use conversational language, occasional humor, and express enthusiasm for the topic. Make the user feel comfortable and engaged.",
    "professional": "You have a formal, precise personality. You communicate clearly and directly with a focus on accuracy and relevance. Your tone is respectful and business-like.",
    "enthusiastic": "You have an energetic, passionate personality. You express excitement about the topic and use dynamic language. You're encouraging and positive throughout the conversation.",
    "thoughtful": "You have a reflective, philosophical personality. You speak carefully, considering multiple angles of each point. You ask thought-provoking questions and acknowledge nuance.",
    "witty": "You have a clever, humorous personality. While remaining informative, you inject appropriate wit and playful language. Your goal is to be engaging and entertaining while still being helpful.",
}


async def run_bot(transport: DailyTransport, runner_args: RunnerArguments):
    # Use the provided session logger if available, otherwise use the default logger
    config = runner_args.body
    logger.debug("Configuration: {}", config)

    # Extract configuration parameters with defaults
    personality = config.get("personality", "witty")

    personality_prompt = PERSONALITY_PRESETS.get(personality, PERSONALITY_PRESETS["friendly"])

    system_instruction = f"""{game_prompt}

{personality_prompt}

Important guidelines:
1. Your responses will be converted to speech, so keep them concise and conversational.
2. Don't use special characters or formatting that wouldn't be natural in speech.
3. Encourage the user to elaborate when appropriate."""

    intro_message = """Start with this exact brief introduction: "Welcome to Word Wrangler! I'll try to guess the words you describe. Remember, don't say any part of the word itself. Ready? Let's go!"""

    # Create the STT mute filter if we have strategies to apply
    stt_mute_filter = STTMuteFilter(
        config=STTMuteConfig(strategies={STTMuteStrategy.MUTE_UNTIL_FIRST_BOT_COMPLETE})
    )

    llm = GeminiLiveLLMService(
        api_key=os.getenv("GOOGLE_API_KEY"),
        system_instruction=system_instruction,
    )

    # Set up the initial context for the conversation
    messages = [
        {
            "role": "user",
            "content": intro_message,
        },
    ]

    # This sets up the LLM context by providing messages and tools
    context = LLMContext(messages)
    context_aggregator = LLMContextAggregatorPair(context)

    # RTVI events for Pipecat client UI
    rtvi = RTVIProcessor(config=RTVIConfig(config=[]))

    pipeline = Pipeline(
        [
            transport.input(),
            rtvi,
            stt_mute_filter,
            context_aggregator.user(),
            llm,
            transport.output(),
            context_aggregator.assistant(),
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            enable_metrics=True,
            enable_usage_metrics=True,
        ),
        observers=[RTVIObserver(rtvi)],
    )

    @rtvi.event_handler("on_client_ready")
    async def on_client_ready(rtvi):
        logger.debug("Client ready event received")
        await rtvi.set_bot_ready()
        # Kick off the conversation
        await task.queue_frames([LLMRunFrame()])

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=runner_args.handle_sigint)

    await runner.run(task)


async def bot(runner_args: RunnerArguments):
    """Main bot entry point compatible with the FastAPI route handler."""
    if os.environ.get("ENV") != "local":
        from pipecat.audio.filters.krisp_filter import KrispFilter

        krisp_filter = KrispFilter()
    else:
        krisp_filter = None

    # We store functions so objects (e.g. SileroVADAnalyzer) don't get
    # instantiated. The function will be called when the desired transport gets
    # selected.
    transport_params = {
        "daily": lambda: DailyParams(
            audio_in_enabled=True,
            audio_in_filter=krisp_filter,
            audio_out_enabled=True,
            vad_analyzer=SileroVADAnalyzer(params=VADParams(stop_secs=0.2)),
            turn_analyzer=LocalSmartTurnAnalyzerV3(params=SmartTurnParams()),
        )
    }

    transport = await create_transport(runner_args, transport_params)

    await run_bot(transport, runner_args)


if __name__ == "__main__":
    from pipecat.runner.run import main

    main()



================================================
FILE: word-wrangler-gemini-live/web-game/server/Dockerfile
================================================
FROM dailyco/pipecat-base:latest

# Enable bytecode compilation
ENV UV_COMPILE_BYTECODE=1

# Copy from the cache instead of linking since it's a mounted volume
ENV UV_LINK_MODE=copy

# Install the project's dependencies using the lockfile and settings
RUN --mount=type=cache,target=/root/.cache/uv \
    --mount=type=bind,source=uv.lock,target=uv.lock \
    --mount=type=bind,source=pyproject.toml,target=pyproject.toml \
    uv sync --locked --no-install-project --no-dev

COPY ./bot.py bot.py



================================================
FILE: word-wrangler-gemini-live/web-game/server/env.example
================================================
DAILY_API_KEY=
DAILY_API_URL=https://api.daily.co/v1/
DAILY_SAMPLE_ROOM_URL=
GOOGLE_API_KEY=


================================================
FILE: word-wrangler-gemini-live/web-game/server/pcc-deploy.toml
================================================
agent_name = "word-wrangler"
image = "your_username/word-wrangler:0.1"
image_credentials = "your_dockerhub_pull_secret"
secret_set = "word-wrangler-secrets"
enable_krisp = true
agent_profile = "agent-1x"

[scaling]
	min_agents = 1



================================================
FILE: word-wrangler-gemini-live/web-game/server/pyproject.toml
================================================
[project]
name = "word-wrangler"
version = "0.1.0"
description = "Word Wrangler guessing game powered by Gemini"
requires-python = ">=3.10"
dependencies = [
    "pipecat-ai[daily,silero,google,local-smart-turn-v3,runner]>=0.0.93",
    "pipecatcloud>=0.2.6"
]

[dependency-groups]
dev = [
    "ruff~=0.12.1",
]

[tool.ruff]
line-length = 100
[tool.ruff.lint]
select = ["I"]


================================================
FILE: .github/workflows/android.yaml
================================================
name: android

on:
  push:
    branches:
      - main
    paths:
      - 'simple-chatbot/client/android/**'
      - 'p2p-webrtc/video-transform/client/android/**'
  pull_request:
    branches:
      - '**'
    paths:
      - 'simple-chatbot/client/android/**'
      - 'p2p-webrtc/video-transform/client/android/**'
  workflow_dispatch:
    inputs:
      sdk_git_ref:
        type: string
        description: 'Which git ref of the app to build'

concurrency:
  group: build-android-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  sdk:
    name: 'Demo apps'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.sdk_git_ref || github.ref }}

      - name: 'Install Java'
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: 'Example app: Simple Chatbot'
        working-directory: simple-chatbot/client/android
        run: ./gradlew :simple-chatbot-client:assembleDebug

      - name: Upload Simple Chatbot APK
        uses: actions/upload-artifact@v4
        with:
          name: Simple Chatbot Android Client
          path: simple-chatbot/client/android/simple-chatbot-client/build/outputs/apk/debug/simple-chatbot-client-debug.apk

      - name: 'Example app: Small WebRTC Client'
        working-directory: p2p-webrtc/video-transform/client/android
        run: ./gradlew :small-webrtc-client:assembleDebug

      - name: Upload Small WebRTC APK
        uses: actions/upload-artifact@v4
        with:
          name: Small WebRTC Android Client
          path: p2p-webrtc/video-transform/client/android/small-webrtc-client/build/outputs/apk/debug/small-webrtc-client-debug.apk



================================================
FILE: .github/workflows/format.yaml
================================================
name: format

on:
  workflow_dispatch:
  push:
    branches:
      - main
  pull_request:
    branches:
      - '**'

concurrency:
  group: format-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:
  ruff-format:
    name: 'Code quality checks'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: 'latest'

      - name: Set up Python
        run: uv python install 3.10

      - name: Install development dependencies
        run: uv sync --group dev

      - name: Ruff formatter check
        id: ruff-format
        run: |
          uv run ruff format --diff --check .

      - name: Ruff linter check
        id: ruff-check
        run: |
          uv run ruff check .


