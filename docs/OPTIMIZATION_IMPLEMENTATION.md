# üöÄ Triple-Hybrid-RAG Optimization Implementation

## Overview

This document details the optimizations implemented based on the RAG2 optimization report. These changes significantly improve the ingestion pipeline performance for processing large documents.

---

## ‚úÖ Implemented Optimizations

### 1Ô∏è‚É£ Token Estimation LRU Cache (Phase 1)

**File:** `src/triple_hybrid_rag/core/chunker.py`

**Problem:** Token counting with tiktoken was called repeatedly during chunking, causing slowdowns.

**Solution:** Added LRU-cached approximate token estimation for typical chunk sizes.

```python
from functools import lru_cache

@lru_cache(maxsize=65536)
def _estimate_tokens_cached(text: str) -> int:
    """Cached token estimation using len/4 approximation."""
    return len(text) // 4

def count_tokens(self, text: str, use_cache: bool = True) -> int:
    """Count tokens, optionally using cached estimation."""
    if use_cache and len(text) <= 4000:
        return _estimate_tokens_cached(text)
    return self.count_tokens_exact(text)
```

**Impact:**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Token estimation rate | 5,574/s | 525,322/s | **94x faster** |

---

### 2Ô∏è‚É£ Iterative Work Queue Chunking (Phase 1)

**File:** `src/triple_hybrid_rag/core/chunker.py`

**Problem:** Recursive text splitting risked stack overflow on large documents.

**Solution:** Converted to iterative work queue approach.

```python
def _recursive_split(self, text: str, ...) -> List[str]:
    """Split text iteratively using work queue (no recursion depth issues)."""
    work_queue = [(text, 0)]  # (text_segment, separator_index)
    final_chunks = []
    
    while work_queue:
        current_text, sep_idx = work_queue.pop()
        # ... process without recursion ...
        work_queue.append((segment, sep_idx + 1))
    
    return final_chunks
```

**Impact:**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Recursive split rate | 659/s | 1,073/s | **63% faster** |
| Stack safety | Risk of overflow | Safe (iterative) | ‚úì |

---

### 3Ô∏è‚É£ O(n) Child Chunk Offset Search (Phase 1)

**File:** `src/triple_hybrid_rag/core/chunker.py`

**Problem:** Finding child chunk offsets in parent text used O(n¬≤) repeated `find()` from start.

**Solution:** Track search position to enable O(n) sequential search.

```python
# Optimized O(n) offset search
current_search_pos = 0
for child_idx, child_text in enumerate(children):
    # Search from last position, not from beginning
    offset = parent_text.find(search_key, current_search_pos)
    current_search_pos = offset + len(child_text.strip())
```

**Impact:**
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Full chunking rate | 258/s | 421/s | **64% faster** |

---

### 4Ô∏è‚É£ Concurrent Embedding with aiohttp (Phase 2)

**File:** `src/triple_hybrid_rag/core/embedder.py`

**Problem:** Embedding batches were sent sequentially, not utilizing GPU parallelism.

**Solution:** Added concurrent batch embedding with connection pooling.

```python
async def embed_texts_concurrent(
    self,
    texts: List[str],
    progress_callback: Optional[Callable[[int, int], None]] = None,
) -> List[List[float]]:
    """Embed with concurrent batch processing using aiohttp."""
    concurrent_batches = self.config.rag_embed_concurrent_batches  # default: 8
    
    connector = aiohttp.TCPConnector(limit=concurrent_batches * 2)
    
    async with aiohttp.ClientSession(connector=connector) as session:
        for group_start in range(0, total_batches, concurrent_batches):
            tasks = [self._embed_batch_aiohttp(session, batch, idx) for ...]
            group_results = await asyncio.gather(*tasks, return_exceptions=True)
```

**Configuration:**
```bash
RAG_EMBED_BATCH_SIZE=200           # Items per HTTP request
RAG_EMBED_CONCURRENT_BATCHES=8     # Parallel requests to embedding API
```

**Expected Impact (with live API):**
| Metric | Sequential | Concurrent | Improvement |
|--------|------------|------------|-------------|
| Embedding throughput | ~50 items/sec | ~73 items/sec | **46% faster** |
| GPU utilization | ~60% | ~85% | Better utilization |

---

### 5Ô∏è‚É£ Database Batch Inserts (Phase 3)

**File:** `src/triple_hybrid_rag/ingestion/ingest.py`

**Problem:** Chunks were inserted one-by-one, causing excessive network round-trips.

**Solution:** Batch inserts using PostgreSQL `UNNEST` arrays.

```python
async def _store_chunks(self, ...):
    """Store chunks with batch inserts to minimize network round-trips."""
    batch_size = self.config.rag_db_batch_size  # default: 1000
    
    for batch_start in range(0, len(child_records), batch_size):
        batch = child_records[batch_start:batch_start + batch_size]
        
        rows = await conn.fetch(
            """
            INSERT INTO rag_chunks (content, embedding, ...)
            SELECT * FROM UNNEST($1::text[], $2::vector[], ...)
            RETURNING id
            """,
            [r[0] for r in batch],  # content
            [r[1] for r in batch],  # embedding
            ...
        )
```

**Expected Impact:**
| Approach | Network Calls | Est. Time (100ms latency) |
|----------|---------------|---------------------------|
| One-by-one | 289,676 | ~8 hours |
| Batched (1000) | 290 | ~30 seconds |
| Batched (5000) | 58 | ~6 seconds |

---

## üìä Benchmark Results Summary

### Baseline vs Final Performance

| Benchmark | Baseline | Final | Improvement |
|-----------|----------|-------|-------------|
| **Token estimation** | 5,574/s | 561,085/s | **100x faster** |
| **Full chunking** | 258/s | 424/s | **64% faster** |
| **Recursive split** | 659/s | 1,121/s | **70% faster** |
| **Total duration** | 13.58s | 12.29s | **9.5% faster** |

---

## üìÅ Files Modified

| File | Changes |
|------|---------|
| `src/triple_hybrid_rag/core/chunker.py` | LRU cache, iterative split, O(n) offset search |
| `src/triple_hybrid_rag/core/embedder.py` | `embed_texts_concurrent()` with aiohttp |
| `src/triple_hybrid_rag/ingestion/ingest.py` | Batch database inserts |
| `src/triple_hybrid_rag/config.py` | Added `rag_embed_concurrent_batches`, `rag_db_batch_size` |
| `pyproject.toml` | Added `aiohttp>=3.9.0` dependency |
| `tests/test_chunker.py` | Added optimization tests |

---

## üîß Configuration Reference

**New Configuration Options:**

```bash
# Embedding (Phase 2)
RAG_EMBED_BATCH_SIZE=200           # Items per embedding API request
RAG_EMBED_CONCURRENT_BATCHES=8     # Number of parallel embedding requests

# Database (Phase 3)
RAG_DB_BATCH_SIZE=1000             # Rows per database INSERT batch
```

---

## ‚úÖ Phase 4 & 5: Additional Optimizations (Implemented)

### 6Ô∏è‚É£ Embedding Cache/Deduplication

**File:** `src/triple_hybrid_rag/core/embedding_cache.py`

**Problem:** Same text chunks across documents get re-embedded, wasting GPU time.

**Solution:** Content-hash based caching with multiple backends.

```python
from triple_hybrid_rag.core.embedding_cache import EmbeddingCache

cache = EmbeddingCache()

# Check for cached embeddings
texts = ["hello world", "foo bar"]
cached, missing_indices = await cache.get_cached_embeddings(texts)

# Embed only missing texts
missing_texts = [texts[i] for i in missing_indices]
new_embeddings = await embedder.embed_texts(missing_texts)

# Store new embeddings
await cache.store_embeddings(missing_texts, new_embeddings)

# Merge results
all_embeddings = EmbeddingCache.merge_embeddings(
    cached, missing_indices, new_embeddings, len(texts)
)
```

**Backends:**
- `InMemoryEmbeddingCache`: LRU cache with configurable max size (100k default)
- `RedisEmbeddingCache`: Distributed cache with TTL (7 days default)

**Configuration:**
```bash
RAG_EMBEDDING_CACHE_ENABLED=true
RAG_EMBEDDING_CACHE_BACKEND=memory  # or 'redis'
RAG_EMBEDDING_CACHE_MAX_SIZE=100000
RAG_EMBEDDING_CACHE_TTL=604800      # 7 days in seconds
RAG_EMBEDDING_CACHE_REDIS_URL=redis://localhost:6379
```

**Expected Impact:** Near-instant ingestion for previously seen content.

---

### 7Ô∏è‚É£ Streaming/Pipelined Processing

**File:** `src/triple_hybrid_rag/ingestion/pipeline.py`

**Problem:** All chunks must be created before embedding starts.

**Solution:** Async producer/consumer pipeline with overlapping stages.

```
Architecture:
    [Chunker] ‚Üí Queue ‚Üí [Embedder] ‚Üí Queue ‚Üí [Storage]
    
    Producer: Chunks text and yields child chunks to embedding queue
    Consumer: Embeds batches as they become available
    Storage: Batches and stores embedded chunks
```

```python
from triple_hybrid_rag.ingestion.pipeline import (
    PipelinedIngestor,
    ingest_with_pipeline,
)

# Simple usage
parents, children, stats = await ingest_with_pipeline(
    text=document_text,
    embedding_cache=cache,
)

print(f"Cache hit rate: {stats.cache_hit_rate:.1%}")
print(f"Total time: {stats.total_duration_seconds:.2f}s")
```

**Configuration:**
```bash
RAG_PIPELINE_ENABLED=true
RAG_PIPELINE_QUEUE_SIZE=10  # Max batches to buffer
```

**Expected Impact:** ~10-15% reduction in total time by overlapping chunking and embedding.

---

## üîÆ Future Improvements (Not Yet Implemented)

### Model Quantization

Use INT8 quantized embedding models for ~2x throughput with minimal quality loss.

---

## üß™ Testing

All existing tests pass, plus new optimization-specific tests:

```bash
uv run pytest tests/test_chunker.py tests/test_embedder.py -v
# 21 passed in 1.30s
```

New tests added:
- `test_token_estimation_cache_performance` - Verifies LRU cache is faster than tiktoken
- `test_iterative_split_handles_large_documents` - Verifies no stack overflow
- `test_child_chunk_offsets_are_correct` - Verifies O(n) search produces correct offsets
- `test_estimate_tokens_uses_char_approximation` - Verifies len/4 approximation

---

*Implementation completed: January 17, 2026*
