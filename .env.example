# ═══════════════════════════════════════════════════════════════════════════════
# Triple-Hybrid-RAG Configuration
# Copy this file to .env and customize for your environment
# ═══════════════════════════════════════════════════════════════════════════════

# ───────────────────────────────────────────────────────────────────────────────
# DATABASE
# ───────────────────────────────────────────────────────────────────────────────
# PostgreSQL connection URL (with pgvector extension)
DATABASE_URL=postgresql://postgres:postgres@localhost:54332/rag_db
DATABASE_POOL_SIZE=10
DATABASE_MAX_OVERFLOW=20

# ───────────────────────────────────────────────────────────────────────────────
# OPENAI API (for query planner, entity extraction, etc.)
# ───────────────────────────────────────────────────────────────────────────────
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4

# ───────────────────────────────────────────────────────────────────────────────
# JINA AI API (for embeddings and reranking)
# Get your API key from: https://jina.ai/?sui=apikey
# ───────────────────────────────────────────────────────────────────────────────
JINA_API_KEY=jina_your-api-key-here
JINA_API_BASE=https://api.jina.ai/v1

# Jina Embedding (jina-embeddings-v4 - multimodal, multilingual)
JINA_EMBED_MODEL=jina-embeddings-v4
JINA_EMBED_DIMENSIONS=1024
JINA_EMBED_TASK_QUERY=retrieval.query
JINA_EMBED_TASK_PASSAGE=retrieval.passage
JINA_EMBED_BATCH_SIZE=100
JINA_EMBED_TIMEOUT=60.0

# Jina Reranker (jina-reranker-v3 - listwise reranking)
JINA_RERANK_MODEL=jina-reranker-v3
JINA_RERANK_TOP_N=20
JINA_RERANK_TIMEOUT=30.0

# Provider Selection: 'jina' (cloud API) or 'local' (Qwen3-VL via vLLM/Ollama)
RAG_EMBED_PROVIDER=jina
RAG_RERANK_PROVIDER=jina

# Image Ingestion Mode: 'ocr' (extract text), 'direct' (embed image), 'auto' (system decides)
# - ocr: Best for scanned documents, PDFs with text
# - direct: Best for photos, diagrams, charts (uses multimodal embeddings)
# - auto: Analyzes image content and chooses the optimal method
RAG_IMAGE_INGESTION_MODE=auto

# ───────────────────────────────────────────────────────────────────────────────
# EMBEDDING API
# Uses OpenAI-compatible API (vLLM, Ollama, LM Studio, etc.)
# ───────────────────────────────────────────────────────────────────────────────
RAG_EMBED_API_BASE=http://127.0.0.1:1234/v1
RAG_EMBED_MODEL=qwen3-vl-embedding-2b
RAG_EMBED_BATCH_SIZE=200
RAG_EMBED_CONCURRENT_BATCHES=8
RAG_EMBED_TIMEOUT=60.0

# Embedding dimensions (Matryoshka)
RAG_EMBED_DIM_MODEL=2048
RAG_EMBED_DIM_STORE=1024

# ───────────────────────────────────────────────────────────────────────────────
# RERANKING API
# ───────────────────────────────────────────────────────────────────────────────
RAG_RERANK_API_BASE=http://127.0.0.1:1234/v1
RAG_RERANK_MODEL=qwen3-vl-reranker-2b
RAG_RERANK_TIMEOUT=30.0

# ───────────────────────────────────────────────────────────────────────────────
# OCR API (Vision Language Model)
# ───────────────────────────────────────────────────────────────────────────────
RAG_OCR_API_BASE=http://127.0.0.1:1234/v1
RAG_OCR_MODEL=qwen3-vl-8b
RAG_OCR_TIMEOUT=60.0

# DeepSeek OCR (alternative)
RAG_DEEPSEEK_OCR_API_BASE=http://127.0.0.1:8000/v1
RAG_DEEPSEEK_OCR_MODEL=deepseek-ocr

# OCR Ingestion Mode: 'qwen', 'deepseek', 'off', or 'auto'
# - qwen: Use Qwen3-VL for OCR processing
# - deepseek: Use DeepSeek OCR for OCR processing
# - off: Skip OCR, only extract native text
# - auto: System decides based on file analysis (text density, scanned pages, etc.)
RAG_OCR_MODE=auto
RAG_OCR_AUTO_PREFERRED=qwen
RAG_OCR_AUTO_THRESHOLD=0.3

# ───────────────────────────────────────────────────────────────────────────────
# PUPPYGRAPH (Graph Database)
# ───────────────────────────────────────────────────────────────────────────────
PUPPYGRAPH_BOLT_URL=bolt://localhost:7697
PUPPYGRAPH_HTTP_URL=http://localhost:8192
PUPPYGRAPH_WEB_UI_URL=http://localhost:8091
PUPPYGRAPH_USERNAME=admin
PUPPYGRAPH_PASSWORD=puppygraph123
PUPPYGRAPH_TIMEOUT=30.0

# ───────────────────────────────────────────────────────────────────────────────
# FEATURE FLAGS
# ───────────────────────────────────────────────────────────────────────────────
# Master switch
RAG_ENABLED=true

# Retrieval channels
RAG_LEXICAL_ENABLED=true
RAG_SEMANTIC_ENABLED=true
RAG_GRAPH_ENABLED=true

# Processing features
RAG_RERANK_ENABLED=true
RAG_DENOISE_ENABLED=true
RAG_QUERY_PLANNER_ENABLED=true

# Ingestion features
RAG_ENTITY_EXTRACTION_ENABLED=true
RAG_OCR_ENABLED=true
RAG_DEEPSEEK_OCR_ENABLED=false
RAG_MULTIMODAL_EMBEDDING_ENABLED=true

# ───────────────────────────────────────────────────────────────────────────────
# INGESTION SETTINGS
# ───────────────────────────────────────────────────────────────────────────────
# Retry settings for transient failures
RAG_INGEST_EMBED_RETRY_ATTEMPTS=3
RAG_INGEST_DB_RETRY_ATTEMPTS=3
RAG_INGEST_RETRY_BACKOFF_MIN=1.0
RAG_INGEST_RETRY_BACKOFF_MAX=10.0

# Database batch insert size (reduces network round-trips)
RAG_DB_BATCH_SIZE=1000

# ───────────────────────────────────────────────────────────────────────────────
# EMBEDDING CACHE
# Caches embeddings to avoid re-embedding identical content
# ───────────────────────────────────────────────────────────────────────────────
RAG_EMBEDDING_CACHE_ENABLED=true

# Backend: 'memory' (in-process LRU) or 'redis' (distributed)
RAG_EMBEDDING_CACHE_BACKEND=memory

# Memory backend settings
RAG_EMBEDDING_CACHE_MAX_SIZE=100000

# Redis backend settings
RAG_EMBEDDING_CACHE_TTL=604800
RAG_EMBEDDING_CACHE_REDIS_URL=redis://localhost:6379

# ───────────────────────────────────────────────────────────────────────────────
# PIPELINED INGESTION
# Overlaps chunking and embedding for faster processing
# ───────────────────────────────────────────────────────────────────────────────
RAG_PIPELINE_ENABLED=true
RAG_PIPELINE_QUEUE_SIZE=10

# ───────────────────────────────────────────────────────────────────────────────
# BATCH INFERENCE (OpenAI Batch API)
# Uses batch API for NER during ingestion - 50% cheaper but 24h completion window
# Ideal for large-scale ingestion where real-time processing is not required
# ───────────────────────────────────────────────────────────────────────────────
RAG_BATCH_INFERENCE_ENABLED=false
RAG_BATCH_INFERENCE_THRESHOLD=10
RAG_BATCH_INFERENCE_CHECK_INTERVAL=60
RAG_BATCH_INFERENCE_MAX_WAIT=86400

# ───────────────────────────────────────────────────────────────────────────────
# CHUNKING CONFIGURATION
# ───────────────────────────────────────────────────────────────────────────────
RAG_PARENT_CHILD_CHUNKING=true
RAG_MATRYOSHKA_EMBEDDINGS=true

# Chunk sizes in tokens
RAG_PARENT_CHUNK_TOKENS=800
RAG_PARENT_CHUNK_MAX_TOKENS=1000
RAG_CHILD_CHUNK_TOKENS=200
RAG_CHUNK_OVERLAP_TOKENS=50

# ───────────────────────────────────────────────────────────────────────────────
# RETRIEVAL CONFIGURATION
# ───────────────────────────────────────────────────────────────────────────────
# RRF weights for each channel
RAG_LEXICAL_WEIGHT=0.7
RAG_SEMANTIC_WEIGHT=0.8
RAG_GRAPH_WEIGHT=1.0

# Safety & Denoising
RAG_SAFETY_THRESHOLD=0.6
RAG_DENOISE_ALPHA=0.6

# Top-K settings for each stage
RAG_LEXICAL_TOP_K=50
RAG_SEMANTIC_TOP_K=100
RAG_GRAPH_TOP_K=50
RAG_RERANK_TOP_K=20
RAG_FINAL_TOP_K=5

# ───────────────────────────────────────────────────────────────────────────────
# QUERY PLANNER
# ───────────────────────────────────────────────────────────────────────────────
RAG_QUERY_PLANNER_MODEL=gpt-4
RAG_QUERY_PLANNER_TEMPERATURE=0.0

# ───────────────────────────────────────────────────────────────────────────────
# ENTITY EXTRACTION
# ───────────────────────────────────────────────────────────────────────────────
RAG_NER_MODEL=gpt-4
RAG_NER_TEMPERATURE=0.0
RAG_NER_MAX_CHUNKS_PER_REQUEST=8
RAG_NER_MAX_CHARS_PER_CHUNK=2000
RAG_ENTITY_TYPES=PERSON,ORGANIZATION,PRODUCT,CLAUSE,DATE,MONEY,LOCATION,TECHNICAL_TERM

# Reasoning effort for gpt-5-nano/o1/o3 models: 'low' (fast), 'medium', 'high'
# 'low' is ~50% faster and cheaper while still producing good results
RAG_NER_REASONING_EFFORT=low

# ───────────────────────────────────────────────────────────────────────────────
# GUNDAM TILING OCR
# Splits large images into tiles for better OCR accuracy
# ───────────────────────────────────────────────────────────────────────────────
RAG_GUNDAM_TILING_ENABLED=true
RAG_GUNDAM_MIN_IMAGE_SIZE=1500
RAG_GUNDAM_TILE_SIZE=1024
RAG_GUNDAM_OVERLAP=128
RAG_GUNDAM_MAX_TILES=16
RAG_GUNDAM_MERGE_STRATEGY=fuzzy
RAG_GUNDAM_FUZZY_THRESHOLD=0.85

# ───────────────────────────────────────────────────────────────────────────────
# HYDE (HYPOTHETICAL DOCUMENT EMBEDDINGS)
# Generates hypothetical documents to bridge semantic gap
# ───────────────────────────────────────────────────────────────────────────────
RAG_HYDE_ENABLED=true
RAG_HYDE_MODEL=gpt-4o-mini
RAG_HYDE_TEMPERATURE=0.7
RAG_HYDE_NUM_HYPOTHETICALS=1
RAG_HYDE_CACHE_ENABLED=true
RAG_HYDE_USE_INTENT_PROMPTS=true
RAG_HYDE_FALLBACK_TO_ORIGINAL=true

# ───────────────────────────────────────────────────────────────────────────────
# QUERY EXPANSION
# Multi-query generation and RAG-Fusion
# ───────────────────────────────────────────────────────────────────────────────
RAG_QUERY_EXPANSION_ENABLED=true
RAG_QUERY_EXPANSION_NUM_VARIANTS=3
RAG_QUERY_EXPANSION_MODEL=gpt-4o-mini
RAG_QUERY_EXPANSION_TEMPERATURE=0.7
RAG_QUERY_PRF_ENABLED=true
RAG_QUERY_PRF_TOP_K=3
RAG_QUERY_PRF_NUM_TERMS=10
RAG_QUERY_DECOMPOSITION_ENABLED=true
RAG_QUERY_DECOMPOSITION_THRESHOLD=5

# ───────────────────────────────────────────────────────────────────────────────
# MULTI-STAGE RERANKING
# Progressive refinement pipeline
# ───────────────────────────────────────────────────────────────────────────────
RAG_MULTISTAGE_RERANK_ENABLED=true

# Stage 1: Bi-encoder filtering
RAG_RERANK_STAGE1_ENABLED=true
RAG_RERANK_STAGE1_TOP_K=100

# Stage 2: Cross-encoder scoring
RAG_RERANK_STAGE2_ENABLED=true
RAG_RERANK_STAGE2_TOP_K=30
RAG_RERANK_STAGE2_MODEL=gpt-4o-mini
RAG_RERANK_STAGE2_BATCH_SIZE=10

# Stage 3: MMR diversity
RAG_RERANK_STAGE3_ENABLED=true
RAG_RERANK_MMR_LAMBDA=0.7

# Stage 4: Score calibration
RAG_RERANK_STAGE4_ENABLED=true

# ───────────────────────────────────────────────────────────────────────────────
# DIVERSITY OPTIMIZATION
# Result diversity and deduplication
# ───────────────────────────────────────────────────────────────────────────────
RAG_DIVERSITY_ENABLED=true
RAG_DIVERSITY_MMR_LAMBDA=0.7
RAG_DIVERSITY_MAX_PER_DOCUMENT=3
RAG_DIVERSITY_MAX_PER_PAGE=2
RAG_DIVERSITY_MIN_SIMILARITY_THRESHOLD=0.95

# ───────────────────────────────────────────────────────────────────────────────
# LLM-AS-JUDGE EVALUATION
# Automated evaluation using LLM
# ───────────────────────────────────────────────────────────────────────────────
RAG_JUDGE_ENABLED=false
RAG_JUDGE_MODEL=gpt-4o-mini
RAG_JUDGE_TEMPERATURE=0.0
RAG_JUDGE_MAX_CONCURRENT=5

# ───────────────────────────────────────────────────────────────────────────────
# OBSERVABILITY
# ───────────────────────────────────────────────────────────────────────────────
RAG_METRICS_ENABLED=false
RAG_METRICS_PORT=9190

# ───────────────────────────────────────────────────────────────────────────────
# LOGGING
# ───────────────────────────────────────────────────────────────────────────────
LOG_LEVEL=INFO
LOG_FORMAT=json
